{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivanakhoo/Trading_Optimizer/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMaSDrxS6ZSO"
      },
      "source": [
        "## Training on One Year of Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjEgAXhLyi5T"
      },
      "source": [
        "### Import Packages and mount drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaURfrO8a7Xy"
      },
      "outputs": [],
      "source": [
        "# First we will import the necessary Library\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For Evalution we will use these library\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score\n",
        "from sklearn.metrics import mean_poisson_deviance, mean_gamma_deviance, accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# For model building we will use these library\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "\n",
        "# For PLotting we will use these library\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W_2Zyz5bbOw",
        "outputId": "44893948-c0ef-454a-c4c7-1adf25104cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BiLhqwjbvFp",
        "outputId": "543b2a90-318a-4de8-d48e-bc75130fa9e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      Date   Value\n",
            "0  9/11/16  621.65\n",
            "1  9/12/16  609.67\n",
            "2  9/13/16  610.92\n",
            "3  9/14/16  608.82\n",
            "4  9/15/16  610.38\n"
          ]
        }
      ],
      "source": [
        "file_path = \"/content/drive/My Drive/modelingComp/BCHAIN-MKPRU.csv\"\n",
        "# Load the CSV into a DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "ETXuZkrvcMaa",
        "outputId": "430ce04f-ab08-49fe-a10e-1c5e24529019"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"273ccfd8-0c8e-4227-aa1d-d16797c753e0\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"273ccfd8-0c8e-4227-aa1d-d16797c753e0\")) {                    Plotly.newPlot(                        \"273ccfd8-0c8e-4227-aa1d-d16797c753e0\",                        [{\"hovertemplate\":\"Date=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"orange\",\"width\":2}},\"mode\":\"lines\",\"name\":\"\",\"showlegend\":false,\"x\":[\"9\\u002f11\\u002f16\",\"9\\u002f12\\u002f16\",\"9\\u002f13\\u002f16\",\"9\\u002f14\\u002f16\",\"9\\u002f15\\u002f16\",\"9\\u002f16\\u002f16\",\"9\\u002f17\\u002f16\",\"9\\u002f18\\u002f16\",\"9\\u002f19\\u002f16\",\"9\\u002f20\\u002f16\",\"9\\u002f21\\u002f16\",\"9\\u002f22\\u002f16\",\"9\\u002f23\\u002f16\",\"9\\u002f24\\u002f16\",\"9\\u002f25\\u002f16\",\"9\\u002f26\\u002f16\",\"9\\u002f27\\u002f16\",\"9\\u002f28\\u002f16\",\"9\\u002f29\\u002f16\",\"9\\u002f30\\u002f16\",\"10\\u002f1\\u002f16\",\"10\\u002f2\\u002f16\",\"10\\u002f3\\u002f16\",\"10\\u002f4\\u002f16\",\"10\\u002f5\\u002f16\",\"10\\u002f6\\u002f16\",\"10\\u002f7\\u002f16\",\"10\\u002f8\\u002f16\",\"10\\u002f9\\u002f16\",\"10\\u002f10\\u002f16\",\"10\\u002f11\\u002f16\",\"10\\u002f12\\u002f16\",\"10\\u002f13\\u002f16\",\"10\\u002f14\\u002f16\",\"10\\u002f15\\u002f16\",\"10\\u002f16\\u002f16\",\"10\\u002f17\\u002f16\",\"10\\u002f18\\u002f16\",\"10\\u002f19\\u002f16\",\"10\\u002f20\\u002f16\",\"10\\u002f21\\u002f16\",\"10\\u002f22\\u002f16\",\"10\\u002f23\\u002f16\",\"10\\u002f24\\u002f16\",\"10\\u002f25\\u002f16\",\"10\\u002f26\\u002f16\",\"10\\u002f27\\u002f16\",\"10\\u002f28\\u002f16\",\"10\\u002f29\\u002f16\",\"10\\u002f30\\u002f16\",\"10\\u002f31\\u002f16\",\"11\\u002f1\\u002f16\",\"11\\u002f2\\u002f16\",\"11\\u002f3\\u002f16\",\"11\\u002f4\\u002f16\",\"11\\u002f5\\u002f16\",\"11\\u002f6\\u002f16\",\"11\\u002f7\\u002f16\",\"11\\u002f8\\u002f16\",\"11\\u002f9\\u002f16\",\"11\\u002f10\\u002f16\",\"11\\u002f11\\u002f16\",\"11\\u002f12\\u002f16\",\"11\\u002f13\\u002f16\",\"11\\u002f14\\u002f16\",\"11\\u002f15\\u002f16\",\"11\\u002f16\\u002f16\",\"11\\u002f17\\u002f16\",\"11\\u002f18\\u002f16\",\"11\\u002f19\\u002f16\",\"11\\u002f20\\u002f16\",\"11\\u002f21\\u002f16\",\"11\\u002f22\\u002f16\",\"11\\u002f23\\u002f16\",\"11\\u002f24\\u002f16\",\"11\\u002f25\\u002f16\",\"11\\u002f26\\u002f16\",\"11\\u002f27\\u002f16\",\"11\\u002f28\\u002f16\",\"11\\u002f29\\u002f16\",\"11\\u002f30\\u002f16\",\"12\\u002f1\\u002f16\",\"12\\u002f2\\u002f16\",\"12\\u002f3\\u002f16\",\"12\\u002f4\\u002f16\",\"12\\u002f5\\u002f16\",\"12\\u002f6\\u002f16\",\"12\\u002f7\\u002f16\",\"12\\u002f8\\u002f16\",\"12\\u002f9\\u002f16\",\"12\\u002f10\\u002f16\",\"12\\u002f11\\u002f16\",\"12\\u002f12\\u002f16\",\"12\\u002f13\\u002f16\",\"12\\u002f14\\u002f16\",\"12\\u002f15\\u002f16\",\"12\\u002f16\\u002f16\",\"12\\u002f17\\u002f16\",\"12\\u002f18\\u002f16\",\"12\\u002f19\\u002f16\",\"12\\u002f20\\u002f16\",\"12\\u002f21\\u002f16\",\"12\\u002f22\\u002f16\",\"12\\u002f23\\u002f16\",\"12\\u002f24\\u002f16\",\"12\\u002f25\\u002f16\",\"12\\u002f26\\u002f16\",\"12\\u002f27\\u002f16\",\"12\\u002f28\\u002f16\",\"12\\u002f29\\u002f16\",\"12\\u002f30\\u002f16\",\"12\\u002f31\\u002f16\",\"1\\u002f1\\u002f17\",\"1\\u002f2\\u002f17\",\"1\\u002f3\\u002f17\",\"1\\u002f4\\u002f17\",\"1\\u002f5\\u002f17\",\"1\\u002f6\\u002f17\",\"1\\u002f7\\u002f17\",\"1\\u002f8\\u002f17\",\"1\\u002f9\\u002f17\",\"1\\u002f10\\u002f17\",\"1\\u002f11\\u002f17\",\"1\\u002f12\\u002f17\",\"1\\u002f13\\u002f17\",\"1\\u002f14\\u002f17\",\"1\\u002f15\\u002f17\",\"1\\u002f16\\u002f17\",\"1\\u002f17\\u002f17\",\"1\\u002f18\\u002f17\",\"1\\u002f19\\u002f17\",\"1\\u002f20\\u002f17\",\"1\\u002f21\\u002f17\",\"1\\u002f22\\u002f17\",\"1\\u002f23\\u002f17\",\"1\\u002f24\\u002f17\",\"1\\u002f25\\u002f17\",\"1\\u002f26\\u002f17\",\"1\\u002f27\\u002f17\",\"1\\u002f28\\u002f17\",\"1\\u002f29\\u002f17\",\"1\\u002f30\\u002f17\",\"1\\u002f31\\u002f17\",\"2\\u002f1\\u002f17\",\"2\\u002f2\\u002f17\",\"2\\u002f3\\u002f17\",\"2\\u002f4\\u002f17\",\"2\\u002f5\\u002f17\",\"2\\u002f6\\u002f17\",\"2\\u002f7\\u002f17\",\"2\\u002f8\\u002f17\",\"2\\u002f9\\u002f17\",\"2\\u002f10\\u002f17\",\"2\\u002f11\\u002f17\",\"2\\u002f12\\u002f17\",\"2\\u002f13\\u002f17\",\"2\\u002f14\\u002f17\",\"2\\u002f15\\u002f17\",\"2\\u002f16\\u002f17\",\"2\\u002f17\\u002f17\",\"2\\u002f18\\u002f17\",\"2\\u002f19\\u002f17\",\"2\\u002f20\\u002f17\",\"2\\u002f21\\u002f17\",\"2\\u002f22\\u002f17\",\"2\\u002f23\\u002f17\",\"2\\u002f24\\u002f17\",\"2\\u002f25\\u002f17\",\"2\\u002f26\\u002f17\",\"2\\u002f27\\u002f17\",\"2\\u002f28\\u002f17\",\"3\\u002f1\\u002f17\",\"3\\u002f2\\u002f17\",\"3\\u002f3\\u002f17\",\"3\\u002f4\\u002f17\",\"3\\u002f5\\u002f17\",\"3\\u002f6\\u002f17\",\"3\\u002f7\\u002f17\",\"3\\u002f8\\u002f17\",\"3\\u002f9\\u002f17\",\"3\\u002f10\\u002f17\",\"3\\u002f11\\u002f17\",\"3\\u002f12\\u002f17\",\"3\\u002f13\\u002f17\",\"3\\u002f14\\u002f17\",\"3\\u002f15\\u002f17\",\"3\\u002f16\\u002f17\",\"3\\u002f17\\u002f17\",\"3\\u002f18\\u002f17\",\"3\\u002f19\\u002f17\",\"3\\u002f20\\u002f17\",\"3\\u002f21\\u002f17\",\"3\\u002f22\\u002f17\",\"3\\u002f23\\u002f17\",\"3\\u002f24\\u002f17\",\"3\\u002f25\\u002f17\",\"3\\u002f26\\u002f17\",\"3\\u002f27\\u002f17\",\"3\\u002f28\\u002f17\",\"3\\u002f29\\u002f17\",\"3\\u002f30\\u002f17\",\"3\\u002f31\\u002f17\",\"4\\u002f1\\u002f17\",\"4\\u002f2\\u002f17\",\"4\\u002f3\\u002f17\",\"4\\u002f4\\u002f17\",\"4\\u002f5\\u002f17\",\"4\\u002f6\\u002f17\",\"4\\u002f7\\u002f17\",\"4\\u002f8\\u002f17\",\"4\\u002f9\\u002f17\",\"4\\u002f10\\u002f17\",\"4\\u002f11\\u002f17\",\"4\\u002f12\\u002f17\",\"4\\u002f13\\u002f17\",\"4\\u002f14\\u002f17\",\"4\\u002f15\\u002f17\",\"4\\u002f16\\u002f17\",\"4\\u002f17\\u002f17\",\"4\\u002f18\\u002f17\",\"4\\u002f19\\u002f17\",\"4\\u002f20\\u002f17\",\"4\\u002f21\\u002f17\",\"4\\u002f22\\u002f17\",\"4\\u002f23\\u002f17\",\"4\\u002f24\\u002f17\",\"4\\u002f25\\u002f17\",\"4\\u002f26\\u002f17\",\"4\\u002f27\\u002f17\",\"4\\u002f28\\u002f17\",\"4\\u002f29\\u002f17\",\"4\\u002f30\\u002f17\",\"5\\u002f1\\u002f17\",\"5\\u002f2\\u002f17\",\"5\\u002f3\\u002f17\",\"5\\u002f4\\u002f17\",\"5\\u002f5\\u002f17\",\"5\\u002f6\\u002f17\",\"5\\u002f7\\u002f17\",\"5\\u002f8\\u002f17\",\"5\\u002f9\\u002f17\",\"5\\u002f10\\u002f17\",\"5\\u002f11\\u002f17\",\"5\\u002f12\\u002f17\",\"5\\u002f13\\u002f17\",\"5\\u002f14\\u002f17\",\"5\\u002f15\\u002f17\",\"5\\u002f16\\u002f17\",\"5\\u002f17\\u002f17\",\"5\\u002f18\\u002f17\",\"5\\u002f19\\u002f17\",\"5\\u002f20\\u002f17\",\"5\\u002f21\\u002f17\",\"5\\u002f22\\u002f17\",\"5\\u002f23\\u002f17\",\"5\\u002f24\\u002f17\",\"5\\u002f25\\u002f17\",\"5\\u002f26\\u002f17\",\"5\\u002f27\\u002f17\",\"5\\u002f28\\u002f17\",\"5\\u002f29\\u002f17\",\"5\\u002f30\\u002f17\",\"5\\u002f31\\u002f17\",\"6\\u002f1\\u002f17\",\"6\\u002f2\\u002f17\",\"6\\u002f3\\u002f17\",\"6\\u002f4\\u002f17\",\"6\\u002f5\\u002f17\",\"6\\u002f6\\u002f17\",\"6\\u002f7\\u002f17\",\"6\\u002f8\\u002f17\",\"6\\u002f9\\u002f17\",\"6\\u002f10\\u002f17\",\"6\\u002f11\\u002f17\",\"6\\u002f12\\u002f17\",\"6\\u002f13\\u002f17\",\"6\\u002f14\\u002f17\",\"6\\u002f15\\u002f17\",\"6\\u002f16\\u002f17\",\"6\\u002f17\\u002f17\",\"6\\u002f18\\u002f17\",\"6\\u002f19\\u002f17\",\"6\\u002f20\\u002f17\",\"6\\u002f21\\u002f17\",\"6\\u002f22\\u002f17\",\"6\\u002f23\\u002f17\",\"6\\u002f24\\u002f17\",\"6\\u002f25\\u002f17\",\"6\\u002f26\\u002f17\",\"6\\u002f27\\u002f17\",\"6\\u002f28\\u002f17\",\"6\\u002f29\\u002f17\",\"6\\u002f30\\u002f17\",\"7\\u002f1\\u002f17\",\"7\\u002f2\\u002f17\",\"7\\u002f3\\u002f17\",\"7\\u002f4\\u002f17\",\"7\\u002f5\\u002f17\",\"7\\u002f6\\u002f17\",\"7\\u002f7\\u002f17\",\"7\\u002f8\\u002f17\",\"7\\u002f9\\u002f17\",\"7\\u002f10\\u002f17\",\"7\\u002f11\\u002f17\",\"7\\u002f12\\u002f17\",\"7\\u002f13\\u002f17\",\"7\\u002f14\\u002f17\",\"7\\u002f15\\u002f17\",\"7\\u002f16\\u002f17\",\"7\\u002f17\\u002f17\",\"7\\u002f18\\u002f17\",\"7\\u002f19\\u002f17\",\"7\\u002f20\\u002f17\",\"7\\u002f21\\u002f17\",\"7\\u002f22\\u002f17\",\"7\\u002f23\\u002f17\",\"7\\u002f24\\u002f17\",\"7\\u002f25\\u002f17\",\"7\\u002f26\\u002f17\",\"7\\u002f27\\u002f17\",\"7\\u002f28\\u002f17\",\"7\\u002f29\\u002f17\",\"7\\u002f30\\u002f17\",\"7\\u002f31\\u002f17\",\"8\\u002f1\\u002f17\",\"8\\u002f2\\u002f17\",\"8\\u002f3\\u002f17\",\"8\\u002f4\\u002f17\",\"8\\u002f5\\u002f17\",\"8\\u002f6\\u002f17\",\"8\\u002f7\\u002f17\",\"8\\u002f8\\u002f17\",\"8\\u002f9\\u002f17\",\"8\\u002f10\\u002f17\",\"8\\u002f11\\u002f17\",\"8\\u002f12\\u002f17\",\"8\\u002f13\\u002f17\",\"8\\u002f14\\u002f17\",\"8\\u002f15\\u002f17\",\"8\\u002f16\\u002f17\",\"8\\u002f17\\u002f17\",\"8\\u002f18\\u002f17\",\"8\\u002f19\\u002f17\",\"8\\u002f20\\u002f17\",\"8\\u002f21\\u002f17\",\"8\\u002f22\\u002f17\",\"8\\u002f23\\u002f17\",\"8\\u002f24\\u002f17\",\"8\\u002f25\\u002f17\",\"8\\u002f26\\u002f17\",\"8\\u002f27\\u002f17\",\"8\\u002f28\\u002f17\",\"8\\u002f29\\u002f17\",\"8\\u002f30\\u002f17\",\"8\\u002f31\\u002f17\",\"9\\u002f1\\u002f17\",\"9\\u002f2\\u002f17\",\"9\\u002f3\\u002f17\",\"9\\u002f4\\u002f17\",\"9\\u002f5\\u002f17\",\"9\\u002f6\\u002f17\",\"9\\u002f7\\u002f17\",\"9\\u002f8\\u002f17\",\"9\\u002f9\\u002f17\",\"9\\u002f10\\u002f17\",\"9\\u002f11\\u002f17\",\"9\\u002f12\\u002f17\",\"9\\u002f13\\u002f17\",\"9\\u002f14\\u002f17\",\"9\\u002f15\\u002f17\",\"9\\u002f16\\u002f17\",\"9\\u002f17\\u002f17\",\"9\\u002f18\\u002f17\",\"9\\u002f19\\u002f17\",\"9\\u002f20\\u002f17\",\"9\\u002f21\\u002f17\",\"9\\u002f22\\u002f17\",\"9\\u002f23\\u002f17\",\"9\\u002f24\\u002f17\",\"9\\u002f25\\u002f17\",\"9\\u002f26\\u002f17\",\"9\\u002f27\\u002f17\",\"9\\u002f28\\u002f17\",\"9\\u002f29\\u002f17\",\"9\\u002f30\\u002f17\",\"10\\u002f1\\u002f17\",\"10\\u002f2\\u002f17\",\"10\\u002f3\\u002f17\",\"10\\u002f4\\u002f17\",\"10\\u002f5\\u002f17\",\"10\\u002f6\\u002f17\",\"10\\u002f7\\u002f17\",\"10\\u002f8\\u002f17\",\"10\\u002f9\\u002f17\",\"10\\u002f10\\u002f17\",\"10\\u002f11\\u002f17\",\"10\\u002f12\\u002f17\",\"10\\u002f13\\u002f17\",\"10\\u002f14\\u002f17\",\"10\\u002f15\\u002f17\",\"10\\u002f16\\u002f17\",\"10\\u002f17\\u002f17\",\"10\\u002f18\\u002f17\",\"10\\u002f19\\u002f17\",\"10\\u002f20\\u002f17\",\"10\\u002f21\\u002f17\",\"10\\u002f22\\u002f17\",\"10\\u002f23\\u002f17\",\"10\\u002f24\\u002f17\",\"10\\u002f25\\u002f17\",\"10\\u002f26\\u002f17\",\"10\\u002f27\\u002f17\",\"10\\u002f28\\u002f17\",\"10\\u002f29\\u002f17\",\"10\\u002f30\\u002f17\",\"10\\u002f31\\u002f17\",\"11\\u002f1\\u002f17\",\"11\\u002f2\\u002f17\",\"11\\u002f3\\u002f17\",\"11\\u002f4\\u002f17\",\"11\\u002f5\\u002f17\",\"11\\u002f6\\u002f17\",\"11\\u002f7\\u002f17\",\"11\\u002f8\\u002f17\",\"11\\u002f9\\u002f17\",\"11\\u002f10\\u002f17\",\"11\\u002f11\\u002f17\",\"11\\u002f12\\u002f17\",\"11\\u002f13\\u002f17\",\"11\\u002f14\\u002f17\",\"11\\u002f15\\u002f17\",\"11\\u002f16\\u002f17\",\"11\\u002f17\\u002f17\",\"11\\u002f18\\u002f17\",\"11\\u002f19\\u002f17\",\"11\\u002f20\\u002f17\",\"11\\u002f21\\u002f17\",\"11\\u002f22\\u002f17\",\"11\\u002f23\\u002f17\",\"11\\u002f24\\u002f17\",\"11\\u002f25\\u002f17\",\"11\\u002f26\\u002f17\",\"11\\u002f27\\u002f17\",\"11\\u002f28\\u002f17\",\"11\\u002f29\\u002f17\",\"11\\u002f30\\u002f17\",\"12\\u002f1\\u002f17\",\"12\\u002f2\\u002f17\",\"12\\u002f3\\u002f17\",\"12\\u002f4\\u002f17\",\"12\\u002f5\\u002f17\",\"12\\u002f6\\u002f17\",\"12\\u002f7\\u002f17\",\"12\\u002f8\\u002f17\",\"12\\u002f9\\u002f17\",\"12\\u002f10\\u002f17\",\"12\\u002f11\\u002f17\",\"12\\u002f12\\u002f17\",\"12\\u002f13\\u002f17\",\"12\\u002f14\\u002f17\",\"12\\u002f15\\u002f17\",\"12\\u002f16\\u002f17\",\"12\\u002f17\\u002f17\",\"12\\u002f18\\u002f17\",\"12\\u002f19\\u002f17\",\"12\\u002f20\\u002f17\",\"12\\u002f21\\u002f17\",\"12\\u002f22\\u002f17\",\"12\\u002f23\\u002f17\",\"12\\u002f24\\u002f17\",\"12\\u002f25\\u002f17\",\"12\\u002f26\\u002f17\",\"12\\u002f27\\u002f17\",\"12\\u002f28\\u002f17\",\"12\\u002f29\\u002f17\",\"12\\u002f30\\u002f17\",\"12\\u002f31\\u002f17\",\"1\\u002f1\\u002f18\",\"1\\u002f2\\u002f18\",\"1\\u002f3\\u002f18\",\"1\\u002f4\\u002f18\",\"1\\u002f5\\u002f18\",\"1\\u002f6\\u002f18\",\"1\\u002f7\\u002f18\",\"1\\u002f8\\u002f18\",\"1\\u002f9\\u002f18\",\"1\\u002f10\\u002f18\",\"1\\u002f11\\u002f18\",\"1\\u002f12\\u002f18\",\"1\\u002f13\\u002f18\",\"1\\u002f14\\u002f18\",\"1\\u002f15\\u002f18\",\"1\\u002f16\\u002f18\",\"1\\u002f17\\u002f18\",\"1\\u002f18\\u002f18\",\"1\\u002f19\\u002f18\",\"1\\u002f20\\u002f18\",\"1\\u002f21\\u002f18\",\"1\\u002f22\\u002f18\",\"1\\u002f23\\u002f18\",\"1\\u002f24\\u002f18\",\"1\\u002f25\\u002f18\",\"1\\u002f26\\u002f18\",\"1\\u002f27\\u002f18\",\"1\\u002f28\\u002f18\",\"1\\u002f29\\u002f18\",\"1\\u002f30\\u002f18\",\"1\\u002f31\\u002f18\",\"2\\u002f1\\u002f18\",\"2\\u002f2\\u002f18\",\"2\\u002f3\\u002f18\",\"2\\u002f4\\u002f18\",\"2\\u002f5\\u002f18\",\"2\\u002f6\\u002f18\",\"2\\u002f7\\u002f18\",\"2\\u002f8\\u002f18\",\"2\\u002f9\\u002f18\",\"2\\u002f10\\u002f18\",\"2\\u002f11\\u002f18\",\"2\\u002f12\\u002f18\",\"2\\u002f13\\u002f18\",\"2\\u002f14\\u002f18\",\"2\\u002f15\\u002f18\",\"2\\u002f16\\u002f18\",\"2\\u002f17\\u002f18\",\"2\\u002f18\\u002f18\",\"2\\u002f19\\u002f18\",\"2\\u002f20\\u002f18\",\"2\\u002f21\\u002f18\",\"2\\u002f22\\u002f18\",\"2\\u002f23\\u002f18\",\"2\\u002f24\\u002f18\",\"2\\u002f25\\u002f18\",\"2\\u002f26\\u002f18\",\"2\\u002f27\\u002f18\",\"2\\u002f28\\u002f18\",\"3\\u002f1\\u002f18\",\"3\\u002f2\\u002f18\",\"3\\u002f3\\u002f18\",\"3\\u002f4\\u002f18\",\"3\\u002f5\\u002f18\",\"3\\u002f6\\u002f18\",\"3\\u002f7\\u002f18\",\"3\\u002f8\\u002f18\",\"3\\u002f9\\u002f18\",\"3\\u002f10\\u002f18\",\"3\\u002f11\\u002f18\",\"3\\u002f12\\u002f18\",\"3\\u002f13\\u002f18\",\"3\\u002f14\\u002f18\",\"3\\u002f15\\u002f18\",\"3\\u002f16\\u002f18\",\"3\\u002f17\\u002f18\",\"3\\u002f18\\u002f18\",\"3\\u002f19\\u002f18\",\"3\\u002f20\\u002f18\",\"3\\u002f21\\u002f18\",\"3\\u002f22\\u002f18\",\"3\\u002f23\\u002f18\",\"3\\u002f24\\u002f18\",\"3\\u002f25\\u002f18\",\"3\\u002f26\\u002f18\",\"3\\u002f27\\u002f18\",\"3\\u002f28\\u002f18\",\"3\\u002f29\\u002f18\",\"3\\u002f30\\u002f18\",\"3\\u002f31\\u002f18\",\"4\\u002f1\\u002f18\",\"4\\u002f2\\u002f18\",\"4\\u002f3\\u002f18\",\"4\\u002f4\\u002f18\",\"4\\u002f5\\u002f18\",\"4\\u002f6\\u002f18\",\"4\\u002f7\\u002f18\",\"4\\u002f8\\u002f18\",\"4\\u002f9\\u002f18\",\"4\\u002f10\\u002f18\",\"4\\u002f11\\u002f18\",\"4\\u002f12\\u002f18\",\"4\\u002f13\\u002f18\",\"4\\u002f14\\u002f18\",\"4\\u002f15\\u002f18\",\"4\\u002f16\\u002f18\",\"4\\u002f17\\u002f18\",\"4\\u002f18\\u002f18\",\"4\\u002f19\\u002f18\",\"4\\u002f20\\u002f18\",\"4\\u002f21\\u002f18\",\"4\\u002f22\\u002f18\",\"4\\u002f23\\u002f18\",\"4\\u002f24\\u002f18\",\"4\\u002f25\\u002f18\",\"4\\u002f26\\u002f18\",\"4\\u002f27\\u002f18\",\"4\\u002f28\\u002f18\",\"4\\u002f29\\u002f18\",\"4\\u002f30\\u002f18\",\"5\\u002f1\\u002f18\",\"5\\u002f2\\u002f18\",\"5\\u002f3\\u002f18\",\"5\\u002f4\\u002f18\",\"5\\u002f5\\u002f18\",\"5\\u002f6\\u002f18\",\"5\\u002f7\\u002f18\",\"5\\u002f8\\u002f18\",\"5\\u002f9\\u002f18\",\"5\\u002f10\\u002f18\",\"5\\u002f11\\u002f18\",\"5\\u002f12\\u002f18\",\"5\\u002f13\\u002f18\",\"5\\u002f14\\u002f18\",\"5\\u002f15\\u002f18\",\"5\\u002f16\\u002f18\",\"5\\u002f17\\u002f18\",\"5\\u002f18\\u002f18\",\"5\\u002f19\\u002f18\",\"5\\u002f20\\u002f18\",\"5\\u002f21\\u002f18\",\"5\\u002f22\\u002f18\",\"5\\u002f23\\u002f18\",\"5\\u002f24\\u002f18\",\"5\\u002f25\\u002f18\",\"5\\u002f26\\u002f18\",\"5\\u002f27\\u002f18\",\"5\\u002f28\\u002f18\",\"5\\u002f29\\u002f18\",\"5\\u002f30\\u002f18\",\"5\\u002f31\\u002f18\",\"6\\u002f1\\u002f18\",\"6\\u002f2\\u002f18\",\"6\\u002f3\\u002f18\",\"6\\u002f4\\u002f18\",\"6\\u002f5\\u002f18\",\"6\\u002f6\\u002f18\",\"6\\u002f7\\u002f18\",\"6\\u002f8\\u002f18\",\"6\\u002f9\\u002f18\",\"6\\u002f10\\u002f18\",\"6\\u002f11\\u002f18\",\"6\\u002f12\\u002f18\",\"6\\u002f13\\u002f18\",\"6\\u002f14\\u002f18\",\"6\\u002f15\\u002f18\",\"6\\u002f16\\u002f18\",\"6\\u002f17\\u002f18\",\"6\\u002f18\\u002f18\",\"6\\u002f19\\u002f18\",\"6\\u002f20\\u002f18\",\"6\\u002f21\\u002f18\",\"6\\u002f22\\u002f18\",\"6\\u002f23\\u002f18\",\"6\\u002f24\\u002f18\",\"6\\u002f25\\u002f18\",\"6\\u002f26\\u002f18\",\"6\\u002f27\\u002f18\",\"6\\u002f28\\u002f18\",\"6\\u002f29\\u002f18\",\"6\\u002f30\\u002f18\",\"7\\u002f1\\u002f18\",\"7\\u002f2\\u002f18\",\"7\\u002f3\\u002f18\",\"7\\u002f4\\u002f18\",\"7\\u002f5\\u002f18\",\"7\\u002f6\\u002f18\",\"7\\u002f7\\u002f18\",\"7\\u002f8\\u002f18\",\"7\\u002f9\\u002f18\",\"7\\u002f10\\u002f18\",\"7\\u002f11\\u002f18\",\"7\\u002f12\\u002f18\",\"7\\u002f13\\u002f18\",\"7\\u002f14\\u002f18\",\"7\\u002f15\\u002f18\",\"7\\u002f16\\u002f18\",\"7\\u002f17\\u002f18\",\"7\\u002f18\\u002f18\",\"7\\u002f19\\u002f18\",\"7\\u002f20\\u002f18\",\"7\\u002f21\\u002f18\",\"7\\u002f22\\u002f18\",\"7\\u002f23\\u002f18\",\"7\\u002f24\\u002f18\",\"7\\u002f25\\u002f18\",\"7\\u002f26\\u002f18\",\"7\\u002f27\\u002f18\",\"7\\u002f28\\u002f18\",\"7\\u002f29\\u002f18\",\"7\\u002f30\\u002f18\",\"7\\u002f31\\u002f18\",\"8\\u002f1\\u002f18\",\"8\\u002f2\\u002f18\",\"8\\u002f3\\u002f18\",\"8\\u002f4\\u002f18\",\"8\\u002f5\\u002f18\",\"8\\u002f6\\u002f18\",\"8\\u002f7\\u002f18\",\"8\\u002f8\\u002f18\",\"8\\u002f9\\u002f18\",\"8\\u002f10\\u002f18\",\"8\\u002f11\\u002f18\",\"8\\u002f12\\u002f18\",\"8\\u002f13\\u002f18\",\"8\\u002f14\\u002f18\",\"8\\u002f15\\u002f18\",\"8\\u002f16\\u002f18\",\"8\\u002f17\\u002f18\",\"8\\u002f18\\u002f18\",\"8\\u002f19\\u002f18\",\"8\\u002f20\\u002f18\",\"8\\u002f21\\u002f18\",\"8\\u002f22\\u002f18\",\"8\\u002f23\\u002f18\",\"8\\u002f24\\u002f18\",\"8\\u002f25\\u002f18\",\"8\\u002f26\\u002f18\",\"8\\u002f27\\u002f18\",\"8\\u002f28\\u002f18\",\"8\\u002f29\\u002f18\",\"8\\u002f30\\u002f18\",\"8\\u002f31\\u002f18\",\"9\\u002f1\\u002f18\",\"9\\u002f2\\u002f18\",\"9\\u002f3\\u002f18\",\"9\\u002f4\\u002f18\",\"9\\u002f5\\u002f18\",\"9\\u002f6\\u002f18\",\"9\\u002f7\\u002f18\",\"9\\u002f8\\u002f18\",\"9\\u002f9\\u002f18\",\"9\\u002f10\\u002f18\",\"9\\u002f11\\u002f18\",\"9\\u002f12\\u002f18\",\"9\\u002f13\\u002f18\",\"9\\u002f14\\u002f18\",\"9\\u002f15\\u002f18\",\"9\\u002f16\\u002f18\",\"9\\u002f17\\u002f18\",\"9\\u002f18\\u002f18\",\"9\\u002f19\\u002f18\",\"9\\u002f20\\u002f18\",\"9\\u002f21\\u002f18\",\"9\\u002f22\\u002f18\",\"9\\u002f23\\u002f18\",\"9\\u002f24\\u002f18\",\"9\\u002f25\\u002f18\",\"9\\u002f26\\u002f18\",\"9\\u002f27\\u002f18\",\"9\\u002f28\\u002f18\",\"9\\u002f29\\u002f18\",\"9\\u002f30\\u002f18\",\"10\\u002f1\\u002f18\",\"10\\u002f2\\u002f18\",\"10\\u002f3\\u002f18\",\"10\\u002f4\\u002f18\",\"10\\u002f5\\u002f18\",\"10\\u002f6\\u002f18\",\"10\\u002f7\\u002f18\",\"10\\u002f8\\u002f18\",\"10\\u002f9\\u002f18\",\"10\\u002f10\\u002f18\",\"10\\u002f11\\u002f18\",\"10\\u002f12\\u002f18\",\"10\\u002f13\\u002f18\",\"10\\u002f14\\u002f18\",\"10\\u002f15\\u002f18\",\"10\\u002f16\\u002f18\",\"10\\u002f17\\u002f18\",\"10\\u002f18\\u002f18\",\"10\\u002f19\\u002f18\",\"10\\u002f20\\u002f18\",\"10\\u002f21\\u002f18\",\"10\\u002f22\\u002f18\",\"10\\u002f23\\u002f18\",\"10\\u002f24\\u002f18\",\"10\\u002f25\\u002f18\",\"10\\u002f26\\u002f18\",\"10\\u002f27\\u002f18\",\"10\\u002f28\\u002f18\",\"10\\u002f29\\u002f18\",\"10\\u002f30\\u002f18\",\"10\\u002f31\\u002f18\",\"11\\u002f1\\u002f18\",\"11\\u002f2\\u002f18\",\"11\\u002f3\\u002f18\",\"11\\u002f4\\u002f18\",\"11\\u002f5\\u002f18\",\"11\\u002f6\\u002f18\",\"11\\u002f7\\u002f18\",\"11\\u002f8\\u002f18\",\"11\\u002f9\\u002f18\",\"11\\u002f10\\u002f18\",\"11\\u002f11\\u002f18\",\"11\\u002f12\\u002f18\",\"11\\u002f13\\u002f18\",\"11\\u002f14\\u002f18\",\"11\\u002f15\\u002f18\",\"11\\u002f16\\u002f18\",\"11\\u002f17\\u002f18\",\"11\\u002f18\\u002f18\",\"11\\u002f19\\u002f18\",\"11\\u002f20\\u002f18\",\"11\\u002f21\\u002f18\",\"11\\u002f22\\u002f18\",\"11\\u002f23\\u002f18\",\"11\\u002f24\\u002f18\",\"11\\u002f25\\u002f18\",\"11\\u002f26\\u002f18\",\"11\\u002f27\\u002f18\",\"11\\u002f28\\u002f18\",\"11\\u002f29\\u002f18\",\"11\\u002f30\\u002f18\",\"12\\u002f1\\u002f18\",\"12\\u002f2\\u002f18\",\"12\\u002f3\\u002f18\",\"12\\u002f4\\u002f18\",\"12\\u002f5\\u002f18\",\"12\\u002f6\\u002f18\",\"12\\u002f7\\u002f18\",\"12\\u002f8\\u002f18\",\"12\\u002f9\\u002f18\",\"12\\u002f10\\u002f18\",\"12\\u002f11\\u002f18\",\"12\\u002f12\\u002f18\",\"12\\u002f13\\u002f18\",\"12\\u002f14\\u002f18\",\"12\\u002f15\\u002f18\",\"12\\u002f16\\u002f18\",\"12\\u002f17\\u002f18\",\"12\\u002f18\\u002f18\",\"12\\u002f19\\u002f18\",\"12\\u002f20\\u002f18\",\"12\\u002f21\\u002f18\",\"12\\u002f22\\u002f18\",\"12\\u002f23\\u002f18\",\"12\\u002f24\\u002f18\",\"12\\u002f25\\u002f18\",\"12\\u002f26\\u002f18\",\"12\\u002f27\\u002f18\",\"12\\u002f28\\u002f18\",\"12\\u002f29\\u002f18\",\"12\\u002f30\\u002f18\",\"12\\u002f31\\u002f18\",\"1\\u002f1\\u002f19\",\"1\\u002f2\\u002f19\",\"1\\u002f3\\u002f19\",\"1\\u002f4\\u002f19\",\"1\\u002f5\\u002f19\",\"1\\u002f6\\u002f19\",\"1\\u002f7\\u002f19\",\"1\\u002f8\\u002f19\",\"1\\u002f9\\u002f19\",\"1\\u002f10\\u002f19\",\"1\\u002f11\\u002f19\",\"1\\u002f12\\u002f19\",\"1\\u002f13\\u002f19\",\"1\\u002f14\\u002f19\",\"1\\u002f15\\u002f19\",\"1\\u002f16\\u002f19\",\"1\\u002f17\\u002f19\",\"1\\u002f18\\u002f19\",\"1\\u002f19\\u002f19\",\"1\\u002f20\\u002f19\",\"1\\u002f21\\u002f19\",\"1\\u002f22\\u002f19\",\"1\\u002f23\\u002f19\",\"1\\u002f24\\u002f19\",\"1\\u002f25\\u002f19\",\"1\\u002f26\\u002f19\",\"1\\u002f27\\u002f19\",\"1\\u002f28\\u002f19\",\"1\\u002f29\\u002f19\",\"1\\u002f30\\u002f19\",\"1\\u002f31\\u002f19\",\"2\\u002f1\\u002f19\",\"2\\u002f2\\u002f19\",\"2\\u002f3\\u002f19\",\"2\\u002f4\\u002f19\",\"2\\u002f5\\u002f19\",\"2\\u002f6\\u002f19\",\"2\\u002f7\\u002f19\",\"2\\u002f8\\u002f19\",\"2\\u002f9\\u002f19\",\"2\\u002f10\\u002f19\",\"2\\u002f11\\u002f19\",\"2\\u002f12\\u002f19\",\"2\\u002f13\\u002f19\",\"2\\u002f14\\u002f19\",\"2\\u002f15\\u002f19\",\"2\\u002f16\\u002f19\",\"2\\u002f17\\u002f19\",\"2\\u002f18\\u002f19\",\"2\\u002f19\\u002f19\",\"2\\u002f20\\u002f19\",\"2\\u002f21\\u002f19\",\"2\\u002f22\\u002f19\",\"2\\u002f23\\u002f19\",\"2\\u002f24\\u002f19\",\"2\\u002f25\\u002f19\",\"2\\u002f26\\u002f19\",\"2\\u002f27\\u002f19\",\"2\\u002f28\\u002f19\",\"3\\u002f1\\u002f19\",\"3\\u002f2\\u002f19\",\"3\\u002f3\\u002f19\",\"3\\u002f4\\u002f19\",\"3\\u002f5\\u002f19\",\"3\\u002f6\\u002f19\",\"3\\u002f7\\u002f19\",\"3\\u002f8\\u002f19\",\"3\\u002f9\\u002f19\",\"3\\u002f10\\u002f19\",\"3\\u002f11\\u002f19\",\"3\\u002f12\\u002f19\",\"3\\u002f13\\u002f19\",\"3\\u002f14\\u002f19\",\"3\\u002f15\\u002f19\",\"3\\u002f16\\u002f19\",\"3\\u002f17\\u002f19\",\"3\\u002f18\\u002f19\",\"3\\u002f19\\u002f19\",\"3\\u002f20\\u002f19\",\"3\\u002f21\\u002f19\",\"3\\u002f22\\u002f19\",\"3\\u002f23\\u002f19\",\"3\\u002f24\\u002f19\",\"3\\u002f25\\u002f19\",\"3\\u002f26\\u002f19\",\"3\\u002f27\\u002f19\",\"3\\u002f28\\u002f19\",\"3\\u002f29\\u002f19\",\"3\\u002f30\\u002f19\",\"3\\u002f31\\u002f19\",\"4\\u002f1\\u002f19\",\"4\\u002f2\\u002f19\",\"4\\u002f3\\u002f19\",\"4\\u002f4\\u002f19\",\"4\\u002f5\\u002f19\",\"4\\u002f6\\u002f19\",\"4\\u002f7\\u002f19\",\"4\\u002f8\\u002f19\",\"4\\u002f9\\u002f19\",\"4\\u002f10\\u002f19\",\"4\\u002f11\\u002f19\",\"4\\u002f12\\u002f19\",\"4\\u002f13\\u002f19\",\"4\\u002f14\\u002f19\",\"4\\u002f15\\u002f19\",\"4\\u002f16\\u002f19\",\"4\\u002f17\\u002f19\",\"4\\u002f18\\u002f19\",\"4\\u002f19\\u002f19\",\"4\\u002f20\\u002f19\",\"4\\u002f21\\u002f19\",\"4\\u002f22\\u002f19\",\"4\\u002f23\\u002f19\",\"4\\u002f24\\u002f19\",\"4\\u002f25\\u002f19\",\"4\\u002f26\\u002f19\",\"4\\u002f27\\u002f19\",\"4\\u002f28\\u002f19\",\"4\\u002f29\\u002f19\",\"4\\u002f30\\u002f19\",\"5\\u002f1\\u002f19\",\"5\\u002f2\\u002f19\",\"5\\u002f3\\u002f19\",\"5\\u002f4\\u002f19\",\"5\\u002f5\\u002f19\",\"5\\u002f6\\u002f19\",\"5\\u002f7\\u002f19\",\"5\\u002f8\\u002f19\",\"5\\u002f9\\u002f19\",\"5\\u002f10\\u002f19\",\"5\\u002f11\\u002f19\",\"5\\u002f12\\u002f19\",\"5\\u002f13\\u002f19\",\"5\\u002f14\\u002f19\",\"5\\u002f15\\u002f19\",\"5\\u002f16\\u002f19\",\"5\\u002f17\\u002f19\",\"5\\u002f18\\u002f19\",\"5\\u002f19\\u002f19\",\"5\\u002f20\\u002f19\",\"5\\u002f21\\u002f19\",\"5\\u002f22\\u002f19\",\"5\\u002f23\\u002f19\",\"5\\u002f24\\u002f19\",\"5\\u002f25\\u002f19\",\"5\\u002f26\\u002f19\",\"5\\u002f27\\u002f19\",\"5\\u002f28\\u002f19\",\"5\\u002f29\\u002f19\",\"5\\u002f30\\u002f19\",\"5\\u002f31\\u002f19\",\"6\\u002f1\\u002f19\",\"6\\u002f2\\u002f19\",\"6\\u002f3\\u002f19\",\"6\\u002f4\\u002f19\",\"6\\u002f5\\u002f19\",\"6\\u002f6\\u002f19\",\"6\\u002f7\\u002f19\",\"6\\u002f8\\u002f19\",\"6\\u002f9\\u002f19\",\"6\\u002f10\\u002f19\",\"6\\u002f11\\u002f19\",\"6\\u002f12\\u002f19\",\"6\\u002f13\\u002f19\",\"6\\u002f14\\u002f19\",\"6\\u002f15\\u002f19\",\"6\\u002f16\\u002f19\",\"6\\u002f17\\u002f19\",\"6\\u002f18\\u002f19\",\"6\\u002f19\\u002f19\",\"6\\u002f20\\u002f19\",\"6\\u002f21\\u002f19\",\"6\\u002f22\\u002f19\",\"6\\u002f23\\u002f19\",\"6\\u002f24\\u002f19\",\"6\\u002f25\\u002f19\",\"6\\u002f26\\u002f19\",\"6\\u002f27\\u002f19\",\"6\\u002f28\\u002f19\",\"6\\u002f29\\u002f19\",\"6\\u002f30\\u002f19\",\"7\\u002f1\\u002f19\",\"7\\u002f2\\u002f19\",\"7\\u002f3\\u002f19\",\"7\\u002f4\\u002f19\",\"7\\u002f5\\u002f19\",\"7\\u002f6\\u002f19\",\"7\\u002f7\\u002f19\",\"7\\u002f8\\u002f19\",\"7\\u002f9\\u002f19\",\"7\\u002f10\\u002f19\",\"7\\u002f11\\u002f19\",\"7\\u002f12\\u002f19\",\"7\\u002f13\\u002f19\",\"7\\u002f14\\u002f19\",\"7\\u002f15\\u002f19\",\"7\\u002f16\\u002f19\",\"7\\u002f17\\u002f19\",\"7\\u002f18\\u002f19\",\"7\\u002f19\\u002f19\",\"7\\u002f20\\u002f19\",\"7\\u002f21\\u002f19\",\"7\\u002f22\\u002f19\",\"7\\u002f23\\u002f19\",\"7\\u002f24\\u002f19\",\"7\\u002f25\\u002f19\",\"7\\u002f26\\u002f19\",\"7\\u002f27\\u002f19\",\"7\\u002f28\\u002f19\",\"7\\u002f29\\u002f19\",\"7\\u002f30\\u002f19\",\"7\\u002f31\\u002f19\",\"8\\u002f1\\u002f19\",\"8\\u002f2\\u002f19\",\"8\\u002f3\\u002f19\",\"8\\u002f4\\u002f19\",\"8\\u002f5\\u002f19\",\"8\\u002f6\\u002f19\",\"8\\u002f7\\u002f19\",\"8\\u002f8\\u002f19\",\"8\\u002f9\\u002f19\",\"8\\u002f10\\u002f19\",\"8\\u002f11\\u002f19\",\"8\\u002f12\\u002f19\",\"8\\u002f13\\u002f19\",\"8\\u002f14\\u002f19\",\"8\\u002f15\\u002f19\",\"8\\u002f16\\u002f19\",\"8\\u002f17\\u002f19\",\"8\\u002f18\\u002f19\",\"8\\u002f19\\u002f19\",\"8\\u002f20\\u002f19\",\"8\\u002f21\\u002f19\",\"8\\u002f22\\u002f19\",\"8\\u002f23\\u002f19\",\"8\\u002f24\\u002f19\",\"8\\u002f25\\u002f19\",\"8\\u002f26\\u002f19\",\"8\\u002f27\\u002f19\",\"8\\u002f28\\u002f19\",\"8\\u002f29\\u002f19\",\"8\\u002f30\\u002f19\",\"8\\u002f31\\u002f19\",\"9\\u002f1\\u002f19\",\"9\\u002f2\\u002f19\",\"9\\u002f3\\u002f19\",\"9\\u002f4\\u002f19\",\"9\\u002f5\\u002f19\",\"9\\u002f6\\u002f19\",\"9\\u002f7\\u002f19\",\"9\\u002f8\\u002f19\",\"9\\u002f9\\u002f19\",\"9\\u002f10\\u002f19\",\"9\\u002f11\\u002f19\",\"9\\u002f12\\u002f19\",\"9\\u002f13\\u002f19\",\"9\\u002f14\\u002f19\",\"9\\u002f15\\u002f19\",\"9\\u002f16\\u002f19\",\"9\\u002f17\\u002f19\",\"9\\u002f18\\u002f19\",\"9\\u002f19\\u002f19\",\"9\\u002f20\\u002f19\",\"9\\u002f21\\u002f19\",\"9\\u002f22\\u002f19\",\"9\\u002f23\\u002f19\",\"9\\u002f24\\u002f19\",\"9\\u002f25\\u002f19\",\"9\\u002f26\\u002f19\",\"9\\u002f27\\u002f19\",\"9\\u002f28\\u002f19\",\"9\\u002f29\\u002f19\",\"9\\u002f30\\u002f19\",\"10\\u002f1\\u002f19\",\"10\\u002f2\\u002f19\",\"10\\u002f3\\u002f19\",\"10\\u002f4\\u002f19\",\"10\\u002f5\\u002f19\",\"10\\u002f6\\u002f19\",\"10\\u002f7\\u002f19\",\"10\\u002f8\\u002f19\",\"10\\u002f9\\u002f19\",\"10\\u002f10\\u002f19\",\"10\\u002f11\\u002f19\",\"10\\u002f12\\u002f19\",\"10\\u002f13\\u002f19\",\"10\\u002f14\\u002f19\",\"10\\u002f15\\u002f19\",\"10\\u002f16\\u002f19\",\"10\\u002f17\\u002f19\",\"10\\u002f18\\u002f19\",\"10\\u002f19\\u002f19\",\"10\\u002f20\\u002f19\",\"10\\u002f21\\u002f19\",\"10\\u002f22\\u002f19\",\"10\\u002f23\\u002f19\",\"10\\u002f24\\u002f19\",\"10\\u002f25\\u002f19\",\"10\\u002f26\\u002f19\",\"10\\u002f27\\u002f19\",\"10\\u002f28\\u002f19\",\"10\\u002f29\\u002f19\",\"10\\u002f30\\u002f19\",\"10\\u002f31\\u002f19\",\"11\\u002f1\\u002f19\",\"11\\u002f2\\u002f19\",\"11\\u002f3\\u002f19\",\"11\\u002f4\\u002f19\",\"11\\u002f5\\u002f19\",\"11\\u002f6\\u002f19\",\"11\\u002f7\\u002f19\",\"11\\u002f8\\u002f19\",\"11\\u002f9\\u002f19\",\"11\\u002f10\\u002f19\",\"11\\u002f11\\u002f19\",\"11\\u002f12\\u002f19\",\"11\\u002f13\\u002f19\",\"11\\u002f14\\u002f19\",\"11\\u002f15\\u002f19\",\"11\\u002f16\\u002f19\",\"11\\u002f17\\u002f19\",\"11\\u002f18\\u002f19\",\"11\\u002f19\\u002f19\",\"11\\u002f20\\u002f19\",\"11\\u002f21\\u002f19\",\"11\\u002f22\\u002f19\",\"11\\u002f23\\u002f19\",\"11\\u002f24\\u002f19\",\"11\\u002f25\\u002f19\",\"11\\u002f26\\u002f19\",\"11\\u002f27\\u002f19\",\"11\\u002f28\\u002f19\",\"11\\u002f29\\u002f19\",\"11\\u002f30\\u002f19\",\"12\\u002f1\\u002f19\",\"12\\u002f2\\u002f19\",\"12\\u002f3\\u002f19\",\"12\\u002f4\\u002f19\",\"12\\u002f5\\u002f19\",\"12\\u002f6\\u002f19\",\"12\\u002f7\\u002f19\",\"12\\u002f8\\u002f19\",\"12\\u002f9\\u002f19\",\"12\\u002f10\\u002f19\",\"12\\u002f11\\u002f19\",\"12\\u002f12\\u002f19\",\"12\\u002f13\\u002f19\",\"12\\u002f14\\u002f19\",\"12\\u002f15\\u002f19\",\"12\\u002f16\\u002f19\",\"12\\u002f17\\u002f19\",\"12\\u002f18\\u002f19\",\"12\\u002f19\\u002f19\",\"12\\u002f20\\u002f19\",\"12\\u002f21\\u002f19\",\"12\\u002f22\\u002f19\",\"12\\u002f23\\u002f19\",\"12\\u002f24\\u002f19\",\"12\\u002f25\\u002f19\",\"12\\u002f26\\u002f19\",\"12\\u002f27\\u002f19\",\"12\\u002f28\\u002f19\",\"12\\u002f29\\u002f19\",\"12\\u002f30\\u002f19\",\"12\\u002f31\\u002f19\",\"1\\u002f1\\u002f20\",\"1\\u002f2\\u002f20\",\"1\\u002f3\\u002f20\",\"1\\u002f4\\u002f20\",\"1\\u002f5\\u002f20\",\"1\\u002f6\\u002f20\",\"1\\u002f7\\u002f20\",\"1\\u002f8\\u002f20\",\"1\\u002f9\\u002f20\",\"1\\u002f10\\u002f20\",\"1\\u002f11\\u002f20\",\"1\\u002f12\\u002f20\",\"1\\u002f13\\u002f20\",\"1\\u002f14\\u002f20\",\"1\\u002f15\\u002f20\",\"1\\u002f16\\u002f20\",\"1\\u002f17\\u002f20\",\"1\\u002f18\\u002f20\",\"1\\u002f19\\u002f20\",\"1\\u002f20\\u002f20\",\"1\\u002f21\\u002f20\",\"1\\u002f22\\u002f20\",\"1\\u002f23\\u002f20\",\"1\\u002f24\\u002f20\",\"1\\u002f25\\u002f20\",\"1\\u002f26\\u002f20\",\"1\\u002f27\\u002f20\",\"1\\u002f28\\u002f20\",\"1\\u002f29\\u002f20\",\"1\\u002f30\\u002f20\",\"1\\u002f31\\u002f20\",\"2\\u002f1\\u002f20\",\"2\\u002f2\\u002f20\",\"2\\u002f3\\u002f20\",\"2\\u002f4\\u002f20\",\"2\\u002f5\\u002f20\",\"2\\u002f6\\u002f20\",\"2\\u002f7\\u002f20\",\"2\\u002f8\\u002f20\",\"2\\u002f9\\u002f20\",\"2\\u002f10\\u002f20\",\"2\\u002f11\\u002f20\",\"2\\u002f12\\u002f20\",\"2\\u002f13\\u002f20\",\"2\\u002f14\\u002f20\",\"2\\u002f15\\u002f20\",\"2\\u002f16\\u002f20\",\"2\\u002f17\\u002f20\",\"2\\u002f18\\u002f20\",\"2\\u002f19\\u002f20\",\"2\\u002f20\\u002f20\",\"2\\u002f21\\u002f20\",\"2\\u002f22\\u002f20\",\"2\\u002f23\\u002f20\",\"2\\u002f24\\u002f20\",\"2\\u002f25\\u002f20\",\"2\\u002f26\\u002f20\",\"2\\u002f27\\u002f20\",\"2\\u002f28\\u002f20\",\"2\\u002f29\\u002f20\",\"3\\u002f1\\u002f20\",\"3\\u002f2\\u002f20\",\"3\\u002f3\\u002f20\",\"3\\u002f4\\u002f20\",\"3\\u002f5\\u002f20\",\"3\\u002f6\\u002f20\",\"3\\u002f7\\u002f20\",\"3\\u002f8\\u002f20\",\"3\\u002f9\\u002f20\",\"3\\u002f10\\u002f20\",\"3\\u002f11\\u002f20\",\"3\\u002f12\\u002f20\",\"3\\u002f13\\u002f20\",\"3\\u002f14\\u002f20\",\"3\\u002f15\\u002f20\",\"3\\u002f16\\u002f20\",\"3\\u002f17\\u002f20\",\"3\\u002f18\\u002f20\",\"3\\u002f19\\u002f20\",\"3\\u002f20\\u002f20\",\"3\\u002f21\\u002f20\",\"3\\u002f22\\u002f20\",\"3\\u002f23\\u002f20\",\"3\\u002f24\\u002f20\",\"3\\u002f25\\u002f20\",\"3\\u002f26\\u002f20\",\"3\\u002f27\\u002f20\",\"3\\u002f28\\u002f20\",\"3\\u002f29\\u002f20\",\"3\\u002f30\\u002f20\",\"3\\u002f31\\u002f20\",\"4\\u002f1\\u002f20\",\"4\\u002f2\\u002f20\",\"4\\u002f3\\u002f20\",\"4\\u002f4\\u002f20\",\"4\\u002f5\\u002f20\",\"4\\u002f6\\u002f20\",\"4\\u002f7\\u002f20\",\"4\\u002f8\\u002f20\",\"4\\u002f9\\u002f20\",\"4\\u002f10\\u002f20\",\"4\\u002f11\\u002f20\",\"4\\u002f12\\u002f20\",\"4\\u002f13\\u002f20\",\"4\\u002f14\\u002f20\",\"4\\u002f15\\u002f20\",\"4\\u002f16\\u002f20\",\"4\\u002f17\\u002f20\",\"4\\u002f18\\u002f20\",\"4\\u002f19\\u002f20\",\"4\\u002f20\\u002f20\",\"4\\u002f21\\u002f20\",\"4\\u002f22\\u002f20\",\"4\\u002f23\\u002f20\",\"4\\u002f24\\u002f20\",\"4\\u002f25\\u002f20\",\"4\\u002f26\\u002f20\",\"4\\u002f27\\u002f20\",\"4\\u002f28\\u002f20\",\"4\\u002f29\\u002f20\",\"4\\u002f30\\u002f20\",\"5\\u002f1\\u002f20\",\"5\\u002f2\\u002f20\",\"5\\u002f3\\u002f20\",\"5\\u002f4\\u002f20\",\"5\\u002f5\\u002f20\",\"5\\u002f6\\u002f20\",\"5\\u002f7\\u002f20\",\"5\\u002f8\\u002f20\",\"5\\u002f9\\u002f20\",\"5\\u002f10\\u002f20\",\"5\\u002f11\\u002f20\",\"5\\u002f12\\u002f20\",\"5\\u002f13\\u002f20\",\"5\\u002f14\\u002f20\",\"5\\u002f15\\u002f20\",\"5\\u002f16\\u002f20\",\"5\\u002f17\\u002f20\",\"5\\u002f18\\u002f20\",\"5\\u002f19\\u002f20\",\"5\\u002f20\\u002f20\",\"5\\u002f21\\u002f20\",\"5\\u002f22\\u002f20\",\"5\\u002f23\\u002f20\",\"5\\u002f24\\u002f20\",\"5\\u002f25\\u002f20\",\"5\\u002f26\\u002f20\",\"5\\u002f27\\u002f20\",\"5\\u002f28\\u002f20\",\"5\\u002f29\\u002f20\",\"5\\u002f30\\u002f20\",\"5\\u002f31\\u002f20\",\"6\\u002f1\\u002f20\",\"6\\u002f2\\u002f20\",\"6\\u002f3\\u002f20\",\"6\\u002f4\\u002f20\",\"6\\u002f5\\u002f20\",\"6\\u002f6\\u002f20\",\"6\\u002f7\\u002f20\",\"6\\u002f8\\u002f20\",\"6\\u002f9\\u002f20\",\"6\\u002f10\\u002f20\",\"6\\u002f11\\u002f20\",\"6\\u002f12\\u002f20\",\"6\\u002f13\\u002f20\",\"6\\u002f14\\u002f20\",\"6\\u002f15\\u002f20\",\"6\\u002f16\\u002f20\",\"6\\u002f17\\u002f20\",\"6\\u002f18\\u002f20\",\"6\\u002f19\\u002f20\",\"6\\u002f20\\u002f20\",\"6\\u002f21\\u002f20\",\"6\\u002f22\\u002f20\",\"6\\u002f23\\u002f20\",\"6\\u002f24\\u002f20\",\"6\\u002f25\\u002f20\",\"6\\u002f26\\u002f20\",\"6\\u002f27\\u002f20\",\"6\\u002f28\\u002f20\",\"6\\u002f29\\u002f20\",\"6\\u002f30\\u002f20\",\"7\\u002f1\\u002f20\",\"7\\u002f2\\u002f20\",\"7\\u002f3\\u002f20\",\"7\\u002f4\\u002f20\",\"7\\u002f5\\u002f20\",\"7\\u002f6\\u002f20\",\"7\\u002f7\\u002f20\",\"7\\u002f8\\u002f20\",\"7\\u002f9\\u002f20\",\"7\\u002f10\\u002f20\",\"7\\u002f11\\u002f20\",\"7\\u002f12\\u002f20\",\"7\\u002f13\\u002f20\",\"7\\u002f14\\u002f20\",\"7\\u002f15\\u002f20\",\"7\\u002f16\\u002f20\",\"7\\u002f17\\u002f20\",\"7\\u002f18\\u002f20\",\"7\\u002f19\\u002f20\",\"7\\u002f20\\u002f20\",\"7\\u002f21\\u002f20\",\"7\\u002f22\\u002f20\",\"7\\u002f23\\u002f20\",\"7\\u002f24\\u002f20\",\"7\\u002f25\\u002f20\",\"7\\u002f26\\u002f20\",\"7\\u002f27\\u002f20\",\"7\\u002f28\\u002f20\",\"7\\u002f29\\u002f20\",\"7\\u002f30\\u002f20\",\"7\\u002f31\\u002f20\",\"8\\u002f1\\u002f20\",\"8\\u002f2\\u002f20\",\"8\\u002f3\\u002f20\",\"8\\u002f4\\u002f20\",\"8\\u002f5\\u002f20\",\"8\\u002f6\\u002f20\",\"8\\u002f7\\u002f20\",\"8\\u002f8\\u002f20\",\"8\\u002f9\\u002f20\",\"8\\u002f10\\u002f20\",\"8\\u002f11\\u002f20\",\"8\\u002f12\\u002f20\",\"8\\u002f13\\u002f20\",\"8\\u002f14\\u002f20\",\"8\\u002f15\\u002f20\",\"8\\u002f16\\u002f20\",\"8\\u002f17\\u002f20\",\"8\\u002f18\\u002f20\",\"8\\u002f19\\u002f20\",\"8\\u002f20\\u002f20\",\"8\\u002f21\\u002f20\",\"8\\u002f22\\u002f20\",\"8\\u002f23\\u002f20\",\"8\\u002f24\\u002f20\",\"8\\u002f25\\u002f20\",\"8\\u002f26\\u002f20\",\"8\\u002f27\\u002f20\",\"8\\u002f28\\u002f20\",\"8\\u002f29\\u002f20\",\"8\\u002f30\\u002f20\",\"8\\u002f31\\u002f20\",\"9\\u002f1\\u002f20\",\"9\\u002f2\\u002f20\",\"9\\u002f3\\u002f20\",\"9\\u002f4\\u002f20\",\"9\\u002f5\\u002f20\",\"9\\u002f6\\u002f20\",\"9\\u002f7\\u002f20\",\"9\\u002f8\\u002f20\",\"9\\u002f9\\u002f20\",\"9\\u002f10\\u002f20\",\"9\\u002f11\\u002f20\",\"9\\u002f12\\u002f20\",\"9\\u002f13\\u002f20\",\"9\\u002f14\\u002f20\",\"9\\u002f15\\u002f20\",\"9\\u002f16\\u002f20\",\"9\\u002f17\\u002f20\",\"9\\u002f18\\u002f20\",\"9\\u002f19\\u002f20\",\"9\\u002f20\\u002f20\",\"9\\u002f21\\u002f20\",\"9\\u002f22\\u002f20\",\"9\\u002f23\\u002f20\",\"9\\u002f24\\u002f20\",\"9\\u002f25\\u002f20\",\"9\\u002f26\\u002f20\",\"9\\u002f27\\u002f20\",\"9\\u002f28\\u002f20\",\"9\\u002f29\\u002f20\",\"9\\u002f30\\u002f20\",\"10\\u002f1\\u002f20\",\"10\\u002f2\\u002f20\",\"10\\u002f3\\u002f20\",\"10\\u002f4\\u002f20\",\"10\\u002f5\\u002f20\",\"10\\u002f6\\u002f20\",\"10\\u002f7\\u002f20\",\"10\\u002f8\\u002f20\",\"10\\u002f9\\u002f20\",\"10\\u002f10\\u002f20\",\"10\\u002f11\\u002f20\",\"10\\u002f12\\u002f20\",\"10\\u002f13\\u002f20\",\"10\\u002f14\\u002f20\",\"10\\u002f15\\u002f20\",\"10\\u002f16\\u002f20\",\"10\\u002f17\\u002f20\",\"10\\u002f18\\u002f20\",\"10\\u002f19\\u002f20\",\"10\\u002f20\\u002f20\",\"10\\u002f21\\u002f20\",\"10\\u002f22\\u002f20\",\"10\\u002f23\\u002f20\",\"10\\u002f24\\u002f20\",\"10\\u002f25\\u002f20\",\"10\\u002f26\\u002f20\",\"10\\u002f27\\u002f20\",\"10\\u002f28\\u002f20\",\"10\\u002f29\\u002f20\",\"10\\u002f30\\u002f20\",\"10\\u002f31\\u002f20\",\"11\\u002f1\\u002f20\",\"11\\u002f2\\u002f20\",\"11\\u002f3\\u002f20\",\"11\\u002f4\\u002f20\",\"11\\u002f5\\u002f20\",\"11\\u002f6\\u002f20\",\"11\\u002f7\\u002f20\",\"11\\u002f8\\u002f20\",\"11\\u002f9\\u002f20\",\"11\\u002f10\\u002f20\",\"11\\u002f11\\u002f20\",\"11\\u002f12\\u002f20\",\"11\\u002f13\\u002f20\",\"11\\u002f14\\u002f20\",\"11\\u002f15\\u002f20\",\"11\\u002f16\\u002f20\",\"11\\u002f17\\u002f20\",\"11\\u002f18\\u002f20\",\"11\\u002f19\\u002f20\",\"11\\u002f20\\u002f20\",\"11\\u002f21\\u002f20\",\"11\\u002f22\\u002f20\",\"11\\u002f23\\u002f20\",\"11\\u002f24\\u002f20\",\"11\\u002f25\\u002f20\",\"11\\u002f26\\u002f20\",\"11\\u002f27\\u002f20\",\"11\\u002f28\\u002f20\",\"11\\u002f29\\u002f20\",\"11\\u002f30\\u002f20\",\"12\\u002f1\\u002f20\",\"12\\u002f2\\u002f20\",\"12\\u002f3\\u002f20\",\"12\\u002f4\\u002f20\",\"12\\u002f5\\u002f20\",\"12\\u002f6\\u002f20\",\"12\\u002f7\\u002f20\",\"12\\u002f8\\u002f20\",\"12\\u002f9\\u002f20\",\"12\\u002f10\\u002f20\",\"12\\u002f11\\u002f20\",\"12\\u002f12\\u002f20\",\"12\\u002f13\\u002f20\",\"12\\u002f14\\u002f20\",\"12\\u002f15\\u002f20\",\"12\\u002f16\\u002f20\",\"12\\u002f17\\u002f20\",\"12\\u002f18\\u002f20\",\"12\\u002f19\\u002f20\",\"12\\u002f20\\u002f20\",\"12\\u002f21\\u002f20\",\"12\\u002f22\\u002f20\",\"12\\u002f23\\u002f20\",\"12\\u002f24\\u002f20\",\"12\\u002f25\\u002f20\",\"12\\u002f26\\u002f20\",\"12\\u002f27\\u002f20\",\"12\\u002f28\\u002f20\",\"12\\u002f29\\u002f20\",\"12\\u002f30\\u002f20\",\"12\\u002f31\\u002f20\",\"1\\u002f1\\u002f21\",\"1\\u002f2\\u002f21\",\"1\\u002f3\\u002f21\",\"1\\u002f4\\u002f21\",\"1\\u002f5\\u002f21\",\"1\\u002f6\\u002f21\",\"1\\u002f7\\u002f21\",\"1\\u002f8\\u002f21\",\"1\\u002f9\\u002f21\",\"1\\u002f10\\u002f21\",\"1\\u002f11\\u002f21\",\"1\\u002f12\\u002f21\",\"1\\u002f13\\u002f21\",\"1\\u002f14\\u002f21\",\"1\\u002f15\\u002f21\",\"1\\u002f16\\u002f21\",\"1\\u002f17\\u002f21\",\"1\\u002f18\\u002f21\",\"1\\u002f19\\u002f21\",\"1\\u002f20\\u002f21\",\"1\\u002f21\\u002f21\",\"1\\u002f22\\u002f21\",\"1\\u002f23\\u002f21\",\"1\\u002f24\\u002f21\",\"1\\u002f25\\u002f21\",\"1\\u002f26\\u002f21\",\"1\\u002f27\\u002f21\",\"1\\u002f28\\u002f21\",\"1\\u002f29\\u002f21\",\"1\\u002f30\\u002f21\",\"1\\u002f31\\u002f21\",\"2\\u002f1\\u002f21\",\"2\\u002f2\\u002f21\",\"2\\u002f3\\u002f21\",\"2\\u002f4\\u002f21\",\"2\\u002f5\\u002f21\",\"2\\u002f6\\u002f21\",\"2\\u002f7\\u002f21\",\"2\\u002f8\\u002f21\",\"2\\u002f9\\u002f21\",\"2\\u002f10\\u002f21\",\"2\\u002f11\\u002f21\",\"2\\u002f12\\u002f21\",\"2\\u002f13\\u002f21\",\"2\\u002f14\\u002f21\",\"2\\u002f15\\u002f21\",\"2\\u002f16\\u002f21\",\"2\\u002f17\\u002f21\",\"2\\u002f18\\u002f21\",\"2\\u002f19\\u002f21\",\"2\\u002f20\\u002f21\",\"2\\u002f21\\u002f21\",\"2\\u002f22\\u002f21\",\"2\\u002f23\\u002f21\",\"2\\u002f24\\u002f21\",\"2\\u002f25\\u002f21\",\"2\\u002f26\\u002f21\",\"2\\u002f27\\u002f21\",\"2\\u002f28\\u002f21\",\"3\\u002f1\\u002f21\",\"3\\u002f2\\u002f21\",\"3\\u002f3\\u002f21\",\"3\\u002f4\\u002f21\",\"3\\u002f5\\u002f21\",\"3\\u002f6\\u002f21\",\"3\\u002f7\\u002f21\",\"3\\u002f8\\u002f21\",\"3\\u002f9\\u002f21\",\"3\\u002f10\\u002f21\",\"3\\u002f11\\u002f21\",\"3\\u002f12\\u002f21\",\"3\\u002f13\\u002f21\",\"3\\u002f14\\u002f21\",\"3\\u002f15\\u002f21\",\"3\\u002f16\\u002f21\",\"3\\u002f17\\u002f21\",\"3\\u002f18\\u002f21\",\"3\\u002f19\\u002f21\",\"3\\u002f20\\u002f21\",\"3\\u002f21\\u002f21\",\"3\\u002f22\\u002f21\",\"3\\u002f23\\u002f21\",\"3\\u002f24\\u002f21\",\"3\\u002f25\\u002f21\",\"3\\u002f26\\u002f21\",\"3\\u002f27\\u002f21\",\"3\\u002f28\\u002f21\",\"3\\u002f29\\u002f21\",\"3\\u002f30\\u002f21\",\"3\\u002f31\\u002f21\",\"4\\u002f1\\u002f21\",\"4\\u002f2\\u002f21\",\"4\\u002f3\\u002f21\",\"4\\u002f4\\u002f21\",\"4\\u002f5\\u002f21\",\"4\\u002f6\\u002f21\",\"4\\u002f7\\u002f21\",\"4\\u002f8\\u002f21\",\"4\\u002f9\\u002f21\",\"4\\u002f10\\u002f21\",\"4\\u002f11\\u002f21\",\"4\\u002f12\\u002f21\",\"4\\u002f13\\u002f21\",\"4\\u002f14\\u002f21\",\"4\\u002f15\\u002f21\",\"4\\u002f16\\u002f21\",\"4\\u002f17\\u002f21\",\"4\\u002f18\\u002f21\",\"4\\u002f19\\u002f21\",\"4\\u002f20\\u002f21\",\"4\\u002f21\\u002f21\",\"4\\u002f22\\u002f21\",\"4\\u002f23\\u002f21\",\"4\\u002f24\\u002f21\",\"4\\u002f25\\u002f21\",\"4\\u002f26\\u002f21\",\"4\\u002f27\\u002f21\",\"4\\u002f28\\u002f21\",\"4\\u002f29\\u002f21\",\"4\\u002f30\\u002f21\",\"5\\u002f1\\u002f21\",\"5\\u002f2\\u002f21\",\"5\\u002f3\\u002f21\",\"5\\u002f4\\u002f21\",\"5\\u002f5\\u002f21\",\"5\\u002f6\\u002f21\",\"5\\u002f7\\u002f21\",\"5\\u002f8\\u002f21\",\"5\\u002f9\\u002f21\",\"5\\u002f10\\u002f21\",\"5\\u002f11\\u002f21\",\"5\\u002f12\\u002f21\",\"5\\u002f13\\u002f21\",\"5\\u002f14\\u002f21\",\"5\\u002f15\\u002f21\",\"5\\u002f16\\u002f21\",\"5\\u002f17\\u002f21\",\"5\\u002f18\\u002f21\",\"5\\u002f19\\u002f21\",\"5\\u002f20\\u002f21\",\"5\\u002f21\\u002f21\",\"5\\u002f22\\u002f21\",\"5\\u002f23\\u002f21\",\"5\\u002f24\\u002f21\",\"5\\u002f25\\u002f21\",\"5\\u002f26\\u002f21\",\"5\\u002f27\\u002f21\",\"5\\u002f28\\u002f21\",\"5\\u002f29\\u002f21\",\"5\\u002f30\\u002f21\",\"5\\u002f31\\u002f21\",\"6\\u002f1\\u002f21\",\"6\\u002f2\\u002f21\",\"6\\u002f3\\u002f21\",\"6\\u002f4\\u002f21\",\"6\\u002f5\\u002f21\",\"6\\u002f6\\u002f21\",\"6\\u002f7\\u002f21\",\"6\\u002f8\\u002f21\",\"6\\u002f9\\u002f21\",\"6\\u002f10\\u002f21\",\"6\\u002f11\\u002f21\",\"6\\u002f12\\u002f21\",\"6\\u002f13\\u002f21\",\"6\\u002f14\\u002f21\",\"6\\u002f15\\u002f21\",\"6\\u002f16\\u002f21\",\"6\\u002f17\\u002f21\",\"6\\u002f18\\u002f21\",\"6\\u002f19\\u002f21\",\"6\\u002f20\\u002f21\",\"6\\u002f21\\u002f21\",\"6\\u002f22\\u002f21\",\"6\\u002f23\\u002f21\",\"6\\u002f24\\u002f21\",\"6\\u002f25\\u002f21\",\"6\\u002f26\\u002f21\",\"6\\u002f27\\u002f21\",\"6\\u002f28\\u002f21\",\"6\\u002f29\\u002f21\",\"6\\u002f30\\u002f21\",\"7\\u002f1\\u002f21\",\"7\\u002f2\\u002f21\",\"7\\u002f3\\u002f21\",\"7\\u002f4\\u002f21\",\"7\\u002f5\\u002f21\",\"7\\u002f6\\u002f21\",\"7\\u002f7\\u002f21\",\"7\\u002f8\\u002f21\",\"7\\u002f9\\u002f21\",\"7\\u002f10\\u002f21\",\"7\\u002f11\\u002f21\",\"7\\u002f12\\u002f21\",\"7\\u002f13\\u002f21\",\"7\\u002f14\\u002f21\",\"7\\u002f15\\u002f21\",\"7\\u002f16\\u002f21\",\"7\\u002f17\\u002f21\",\"7\\u002f18\\u002f21\",\"7\\u002f19\\u002f21\",\"7\\u002f20\\u002f21\",\"7\\u002f21\\u002f21\",\"7\\u002f22\\u002f21\",\"7\\u002f23\\u002f21\",\"7\\u002f24\\u002f21\",\"7\\u002f25\\u002f21\",\"7\\u002f26\\u002f21\",\"7\\u002f27\\u002f21\",\"7\\u002f28\\u002f21\",\"7\\u002f29\\u002f21\",\"7\\u002f30\\u002f21\",\"7\\u002f31\\u002f21\",\"8\\u002f1\\u002f21\",\"8\\u002f2\\u002f21\",\"8\\u002f3\\u002f21\",\"8\\u002f4\\u002f21\",\"8\\u002f5\\u002f21\",\"8\\u002f6\\u002f21\",\"8\\u002f7\\u002f21\",\"8\\u002f8\\u002f21\",\"8\\u002f9\\u002f21\",\"8\\u002f10\\u002f21\",\"8\\u002f11\\u002f21\",\"8\\u002f12\\u002f21\",\"8\\u002f13\\u002f21\",\"8\\u002f14\\u002f21\",\"8\\u002f15\\u002f21\",\"8\\u002f16\\u002f21\",\"8\\u002f17\\u002f21\",\"8\\u002f18\\u002f21\",\"8\\u002f19\\u002f21\",\"8\\u002f20\\u002f21\",\"8\\u002f21\\u002f21\",\"8\\u002f22\\u002f21\",\"8\\u002f23\\u002f21\",\"8\\u002f24\\u002f21\",\"8\\u002f25\\u002f21\",\"8\\u002f26\\u002f21\",\"8\\u002f27\\u002f21\",\"8\\u002f28\\u002f21\",\"8\\u002f29\\u002f21\",\"8\\u002f30\\u002f21\",\"8\\u002f31\\u002f21\",\"9\\u002f1\\u002f21\",\"9\\u002f2\\u002f21\",\"9\\u002f3\\u002f21\",\"9\\u002f4\\u002f21\",\"9\\u002f5\\u002f21\",\"9\\u002f6\\u002f21\",\"9\\u002f7\\u002f21\",\"9\\u002f8\\u002f21\",\"9\\u002f9\\u002f21\",\"9\\u002f10\\u002f21\"],\"xaxis\":\"x\",\"y\":[621.65,609.67,610.92,608.82,610.38,609.11,607.04,611.58,610.19,608.66,598.88,597.42,594.08,603.88,601.74,598.98,605.96,605.67,603.85,609.39,614.82,612.98,611.85,609.62,607.18,612.08,617.21,614.74,615.65,617.54,614.77,635.01,635.96,634.02,637.94,641.42,638.97,636.29,629.25,627.72,631.92,655.48,653.25,651.39,655.31,651.45,682.22,687.68,685.91,698.0,702.0,697.01,733.33,686.17,683.69,704.79,712.0,709.96,708.97,720.93,721.5,715.45,703.71,702.28,706.46,710.91,711.73,736.96,747.52,748.98,729.06,738.53,736.97,741.63,737.45,735.64,733.67,727.96,727.31,732.71,742.69,742.05,772.43,764.33,762.97,754.63,756.62,758.99,769.72,770.02,769.08,777.0,777.0,777.99,774.89,776.75,775.88,788.7,788.4,788.67,793.09,824.21,834.97,901.31,891.61,892.6,897.33,930.37,930.34,963.38,952.15,958.12,997.72,1015.97,1013.42,1126.76,994.67,999.65,896.83,908.14,910.49,906.05,785.22,780.92,826.29,817.91,819.55,830.5,903.84,906.6,895.798875,893.6210875,895.64,918.603625,922.0736125,919.97,893.045625,915.95625,915.05,920.31225,915.933,912.19,964.706075,979.703875,983.42,1013.027,1030.999413,1034.07,1024.01375,1050.11,1052.84,976.103,999.1035,997.59,1000.604625,999.877375,1001.2,1012.325988,1035.208125,1035.2,1056.637138,1052.779286,1050.87,1123.788429,1123.223188,1130.01,1174.86625,1150.605714,1150.37,1190.75195,1187.565286,1190.89,1259.410817,1285.14,1287.0,1270.9333,1275.197375,1279.5,1157.3933,1192.469143,1192.09,1179.159875,1227.494625,1227.68,1245.370786,1257.399625,1257.0,1091.171888,952.2323625,967.69,1049.084488,1118.630043,1113.0,1038.789,941.9197143,927.93,956.7863125,1037.22925,1041.04,1040.5755,1037.90455,1034.74,1086.929571,1099.169125,1079.99,1141.600363,1133.079314,1133.53,1190.45425,1181.149838,1184.03,1207.744875,1226.617038,1219.09,1180.023713,1185.260057,1170.36,1186.927413,1205.634875,1176.97,1217.930088,1241.686325,1236.63,1261.311225,1257.988113,1240.84,1279.414688,1309.109875,1288.02,1331.294429,1334.979038,1333.0,1417.172813,1452.076288,1447.55,1508.292125,1533.335071,1518.75,1535.868429,1640.619225,1651.1,1762.88625,1820.990563,1829.0,1771.920013,1776.3165,1780.0,1739.031975,1807.485063,1783.98,1961.520488,2052.909788,2008.84,2090.662313,2287.710288,2256.32,2387.206286,2211.976857,2279.82,2192.9808,2275.9307,2290.02,2285.933914,2399.242671,2409.93,2525.765158,2516.173143,2529.53,2883.313697,2664.920863,2683.03,2827.4913,2845.372857,2898.63,2657.675063,2748.185086,2706.0,2442.48025,2464.959814,2477.94,2507.389252,2617.210263,2600.0,2671.04325,2727.288013,2713.48,2589.164888,2512.366286,2502.03,2517.903114,2585.349186,2558.3,2477.641375,2501.191343,2421.26,2561.225429,2599.729838,2598.89,2609.96775,2491.201214,2502.86,2536.238938,2366.170143,2327.09,2385.748571,2354.783417,2338.49,2058.9956,1931.2143,1910.96,2320.12225,2264.7657,2265.21,2682.195363,2807.609857,2825.51,2751.821029,2560.997917,2564.3,2647.625,2781.636583,2784.8,2745.955417,2866.431667,2862.9,2693.633983,2794.117717,2790.0,3218.115017,3252.562533,3210.2,3457.374333,3357.326317,3340.28,3632.506667,3852.802914,3868.52,4282.992,4217.028329,4179.97,4328.725717,4130.440067,4104.71,4157.958033,4043.722,3986.28,4174.95,4340.316717,4331.77,4360.513317,4354.308333,4339.05,4607.98545,4594.98785,4582.52,4911.740017,4580.38748,4585.57,4344.098317,4488.72014,4385.02,4654.6585,4310.750183,4312.26,4329.955,4248.090017,4198.59,3961.271267,3319.63,3216.43,3763.62604,3746.060783,3678.53,3943.413333,3977.561667,3894.48,3637.50255,3776.3869,3777.62,3942.555,3910.307383,3881.0,4201.98905,4193.574667,4164.27,4360.722967,4386.88375,4394.07,4225.175,4338.852,4318.58,4376.191667,4602.280883,4605.66,4782.28,4819.485767,4822.17,5563.806567,5739.438733,5825.99,5711.205867,5603.71294,5594.66,5727.6335,5979.45984,5986.44,5983.18455,5876.079867,5900.59,5669.622533,5893.138417,5889.82,5776.69695,6155.43402,6136.49,6388.645167,6665.306683,6751.98,7197.72006,7437.543317,7365.99,6989.071667,7092.127233,7128.64,7158.03706,6719.39785,6569.22,5716.301583,6550.227533,6493.63,7301.42992,7815.0307,7868.77,7817.140383,8007.654067,8049.72,8059.8,8268.035,8236.72,8250.978333,8707.407267,8768.8,9718.29505,9952.50882,9919.0,10147.372,10883.912,10879.88,11332.622,11584.83,11591.22,13540.98,16501.97167,16860.88,15142.83415,14869.805,15068.95,17276.39333,16808.36667,16252.31,17771.9,19498.68333,19279.9,18961.85667,17737.11167,17382.94,16047.51,15190.945,13776.61,13949.175,14119.02833,13740.94,15589.32167,14380.58167,14214.01,13215.574,14165.575,13796.0,15005.85667,15053.26167,15039.24,17174.12,17319.198,17155.95,15265.90667,14714.25333,14437.42,13296.794,13912.882,13791.19,13852.92,14012.196,13554.14,11116.94667,11345.42333,11174.82,12950.79333,11505.228,11513.42,11223.064,11282.25833,11392.03,10969.815,11524.77667,11431.37,11212.655,10184.06167,10082.52,9083.258333,8901.901667,8852.73,8400.648333,6838.816667,6925.46,8099.958333,8240.536667,8245.08,8319.876566,8343.455,8068.02,8597.7675,9334.633333,9471.64,10127.16167,10841.99167,11085.83,11110.965,11390.39167,11245.98,9931.071667,10162.11667,10171.3,9696.593333,10348.60333,10319.46,10370.165,11009.38167,10931.12,11326.94833,11430.18167,11516.83,10763.19833,10118.058,9920.46,9089.278333,8746.002,8777.37,9182.843333,9154.7,9153.54,8358.121667,8530.402,8297.89,8171.415,8412.033333,8605.64,8947.753333,8690.408333,8725.37,8662.378333,8617.296667,8457.96,7876.195,7960.38,7950.61,6882.531667,6935.48,6937.56,7035.848333,7410.435,7426.48,6826.51,6603.876667,6634.68,7017.656667,6699.273333,6784.41,6926.266667,7847.845,7941.46,8036.511051,8340.748333,8368.1,7895.416926,8164.937426,8175.96,8852.718333,8807.205,8930.6,8933.861667,9555.542,9658.11,9258.398333,9010.32,8928.55,9334.281667,9259.57,9239.55,9221.426,9639.268333,9753.35,9803.306667,9630.136277,9631.44,9228.608333,9322.041667,9310.37,8468.788,8484.346667,8463.52,8652.038333,8511.458,8471.06,8106.118333,8240.055,8239.08,8507.406667,8385.556187,8396.63,7555.74,7566.193333,7576.78,7342.908333,7361.831667,7344.56,7459.876667,7385.395,7386.72,7535.146667,7645.146667,7637.14,7500.273333,7615.722142,7619.43,7676.271667,7620.153333,7615.54,6776.888333,6875.678333,6879.31,6315.7,6647.013333,6637.41,6509.426667,6464.411667,6445.79,6737.0,6714.718333,6760.83,6332.573333,6141.605833,6170.46,6211.4475,6218.595,6097.46,6107.896154,5908.7025,6223.28,6374.754167,6466.069167,6626.44,6593.289167,6603.376667,6555.51,6594.281667,6753.559167,6724.04,6510.791667,6377.363333,6403.18,6244.3575,6241.0,6248.84,6514.390833,6869.910833,7321.62,7396.401667,7428.045,7334.99,7451.289167,7689.884167,7715.1,8251.165,8187.324167,7939.81,8182.251667,8206.341667,8220.91,7916.800833,7570.869167,7607.7,7394.499167,7247.769167,7011.28,6988.079167,6993.513333,6718.23,6396.7725,6396.494667,6138.96,6311.131667,6347.07,6252.13,6362.676923,6342.629231,6312.75,6436.720833,6404.063333,6486.58,6401.246154,6575.229167,6354.57,6543.645714,6719.429231,6738.27,6719.266154,7000.04,7078.19,6932.6625,6981.946154,7018.78,7247.935385,7260.949231,7265.08,7113.069231,6433.271667,6501.19,6366.1075,6286.425833,6240.98,6296.320833,6273.1375,6326.04,6499.0625,6518.655,6522.39,6400.600833,6296.631667,6342.39,6418.562667,6669.990833,6756.12,6710.445,6639.304167,6583.45,6468.631667,6535.476667,6686.08,6550.474167,6593.135,6611.02,6562.641667,6470.4025,6483.73,6568.549167,6581.486667,6573.62,6618.567692,6621.711667,6626.85,6248.635833,6260.530833,6247.0,6299.399167,6452.571667,6599.17,6596.276154,6568.040769,6509.61,6488.825833,6531.601667,6504.78,6481.426,6508.31,6489.77,6473.753333,6465.9175,6457.21,6382.668333,6309.452857,6301.61,6342.280833,6387.674167,6390.42,6391.873333,6436.965,6437.76,6538.79,6486.251667,6444.97,6399.033333,6378.268333,6408.53,6372.063333,6176.155,5756.77,5596.1925,5558.243333,5572.44,5303.9425,4671.97,4491.98,4548.7975,4309.3375,4352.98,3823.511667,3920.536667,3793.35,4103.453846,4263.783333,4278.77,4116.7775,4167.546667,4140.6,3961.493333,3858.349167,3744.98,3405.643333,3435.34,3453.66,3523.96,3426.19,3407.28,3406.7625,3278.374167,3242.42,3271.238333,3392.405,3556.54,3808.0425,3982.850833,4135.19,3910.973333,4027.478333,3995.17,3813.88,3825.379167,3848.21,3743.905,3912.285833,3770.36,3791.545833,3752.271667,3855.88,3865.7975,3822.626667,3856.62,3920.456667,4036.093333,4032.25,4034.138333,3812.585833,3646.25,3646.345833,3599.841667,3531.96,3656.785,3621.270833,3621.43,3632.395,3690.523333,3703.98,3548.4275,3540.02,3587.35,3566.4,3586.25,3577.79,3576.3,3554.5,3448.58,3421.6,3470.0,3444.81,3475.33,3506.22,3456.02,3452.35,3467.34,3405.7,3394.76,3661.08,3664.38,3686.29,3629.47,3632.42,3614.5,3600.31,3608.2,3619.55,3670.74,3910.8,3926.53,3987.18,3944.88,3983.48,4139.6,3766.89,3848.33,3831.45,3833.23,3850.08,3834.62,3831.44,3814.58,3730.98,3872.25,3876.85,3886.82,3875.96,3950.56,3928.17,3881.09,3892.51,3877.77,3885.21,3936.5,4015.95,3994.92,3998.77,4029.11,4056.4,3992.34,3999.06,4011.92,3994.11,3935.89,3947.74,4048.51,4037.37,4115.55,4114.44,4114.16,4152.53,4882.88,4959.81,4911.24,5028.77,5053.52,5189.39,5268.71,5175.81,5310.18,5035.02,5072.85,5064.62,5152.51,5036.18,5196.65,5220.37,5276.31,5277.04,5309.28,5281.83,5377.19,5518.16,5434.19,5195.61,5292.83,5280.19,5301.29,5260.65,5269.79,5326.67,5390.16,5657.14,5771.08,5717.66,5684.47,5755.72,5936.72,6146.91,6348.02,7248.31,6978.63,7823.11,7992.69,8206.9,7885.96,7362.22,7265.05,8193.7,8007.03,7952.5,7625.93,7880.29,8003.26,8071.45,8744.42,8770.06,8719.88,8662.98,8272.46,8553.81,8554.8,8739.03,8134.92,7675.8,7786.04,7803.23,7998.29,7931.34,7640.61,8020.38,7917.58,8173.06,8237.88,8693.6,8859.47,8978.99,9327.48,9076.18,9281.7,9531.35,10209.38,10668.63,10814.48,11017.5,11766.4,12932.55,11132.85,12374.35,11890.38,10737.73,10578.72,10805.4,11984.76,11160.49,10955.16,11232.04,11477.01,12313.08,12586.78,12099.9,11352.87,11803.97,11389.1,10186.96,10873.5,9397.72,9674.28,10638.56,10535.75,10764.9,10587.41,10323.62,9844.3,9772.17,9875.17,9848.65,9473.99,9530.0,9501.33,9589.13,10084.7,10407.17,10529.55,10814.57,10980.23,11787.99,11465.67,11960.82,11996.41,11856.64,11282.22,11566.84,11386.26,10858.12,10016.96,10302.17,10359.44,10214.52,10317.6,10917.26,10760.56,10129.4,10111.98,10405.81,10143.8,10135.06,10360.28,10171.95,9717.82,9484.55,9577.99,9600.9,9769.79,10386.64,10621.29,10584.16,10577.8,10317.47,10487.21,10406.31,10313.66,10101.03,10159.32,10420.16,10363.9,10361.33,10310.43,10265.63,10190.36,10157.59,10275.88,10173.11,9979.49,10033.05,9683.38,8553.61,8432.23,8055.64,8193.9,8225.0,8056.74,8307.74,8322.92,8382.03,8236.17,8155.48,8147.69,7869.74,8212.02,8190.0,8587.92,8586.9,8269.73,8308.01,8283.76,8353.54,8162.16,8002.51,8076.78,7954.15,7960.04,8231.06,8222.52,8026.76,7469.56,7431.88,8666.79,9259.8,9551.54,9218.76,9433.35,9164.62,9147.98,9252.99,9301.18,9206.16,9418.05,9310.19,9343.34,9204.24,8766.04,8809.41,9037.12,8717.81,8801.52,8762.42,8632.32,8457.69,8482.7,8503.93,8175.99,8120.8,8081.81,7617.07,7286.35,7324.03,6907.4,7130.25,7163.63,7523.83,7431.0,7757.47,7557.72,7402.69,7309.59,7296.77,7192.85,7395.97,7547.19,7504.83,7522.39,7337.42,7220.76,7202.31,7189.16,7251.87,7067.74,7111.14,6879.54,6612.12,7284.29,7150.86,7190.17,7143.2,7514.41,7322.08,7250.69,7192.72,7194.4,7243.93,7301.07,7385.36,7219.6,7168.31,7175.68,6944.33,7326.35,7347.89,7351.57,7759.24,8165.47,8042.65,7817.92,8184.66,8021.49,8173.97,8105.24,8842.42,8813.89,8722.03,8900.34,8910.85,8703.36,8626.47,8722.26,8658.94,8388.11,8428.17,8327.36,8588.42,8895.78,9385.69,9279.81,9502.37,9333.77,9378.09,9314.56,9162.14,9162.14,9614.9,9755.66,9807.54,9907.12,10162.41,9854.79,10275.38,10354.3,10242.43,10368.53,9904.17,9937.67,9703.93,10180.65,9604.72,9606.86,9696.58,9669.63,9989.39,9663.75,9309.15,8785.52,8804.72,8712.35,8534.17,8912.82,8912.82,8754.34,8758.9,9067.39,9155.89,8898.63,8039.38,7931.94,7885.46,7936.65,4830.21,5609.03,5166.26,5348.44,5026.35,5357.61,5410.23,6195.2,6226.44,6189.85,5822.62,6502.16,6768.49,6698.46,6763.75,6369.09,6260.95,5885.41,6405.29,6428.28,6652.87,6809.11,6741.67,6871.69,6777.44,7343.2,7205.55,7365.03,7293.69,6873.24,6891.6,6915.37,6857.66,6871.95,6623.8,7112.27,7034.89,7259.36,7130.04,6840.24,6852.52,7130.99,7477.6,7507.08,7549.52,7699.27,7790.66,7765.33,8777.63,8628.77,8824.66,8973.82,8903.95,8885.93,9030.96,9170.79,10002.48,9821.8,9527.39,8754.46,8617.25,8814.53,9305.93,9790.31,9303.59,9385.7,9669.4,9719.37,9785.74,9510.67,9057.57,9167.26,9178.32,8730.73,8899.66,8842.85,9197.54,9569.21,9425.98,9698.1,9450.84,10204.23,9525.57,9658.04,9794.56,9623.75,9670.43,9753.85,9782.59,9775.15,9892.13,9286.42,9459.97,9473.5,9330.07,9426.7,9526.0,9454.81,9380.03,9300.15,9357.43,9284.78,9691.9,9621.49,9276.58,9240.85,9154.45,9004.23,9127.47,9185.35,9133.97,9236.38,9087.98,9072.42,9131.31,9089.09,9348.91,9256.23,9440.07,9238.04,9287.4,9235.96,9295.9,9238.13,9254.52,9193.51,9131.0,9154.19,9174.71,9214.66,9163.87,9392.66,9537.4,9613.11,9551.28,9707.5,9938.83,11042.4,10934.94,11102.67,11114.93,11343.88,11823.69,11077.77,11242.57,11194.25,11750.28,11772.94,11605.6,11767.6,11684.06,11893.03,11392.43,11573.11,11777.43,11774.38,11873.98,11914.01,12293.72,11969.53,11734.0,11865.82,11522.8,11683.44,11653.02,11763.93,11337.4,11467.37,11302.01,11534.75,11481.64,11707.78,11659.57,11923.25,11397.44,10187.51,10467.89,10159.62,10254.93,10367.74,10121.52,10227.83,10352.66,10395.44,10446.44,10330.77,10674.64,10785.62,10948.43,10943.89,10931.79,11081.43,10919.65,10430.46,10532.22,10234.48,10732.43,10692.84,10732.4,10774.24,10692.33,10840.8,10777.92,10619.24,10575.06,10551.77,10673.46,10788.56,10603.74,10670.8,10923.3,11063.19,11302.67,11376.61,11540.04,11428.24,11431.32,11503.73,11327.57,11366.51,11508.2,11758.16,11925.46,12831.56,12990.25,12944.52,13128.46,13036.77,13076.37,13651.47,13289.0,13458.66,13564.72,13810.32,13758.88,13575.17,14023.31,14155.59,15591.39,15595.77,14839.84,15490.6,15328.53,15317.04,15708.65,16295.57,16339.33,16091.07,15968.16,16725.15,17679.72,17798.45,17820.57,18687.45,18699.75,18422.28,18398.91,19172.52,18739.8,17151.44,17138.87,17732.42,18191.6,19709.73,18792.52,19226.97,19454.54,18670.49,19155.06,19377.66,19181.41,18318.87,18554.15,18247.76,18029.36,18803.44,19164.48,19276.59,19439.75,21379.48,22847.46,23150.79,23869.92,23490.58,22745.48,23824.99,23253.37,23715.53,24693.58,26443.21,26246.58,27036.69,27376.37,28856.59,28982.56,29393.75,32195.46,33000.78,32035.03,34046.67,36860.41,39486.04,40670.25,40240.72,38240.09,35544.94,34011.82,37393.13,39158.47,36828.52,36065.2,35793.01,36632.35,36020.13,35538.98,30797.88,33002.38,32099.74,32276.84,32243.26,32541.8,30419.17,33403.17,34314.26,34318.1,33136.46,33522.9,35529.66,37676.25,37002.09,38278.61,39323.26,38928.1,46364.3,46589.58,44878.17,48013.38,47471.4,47185.19,48720.37,47951.85,49160.1,52118.23,51608.15,55916.5,56001.2,57487.86,54123.4,48880.43,50624.84,46800.42,46340.31,46155.87,45113.92,49618.43,48356.04,50477.7,48448.91,48861.38,48881.59,51169.7,52299.33,54881.52,55997.23,57764.0,57253.28,61258.73,59133.47,55754.72,56872.38,58913.0,57665.9,58075.1,58085.8,57411.17,54204.96,54477.46,52508.23,51415.92,55074.47,55863.93,55783.71,57627.67,58730.13,58735.25,58736.92,59031.32,57076.49,58206.55,59054.1,58020.46,55947.27,58048.59,58102.58,59774.0,59964.87,59834.74,63554.44,62969.12,63252.63,61455.98,60087.09,56251.48,55703.14,56507.91,53808.8,51731.71,51153.13,50110.53,49075.58,54056.64,55071.46,54884.1,53584.15,57796.62,57857.5,56610.46,57213.33,53241.72,57473.23,56428.16,57380.27,58928.81,58280.73,55883.5,56750.0,49007.09,49702.27,49922.52,46736.58,46441.64,43596.24,42912.19,36964.27,40784.32,37280.35,37528.3,34754.54,38728.59,38410.5,39266.04,38445.29,35689.62,34647.67,35684.59,37310.54,36662.64,37585.24,39188.59,36885.51,35530.38,35816.17,33514.87,33450.19,37338.36,36704.57,37313.18,35494.9,39066.82,40525.8,40188.56,38324.87,38068.04,35729.82,35524.17,35592.35,31686.55,32447.59,33674.66,34639.38,31640.58,32160.91,34644.45,34456.67,35847.7,35047.36,33536.88,33856.86,34688.98,35309.3,33747.97,34211.01,33839.04,32877.41,33818.52,33515.57,34227.64,33158.25,32686.56,32814.61,31738.59,31421.25,31520.66,31783.49,30815.94,29790.24,32118.06,32297.89,33581.63,34279.34,35365.2,37318.14,39405.95,40002.53,40005.93,42214.15,41659.06,40000.46,39193.94,38138.0,39750.14,40882.0,42825.95,44634.13,43816.14,46333.46,45608.37,45611.46,44417.78,47833.98,47112.19,47056.41,45982.55,44648.57,44777.86,46734.65,49327.75,48932.02,49335.68,49523.5,47744.58,48972.09,46962.8,49056.86,48897.65,48806.78,47074.77,47155.87,48862.76,49329.01,50035.33,49947.38,51769.06,52677.4,46809.17,46078.38,46368.69],\"yaxis\":\"y\",\"type\":\"scattergl\",\"opacity\":0.8}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Date\"},\"showgrid\":false},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Value\"},\"showgrid\":false},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"Whole period of timeframe of Bitcoin close price 2016-2021\"},\"font\":{\"size\":15,\"color\":\"black\"},\"plot_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('273ccfd8-0c8e-4227-aa1d-d16797c753e0');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig = px.line(df, x=df.Date, y=df.Value,labels={'date':'Date','close':'Close Stock'})\n",
        "fig.update_traces(marker_line_width=2, opacity=0.8, marker_line_color='orange')\n",
        "fig.update_layout(title_text='Whole period of timeframe of Bitcoin close price 2016-2021', plot_bgcolor='white',\n",
        "                  font_size=15, font_color='black')\n",
        "fig.update_xaxes(showgrid=False)\n",
        "fig.update_yaxes(showgrid=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu5kNL7qaUw8",
        "outputId": "92bee2a1-5104-42e0-df34-e743ac3fae21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total data for prediction:  384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-2a6f11495b79>:1: UserWarning:\n",
            "\n",
            "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "closedf = df[df['Date'] > '8/22/20']\n",
        "close_stock = closedf.copy()\n",
        "print(\"Total data for prediction: \",closedf.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFsFKHiEyd2W"
      },
      "source": [
        "### cut_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KHIRzcp9gXB"
      },
      "outputs": [],
      "source": [
        "def cut_df(df, start_date, end_date):\n",
        "    mask = (df['Date'] >= start_date) & (df['Date'] <= end_date)\n",
        "    #print(mask)\n",
        "    return df.loc[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "u2kzVS_f94QU",
        "outputId": "b77976a1-223f-4c7d-a5c6-880e6feaa5fc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"cut_df(df, '01/01/17', '01/01/20')\",\n  \"rows\": 1095,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2017-01-01 00:00:00\",\n        \"max\": \"2019-12-31 00:00:00\",\n        \"num_unique_values\": 1095,\n        \"samples\": [\n          \"2018-06-18 00:00:00\",\n          \"2017-05-20 00:00:00\",\n          \"2017-03-30 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3518.260806844108,\n        \"min\": 780.92,\n        \"max\": 19498.68333,\n        \"num_unique_values\": 1095,\n        \"samples\": [\n          6445.79,\n          2052.909788,\n          1037.90455\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-88df1955-97cd-4a5e-b875-014f5cc47ed6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>997.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>2017-01-02</td>\n",
              "      <td>1015.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>2017-01-03</td>\n",
              "      <td>1013.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>2017-01-04</td>\n",
              "      <td>1126.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>2017-01-05</td>\n",
              "      <td>994.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1202</th>\n",
              "      <td>2019-12-27</td>\n",
              "      <td>7194.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1203</th>\n",
              "      <td>2019-12-28</td>\n",
              "      <td>7243.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1204</th>\n",
              "      <td>2019-12-29</td>\n",
              "      <td>7301.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1205</th>\n",
              "      <td>2019-12-30</td>\n",
              "      <td>7385.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1206</th>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>7219.60</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1095 rows  2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88df1955-97cd-4a5e-b875-014f5cc47ed6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-88df1955-97cd-4a5e-b875-014f5cc47ed6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-88df1955-97cd-4a5e-b875-014f5cc47ed6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6899f139-2536-404a-807e-cb5a37cfc0f4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6899f139-2536-404a-807e-cb5a37cfc0f4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6899f139-2536-404a-807e-cb5a37cfc0f4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "           Date    Value\n",
              "112  2017-01-01   997.72\n",
              "113  2017-01-02  1015.97\n",
              "114  2017-01-03  1013.42\n",
              "115  2017-01-04  1126.76\n",
              "116  2017-01-05   994.67\n",
              "...         ...      ...\n",
              "1202 2019-12-27  7194.40\n",
              "1203 2019-12-28  7243.93\n",
              "1204 2019-12-29  7301.07\n",
              "1205 2019-12-30  7385.36\n",
              "1206 2019-12-31  7219.60\n",
              "\n",
              "[1095 rows x 2 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cut_df(df, '01/01/17', '01/01/20').head(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Jr-SqgOAbZ5O",
        "outputId": "4bf73be3-63ce-4f88-d8f8-d596ae0ab82c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"closedf\",\n  \"rows\": 384,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2020-08-23 00:00:00\",\n        \"max\": \"2021-09-10 00:00:00\",\n        \"num_unique_values\": 384,\n        \"samples\": [\n          \"2021-05-18 00:00:00\",\n          \"2021-04-30 00:00:00\",\n          \"2021-08-13 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16228.202682245526,\n        \"min\": 10121.52,\n        \"max\": 63554.44,\n        \"num_unique_values\": 384,\n        \"samples\": [\n          43596.24,\n          53584.15,\n          44417.78\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "closedf"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-6ed6e3ae-3dfa-4760-992b-dbc3d15f9c28\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1442</th>\n",
              "      <td>2020-08-23</td>\n",
              "      <td>11683.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1443</th>\n",
              "      <td>2020-08-24</td>\n",
              "      <td>11653.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1444</th>\n",
              "      <td>2020-08-25</td>\n",
              "      <td>11763.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1445</th>\n",
              "      <td>2020-08-26</td>\n",
              "      <td>11337.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1446</th>\n",
              "      <td>2020-08-27</td>\n",
              "      <td>11467.37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ed6e3ae-3dfa-4760-992b-dbc3d15f9c28')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6ed6e3ae-3dfa-4760-992b-dbc3d15f9c28 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6ed6e3ae-3dfa-4760-992b-dbc3d15f9c28');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-04382c2e-16ce-445b-974e-66e4d6a420b0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-04382c2e-16ce-445b-974e-66e4d6a420b0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-04382c2e-16ce-445b-974e-66e4d6a420b0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "           Date     Value\n",
              "1442 2020-08-23  11683.44\n",
              "1443 2020-08-24  11653.02\n",
              "1444 2020-08-25  11763.93\n",
              "1445 2020-08-26  11337.40\n",
              "1446 2020-08-27  11467.37"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "closedf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "RLGPvVX7cUeY",
        "outputId": "389745b8-9c8c-43a0-cb47-a75ff0ba8295"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"230c3a46-af74-4119-bf9b-dda421289f33\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"230c3a46-af74-4119-bf9b-dda421289f33\")) {                    Plotly.newPlot(                        \"230c3a46-af74-4119-bf9b-dda421289f33\",                        [{\"hovertemplate\":\"Date=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"orange\",\"width\":2}},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[\"2020-08-23T00:00:00\",\"2020-08-24T00:00:00\",\"2020-08-25T00:00:00\",\"2020-08-26T00:00:00\",\"2020-08-27T00:00:00\",\"2020-08-28T00:00:00\",\"2020-08-29T00:00:00\",\"2020-08-30T00:00:00\",\"2020-08-31T00:00:00\",\"2020-09-01T00:00:00\",\"2020-09-02T00:00:00\",\"2020-09-03T00:00:00\",\"2020-09-04T00:00:00\",\"2020-09-05T00:00:00\",\"2020-09-06T00:00:00\",\"2020-09-07T00:00:00\",\"2020-09-08T00:00:00\",\"2020-09-09T00:00:00\",\"2020-09-10T00:00:00\",\"2020-09-11T00:00:00\",\"2020-09-12T00:00:00\",\"2020-09-13T00:00:00\",\"2020-09-14T00:00:00\",\"2020-09-15T00:00:00\",\"2020-09-16T00:00:00\",\"2020-09-17T00:00:00\",\"2020-09-18T00:00:00\",\"2020-09-19T00:00:00\",\"2020-09-20T00:00:00\",\"2020-09-21T00:00:00\",\"2020-09-22T00:00:00\",\"2020-09-23T00:00:00\",\"2020-09-24T00:00:00\",\"2020-09-25T00:00:00\",\"2020-09-26T00:00:00\",\"2020-09-27T00:00:00\",\"2020-09-28T00:00:00\",\"2020-09-29T00:00:00\",\"2020-09-30T00:00:00\",\"2020-10-01T00:00:00\",\"2020-10-02T00:00:00\",\"2020-10-03T00:00:00\",\"2020-10-04T00:00:00\",\"2020-10-05T00:00:00\",\"2020-10-06T00:00:00\",\"2020-10-07T00:00:00\",\"2020-10-08T00:00:00\",\"2020-10-09T00:00:00\",\"2020-10-10T00:00:00\",\"2020-10-11T00:00:00\",\"2020-10-12T00:00:00\",\"2020-10-13T00:00:00\",\"2020-10-14T00:00:00\",\"2020-10-15T00:00:00\",\"2020-10-16T00:00:00\",\"2020-10-17T00:00:00\",\"2020-10-18T00:00:00\",\"2020-10-19T00:00:00\",\"2020-10-20T00:00:00\",\"2020-10-21T00:00:00\",\"2020-10-22T00:00:00\",\"2020-10-23T00:00:00\",\"2020-10-24T00:00:00\",\"2020-10-25T00:00:00\",\"2020-10-26T00:00:00\",\"2020-10-27T00:00:00\",\"2020-10-28T00:00:00\",\"2020-10-29T00:00:00\",\"2020-10-30T00:00:00\",\"2020-10-31T00:00:00\",\"2020-11-01T00:00:00\",\"2020-11-02T00:00:00\",\"2020-11-03T00:00:00\",\"2020-11-04T00:00:00\",\"2020-11-05T00:00:00\",\"2020-11-06T00:00:00\",\"2020-11-07T00:00:00\",\"2020-11-08T00:00:00\",\"2020-11-09T00:00:00\",\"2020-11-10T00:00:00\",\"2020-11-11T00:00:00\",\"2020-11-12T00:00:00\",\"2020-11-13T00:00:00\",\"2020-11-14T00:00:00\",\"2020-11-15T00:00:00\",\"2020-11-16T00:00:00\",\"2020-11-17T00:00:00\",\"2020-11-18T00:00:00\",\"2020-11-19T00:00:00\",\"2020-11-20T00:00:00\",\"2020-11-21T00:00:00\",\"2020-11-22T00:00:00\",\"2020-11-23T00:00:00\",\"2020-11-24T00:00:00\",\"2020-11-25T00:00:00\",\"2020-11-26T00:00:00\",\"2020-11-27T00:00:00\",\"2020-11-28T00:00:00\",\"2020-11-29T00:00:00\",\"2020-11-30T00:00:00\",\"2020-12-01T00:00:00\",\"2020-12-02T00:00:00\",\"2020-12-03T00:00:00\",\"2020-12-04T00:00:00\",\"2020-12-05T00:00:00\",\"2020-12-06T00:00:00\",\"2020-12-07T00:00:00\",\"2020-12-08T00:00:00\",\"2020-12-09T00:00:00\",\"2020-12-10T00:00:00\",\"2020-12-11T00:00:00\",\"2020-12-12T00:00:00\",\"2020-12-13T00:00:00\",\"2020-12-14T00:00:00\",\"2020-12-15T00:00:00\",\"2020-12-16T00:00:00\",\"2020-12-17T00:00:00\",\"2020-12-18T00:00:00\",\"2020-12-19T00:00:00\",\"2020-12-20T00:00:00\",\"2020-12-21T00:00:00\",\"2020-12-22T00:00:00\",\"2020-12-23T00:00:00\",\"2020-12-24T00:00:00\",\"2020-12-25T00:00:00\",\"2020-12-26T00:00:00\",\"2020-12-27T00:00:00\",\"2020-12-28T00:00:00\",\"2020-12-29T00:00:00\",\"2020-12-30T00:00:00\",\"2020-12-31T00:00:00\",\"2021-01-01T00:00:00\",\"2021-01-02T00:00:00\",\"2021-01-03T00:00:00\",\"2021-01-04T00:00:00\",\"2021-01-05T00:00:00\",\"2021-01-06T00:00:00\",\"2021-01-07T00:00:00\",\"2021-01-08T00:00:00\",\"2021-01-09T00:00:00\",\"2021-01-10T00:00:00\",\"2021-01-11T00:00:00\",\"2021-01-12T00:00:00\",\"2021-01-13T00:00:00\",\"2021-01-14T00:00:00\",\"2021-01-15T00:00:00\",\"2021-01-16T00:00:00\",\"2021-01-17T00:00:00\",\"2021-01-18T00:00:00\",\"2021-01-19T00:00:00\",\"2021-01-20T00:00:00\",\"2021-01-21T00:00:00\",\"2021-01-22T00:00:00\",\"2021-01-23T00:00:00\",\"2021-01-24T00:00:00\",\"2021-01-25T00:00:00\",\"2021-01-26T00:00:00\",\"2021-01-27T00:00:00\",\"2021-01-28T00:00:00\",\"2021-01-29T00:00:00\",\"2021-01-30T00:00:00\",\"2021-01-31T00:00:00\",\"2021-02-01T00:00:00\",\"2021-02-02T00:00:00\",\"2021-02-03T00:00:00\",\"2021-02-04T00:00:00\",\"2021-02-05T00:00:00\",\"2021-02-06T00:00:00\",\"2021-02-07T00:00:00\",\"2021-02-08T00:00:00\",\"2021-02-09T00:00:00\",\"2021-02-10T00:00:00\",\"2021-02-11T00:00:00\",\"2021-02-12T00:00:00\",\"2021-02-13T00:00:00\",\"2021-02-14T00:00:00\",\"2021-02-15T00:00:00\",\"2021-02-16T00:00:00\",\"2021-02-17T00:00:00\",\"2021-02-18T00:00:00\",\"2021-02-19T00:00:00\",\"2021-02-20T00:00:00\",\"2021-02-21T00:00:00\",\"2021-02-22T00:00:00\",\"2021-02-23T00:00:00\",\"2021-02-24T00:00:00\",\"2021-02-25T00:00:00\",\"2021-02-26T00:00:00\",\"2021-02-27T00:00:00\",\"2021-02-28T00:00:00\",\"2021-03-01T00:00:00\",\"2021-03-02T00:00:00\",\"2021-03-03T00:00:00\",\"2021-03-04T00:00:00\",\"2021-03-05T00:00:00\",\"2021-03-06T00:00:00\",\"2021-03-07T00:00:00\",\"2021-03-08T00:00:00\",\"2021-03-09T00:00:00\",\"2021-03-10T00:00:00\",\"2021-03-11T00:00:00\",\"2021-03-12T00:00:00\",\"2021-03-13T00:00:00\",\"2021-03-14T00:00:00\",\"2021-03-15T00:00:00\",\"2021-03-16T00:00:00\",\"2021-03-17T00:00:00\",\"2021-03-18T00:00:00\",\"2021-03-19T00:00:00\",\"2021-03-20T00:00:00\",\"2021-03-21T00:00:00\",\"2021-03-22T00:00:00\",\"2021-03-23T00:00:00\",\"2021-03-24T00:00:00\",\"2021-03-25T00:00:00\",\"2021-03-26T00:00:00\",\"2021-03-27T00:00:00\",\"2021-03-28T00:00:00\",\"2021-03-29T00:00:00\",\"2021-03-30T00:00:00\",\"2021-03-31T00:00:00\",\"2021-04-01T00:00:00\",\"2021-04-02T00:00:00\",\"2021-04-03T00:00:00\",\"2021-04-04T00:00:00\",\"2021-04-05T00:00:00\",\"2021-04-06T00:00:00\",\"2021-04-07T00:00:00\",\"2021-04-08T00:00:00\",\"2021-04-09T00:00:00\",\"2021-04-10T00:00:00\",\"2021-04-11T00:00:00\",\"2021-04-12T00:00:00\",\"2021-04-13T00:00:00\",\"2021-04-14T00:00:00\",\"2021-04-15T00:00:00\",\"2021-04-16T00:00:00\",\"2021-04-17T00:00:00\",\"2021-04-18T00:00:00\",\"2021-04-19T00:00:00\",\"2021-04-20T00:00:00\",\"2021-04-21T00:00:00\",\"2021-04-22T00:00:00\",\"2021-04-23T00:00:00\",\"2021-04-24T00:00:00\",\"2021-04-25T00:00:00\",\"2021-04-26T00:00:00\",\"2021-04-27T00:00:00\",\"2021-04-28T00:00:00\",\"2021-04-29T00:00:00\",\"2021-04-30T00:00:00\",\"2021-05-01T00:00:00\",\"2021-05-02T00:00:00\",\"2021-05-03T00:00:00\",\"2021-05-04T00:00:00\",\"2021-05-05T00:00:00\",\"2021-05-06T00:00:00\",\"2021-05-07T00:00:00\",\"2021-05-08T00:00:00\",\"2021-05-09T00:00:00\",\"2021-05-10T00:00:00\",\"2021-05-11T00:00:00\",\"2021-05-12T00:00:00\",\"2021-05-13T00:00:00\",\"2021-05-14T00:00:00\",\"2021-05-15T00:00:00\",\"2021-05-16T00:00:00\",\"2021-05-17T00:00:00\",\"2021-05-18T00:00:00\",\"2021-05-19T00:00:00\",\"2021-05-20T00:00:00\",\"2021-05-21T00:00:00\",\"2021-05-22T00:00:00\",\"2021-05-23T00:00:00\",\"2021-05-24T00:00:00\",\"2021-05-25T00:00:00\",\"2021-05-26T00:00:00\",\"2021-05-27T00:00:00\",\"2021-05-28T00:00:00\",\"2021-05-29T00:00:00\",\"2021-05-30T00:00:00\",\"2021-05-31T00:00:00\",\"2021-06-01T00:00:00\",\"2021-06-02T00:00:00\",\"2021-06-03T00:00:00\",\"2021-06-04T00:00:00\",\"2021-06-05T00:00:00\",\"2021-06-06T00:00:00\",\"2021-06-07T00:00:00\",\"2021-06-08T00:00:00\",\"2021-06-09T00:00:00\",\"2021-06-10T00:00:00\",\"2021-06-11T00:00:00\",\"2021-06-12T00:00:00\",\"2021-06-13T00:00:00\",\"2021-06-14T00:00:00\",\"2021-06-15T00:00:00\",\"2021-06-16T00:00:00\",\"2021-06-17T00:00:00\",\"2021-06-18T00:00:00\",\"2021-06-19T00:00:00\",\"2021-06-20T00:00:00\",\"2021-06-21T00:00:00\",\"2021-06-22T00:00:00\",\"2021-06-23T00:00:00\",\"2021-06-24T00:00:00\",\"2021-06-25T00:00:00\",\"2021-06-26T00:00:00\",\"2021-06-27T00:00:00\",\"2021-06-28T00:00:00\",\"2021-06-29T00:00:00\",\"2021-06-30T00:00:00\",\"2021-07-01T00:00:00\",\"2021-07-02T00:00:00\",\"2021-07-03T00:00:00\",\"2021-07-04T00:00:00\",\"2021-07-05T00:00:00\",\"2021-07-06T00:00:00\",\"2021-07-07T00:00:00\",\"2021-07-08T00:00:00\",\"2021-07-09T00:00:00\",\"2021-07-10T00:00:00\",\"2021-07-11T00:00:00\",\"2021-07-12T00:00:00\",\"2021-07-13T00:00:00\",\"2021-07-14T00:00:00\",\"2021-07-15T00:00:00\",\"2021-07-16T00:00:00\",\"2021-07-17T00:00:00\",\"2021-07-18T00:00:00\",\"2021-07-19T00:00:00\",\"2021-07-20T00:00:00\",\"2021-07-21T00:00:00\",\"2021-07-22T00:00:00\",\"2021-07-23T00:00:00\",\"2021-07-24T00:00:00\",\"2021-07-25T00:00:00\",\"2021-07-26T00:00:00\",\"2021-07-27T00:00:00\",\"2021-07-28T00:00:00\",\"2021-07-29T00:00:00\",\"2021-07-30T00:00:00\",\"2021-07-31T00:00:00\",\"2021-08-01T00:00:00\",\"2021-08-02T00:00:00\",\"2021-08-03T00:00:00\",\"2021-08-04T00:00:00\",\"2021-08-05T00:00:00\",\"2021-08-06T00:00:00\",\"2021-08-07T00:00:00\",\"2021-08-08T00:00:00\",\"2021-08-09T00:00:00\",\"2021-08-10T00:00:00\",\"2021-08-11T00:00:00\",\"2021-08-12T00:00:00\",\"2021-08-13T00:00:00\",\"2021-08-14T00:00:00\",\"2021-08-15T00:00:00\",\"2021-08-16T00:00:00\",\"2021-08-17T00:00:00\",\"2021-08-18T00:00:00\",\"2021-08-19T00:00:00\",\"2021-08-20T00:00:00\",\"2021-08-21T00:00:00\",\"2021-08-22T00:00:00\",\"2021-08-23T00:00:00\",\"2021-08-24T00:00:00\",\"2021-08-25T00:00:00\",\"2021-08-26T00:00:00\",\"2021-08-27T00:00:00\",\"2021-08-28T00:00:00\",\"2021-08-29T00:00:00\",\"2021-08-30T00:00:00\",\"2021-08-31T00:00:00\",\"2021-09-01T00:00:00\",\"2021-09-02T00:00:00\",\"2021-09-03T00:00:00\",\"2021-09-04T00:00:00\",\"2021-09-05T00:00:00\",\"2021-09-06T00:00:00\",\"2021-09-07T00:00:00\",\"2021-09-08T00:00:00\",\"2021-09-09T00:00:00\",\"2021-09-10T00:00:00\"],\"xaxis\":\"x\",\"y\":[11683.44,11653.02,11763.93,11337.4,11467.37,11302.01,11534.75,11481.64,11707.78,11659.57,11923.25,11397.44,10187.51,10467.89,10159.62,10254.93,10367.74,10121.52,10227.83,10352.66,10395.44,10446.44,10330.77,10674.64,10785.62,10948.43,10943.89,10931.79,11081.43,10919.65,10430.46,10532.22,10234.48,10732.43,10692.84,10732.4,10774.24,10692.33,10840.8,10777.92,10619.24,10575.06,10551.77,10673.46,10788.56,10603.74,10670.8,10923.3,11063.19,11302.67,11376.61,11540.04,11428.24,11431.32,11503.73,11327.57,11366.51,11508.2,11758.16,11925.46,12831.56,12990.25,12944.52,13128.46,13036.77,13076.37,13651.47,13289.0,13458.66,13564.72,13810.32,13758.88,13575.17,14023.31,14155.59,15591.39,15595.77,14839.84,15490.6,15328.53,15317.04,15708.65,16295.57,16339.33,16091.07,15968.16,16725.15,17679.72,17798.45,17820.57,18687.45,18699.75,18422.28,18398.91,19172.52,18739.8,17151.44,17138.87,17732.42,18191.6,19709.73,18792.52,19226.97,19454.54,18670.49,19155.06,19377.66,19181.41,18318.87,18554.15,18247.76,18029.36,18803.44,19164.48,19276.59,19439.75,21379.48,22847.46,23150.79,23869.92,23490.58,22745.48,23824.99,23253.37,23715.53,24693.58,26443.21,26246.58,27036.69,27376.37,28856.59,28982.56,29393.75,32195.46,33000.78,32035.03,34046.67,36860.41,39486.04,40670.25,40240.72,38240.09,35544.94,34011.82,37393.13,39158.47,36828.52,36065.2,35793.01,36632.35,36020.13,35538.98,30797.88,33002.38,32099.74,32276.84,32243.26,32541.8,30419.17,33403.17,34314.26,34318.1,33136.46,33522.9,35529.66,37676.25,37002.09,38278.61,39323.26,38928.1,46364.3,46589.58,44878.17,48013.38,47471.4,47185.19,48720.37,47951.85,49160.1,52118.23,51608.15,55916.5,56001.2,57487.86,54123.4,48880.43,50624.84,46800.42,46340.31,46155.87,45113.92,49618.43,48356.04,50477.7,48448.91,48861.38,48881.59,51169.7,52299.33,54881.52,55997.23,57764.0,57253.28,61258.73,59133.47,55754.72,56872.38,58913.0,57665.9,58075.1,58085.8,57411.17,54204.96,54477.46,52508.23,51415.92,55074.47,55863.93,55783.71,57627.67,58730.13,58735.25,58736.92,59031.32,57076.49,58206.55,59054.1,58020.46,55947.27,58048.59,58102.58,59774.0,59964.87,59834.74,63554.44,62969.12,63252.63,61455.98,60087.09,56251.48,55703.14,56507.91,53808.8,51731.71,51153.13,50110.53,49075.58,54056.64,55071.46,54884.1,53584.15,57796.62,57857.5,56610.46,57213.33,53241.72,57473.23,56428.16,57380.27,58928.81,58280.73,55883.5,56750.0,49007.09,49702.27,49922.52,46736.58,46441.64,43596.24,42912.19,36964.27,40784.32,37280.35,37528.3,34754.54,38728.59,38410.5,39266.04,38445.29,35689.62,34647.67,35684.59,37310.54,36662.64,37585.24,39188.59,36885.51,35530.38,35816.17,33514.87,33450.19,37338.36,36704.57,37313.18,35494.9,39066.82,40525.8,40188.56,38324.87,38068.04,35729.82,35524.17,35592.35,31686.55,32447.59,33674.66,34639.38,31640.58,32160.91,34644.45,34456.67,35847.7,35047.36,33536.88,33856.86,34688.98,35309.3,33747.97,34211.01,33839.04,32877.41,33818.52,33515.57,34227.64,33158.25,32686.56,32814.61,31738.59,31421.25,31520.66,31783.49,30815.94,29790.24,32118.06,32297.89,33581.63,34279.34,35365.2,37318.14,39405.95,40002.53,40005.93,42214.15,41659.06,40000.46,39193.94,38138.0,39750.14,40882.0,42825.95,44634.13,43816.14,46333.46,45608.37,45611.46,44417.78,47833.98,47112.19,47056.41,45982.55,44648.57,44777.86,46734.65,49327.75,48932.02,49335.68,49523.5,47744.58,48972.09,46962.8,49056.86,48897.65,48806.78,47074.77,47155.87,48862.76,49329.01,50035.33,49947.38,51769.06,52677.4,46809.17,46078.38,46368.69],\"yaxis\":\"y\",\"type\":\"scatter\",\"opacity\":0.8}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Date\"},\"showgrid\":false},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Value\"},\"showgrid\":false},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"Considered period to predict Bitcoin close price\"},\"font\":{\"size\":15,\"color\":\"black\"},\"plot_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('230c3a46-af74-4119-bf9b-dda421289f33');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig = px.line(closedf, x=closedf.Date, y=closedf.Value,labels={'date':'Date','close':'Close Stock'})\n",
        "fig.update_traces(marker_line_width=2, opacity=0.8, marker_line_color='orange')\n",
        "fig.update_layout(title_text='Considered period to predict Bitcoin close price',\n",
        "                  plot_bgcolor='white', font_size=15, font_color='black')\n",
        "fig.update_xaxes(showgrid=False)\n",
        "fig.update_yaxes(showgrid=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJsNPSfLcwZo",
        "outputId": "b45196ad-7458-489a-941d-0ba6f8767475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(384, 1)\n"
          ]
        }
      ],
      "source": [
        "# deleting date column and normalizing using MinMax Scaler\n",
        "\n",
        "del closedf['Date']\n",
        "scaler=MinMaxScaler(feature_range=(0,1))\n",
        "closedf=scaler.fit_transform(np.array(closedf).reshape(-1,1))\n",
        "print(closedf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krPCk1hFc0HU",
        "outputId": "88168687-d885-4679-c815-a5bfc4d83bf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_data:  (230, 1)\n",
            "test_data:  (154, 1)\n"
          ]
        }
      ],
      "source": [
        "training_size=int(len(closedf)*0.60)\n",
        "test_size=len(closedf)-training_size\n",
        "train_data,test_data=closedf[0:training_size,:],closedf[training_size:len(closedf),:1]\n",
        "print(\"train_data: \", train_data.shape)\n",
        "print(\"test_data: \", test_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR2EB_c_yZOf"
      },
      "source": [
        "### Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JBmYaVoc3Bj"
      },
      "outputs": [],
      "source": [
        "def create_dataset(dataset, time_step=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-time_step-1):\n",
        "        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + time_step, 0])\n",
        "    return np.array(dataX), np.array(dataY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBEYkV6Ic5Ol",
        "outputId": "67b4aae3-e620-41de-a35d-0f3116df9d0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train:  (214, 15)\n",
            "y_train:  (214,)\n",
            "X_test:  (138, 15)\n",
            "y_test (138,)\n"
          ]
        }
      ],
      "source": [
        "time_step = 15\n",
        "X_train, y_train = create_dataset(train_data, time_step)\n",
        "X_test, y_test = create_dataset(test_data, time_step)\n",
        "\n",
        "print(\"X_train: \", X_train.shape)\n",
        "print(\"y_train: \", y_train.shape)\n",
        "print(\"X_test: \", X_test.shape)\n",
        "print(\"y_test\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPZm5wsFc8w4",
        "outputId": "06c962c9-f023-4fe9-d8c6-50192bb21e3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train:  (214, 15, 1)\n",
            "X_test:  (138, 15, 1)\n"
          ]
        }
      ],
      "source": [
        "# reshape input to be [samples, time steps, features] which is required for LSTM\n",
        "X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
        "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
        "\n",
        "print(\"X_train: \", X_train.shape)\n",
        "print(\"X_test: \", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J53o0y79dA_6",
        "outputId": "386ff864-656e-46fe-aad8-a110a39e48e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning:\n",
            "\n",
            "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model=Sequential()\n",
        "\n",
        "model.add(LSTM(10,input_shape=(None,1),activation=\"relu\"))\n",
        "\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLKJVpxvdDSK",
        "outputId": "479b824d-0733-4194-a391-b998952c1a81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.2300 - val_loss: 0.3364\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2039 - val_loss: 0.2996\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1875 - val_loss: 0.2655\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1657 - val_loss: 0.2343\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1391 - val_loss: 0.2051\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1290 - val_loss: 0.1771\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1137 - val_loss: 0.1496\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0924 - val_loss: 0.1240\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0801 - val_loss: 0.0991\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0688 - val_loss: 0.0744\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0600 - val_loss: 0.0508\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0379 - val_loss: 0.0296\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0259 - val_loss: 0.0132\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0193 - val_loss: 0.0053\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0143 - val_loss: 0.0067\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0146 - val_loss: 0.0078\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0141 - val_loss: 0.0059\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0129 - val_loss: 0.0054\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0125 - val_loss: 0.0055\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0109 - val_loss: 0.0057\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0110 - val_loss: 0.0061\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0110 - val_loss: 0.0064\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0108 - val_loss: 0.0066\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0094 - val_loss: 0.0066\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0098 - val_loss: 0.0068\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0093 - val_loss: 0.0070\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0090 - val_loss: 0.0070\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0089 - val_loss: 0.0073\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0095 - val_loss: 0.0075\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0084 - val_loss: 0.0074\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0087 - val_loss: 0.0075\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0088 - val_loss: 0.0075\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0080 - val_loss: 0.0075\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0076 - val_loss: 0.0078\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0076 - val_loss: 0.0076\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0080 - val_loss: 0.0075\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0072 - val_loss: 0.0077\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0074 - val_loss: 0.0079\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0074 - val_loss: 0.0075\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0075 - val_loss: 0.0075\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0075 - val_loss: 0.0075\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0073 - val_loss: 0.0076\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0069 - val_loss: 0.0073\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0070 - val_loss: 0.0072\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0068 - val_loss: 0.0072\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0069 - val_loss: 0.0070\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0065 - val_loss: 0.0071\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0069 - val_loss: 0.0072\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0067 - val_loss: 0.0068\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0055 - val_loss: 0.0067\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0062 - val_loss: 0.0067\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0065 - val_loss: 0.0066\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0065 - val_loss: 0.0065\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0059 - val_loss: 0.0061\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0059 - val_loss: 0.0061\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0053 - val_loss: 0.0062\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0054 - val_loss: 0.0059\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0053 - val_loss: 0.0054\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0051 - val_loss: 0.0059\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0053 - val_loss: 0.0058\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0052 - val_loss: 0.0048\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0050\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0047 - val_loss: 0.0051\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0045\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0046\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0045 - val_loss: 0.0045\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0041\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0042\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0040 - val_loss: 0.0036\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0041\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0035 - val_loss: 0.0035\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0032 - val_loss: 0.0038\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0033 - val_loss: 0.0036\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0031 - val_loss: 0.0037\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0031 - val_loss: 0.0037\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0025 - val_loss: 0.0034\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0037\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0037\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0029 - val_loss: 0.0036\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0025 - val_loss: 0.0036\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0035\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0027 - val_loss: 0.0035\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0038\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0034\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0037\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0025 - val_loss: 0.0035\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0022 - val_loss: 0.0041\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0023 - val_loss: 0.0033\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0039\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0026 - val_loss: 0.0037\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0038\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0020 - val_loss: 0.0036\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0037\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022 - val_loss: 0.0034\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0039\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0037\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0020 - val_loss: 0.0035\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0026 - val_loss: 0.0036\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0035\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0039\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0037\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022 - val_loss: 0.0036\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0038\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022 - val_loss: 0.0037\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0037\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0021 - val_loss: 0.0035\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0036\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0024 - val_loss: 0.0034\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0039\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0020 - val_loss: 0.0032\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0041\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0034\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0036\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022 - val_loss: 0.0037\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0018 - val_loss: 0.0033\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0019 - val_loss: 0.0035\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 0.0038\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0022 - val_loss: 0.0033\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0021 - val_loss: 0.0036\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0035\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0020 - val_loss: 0.0036\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0020 - val_loss: 0.0031\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0044\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0032\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0035\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0020 - val_loss: 0.0037\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0020 - val_loss: 0.0034\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0034\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019 - val_loss: 0.0038\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0021 - val_loss: 0.0034\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0033\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0021 - val_loss: 0.0033\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0037\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0020 - val_loss: 0.0035\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0034\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0021 - val_loss: 0.0033\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.0037\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0033\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0018 - val_loss: 0.0035\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.0039\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.0035\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0018 - val_loss: 0.0036\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0033\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0034\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0034\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0035\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0034\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0036\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0018 - val_loss: 0.0033\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0036\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0032\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0036\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0019 - val_loss: 0.0032\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0037\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0032\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0035\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0032\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0037\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0018 - val_loss: 0.0031\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0019 - val_loss: 0.0036\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0034\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0033\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022 - val_loss: 0.0032\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0019 - val_loss: 0.0037\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0033\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0035\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0032\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0018 - val_loss: 0.0035\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0018 - val_loss: 0.0032\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0017 - val_loss: 0.0033\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0034\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0031\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0037\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0034\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0033\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0033\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0033\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0034\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0019 - val_loss: 0.0033\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0032\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0035\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0032\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0017 - val_loss: 0.0034\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0033\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0017 - val_loss: 0.0032\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0034\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0033\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0019 - val_loss: 0.0034\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0031\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0035\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0031\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0033\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0035\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0014 - val_loss: 0.0030\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0022 - val_loss: 0.0038\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0018 - val_loss: 0.0030\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0033\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0033\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0031\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0016 - val_loss: 0.0036\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=200,batch_size=32,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "fxHpzqvLdSYL",
        "outputId": "0e3820e1-24e1-44cc-927d-6b9de3e3b8f8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGzCAYAAAAMr0ziAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABhBUlEQVR4nO3deVxU5eIG8GdmgBnWAQEZUGQT1xDKhczUfkmCmWUreiuX29V7KzUvWWbl3g1TM8tMb93r0mbWzWynlKRFccklTY3EUBQdNoVhH5h5f3+cZmQEgcFhZsDn+/mcz8yc88457+EMzMP7vuccmRBCgIiIiMiJyR1dASIiIqLmMLAQERGR02NgISIiIqfHwEJEREROj4GFiIiInB4DCxERETk9BhYiIiJyegwsRERE5PQYWIiIiMjpMbAQ2cikSZMQHh7eqvcuWLAAMpnMthVyMqdOnYJMJsOGDRvsut2MjAzIZDJkZGSY57X0WLVVncPDwzFp0iSbrrMlNmzYAJlMhlOnTtl920RXi4GFOjyZTNaiqf4XGtHV2rVrFxYsWICSkhJHV4WoQ3BxdAWI2to777xj8frtt9/Gtm3bGszv3bv3VW3nrbfegtFobNV7n3/+eTzzzDNXtX1quas5Vi21a9cuLFy4EJMmTYKvr6/FsqysLMjl/H+RyBoMLNThPfTQQxavd+/ejW3btjWYf7nKykp4eHi0eDuurq6tqh8AuLi4wMWFv472cjXHyhaUSqVDt0/UHjHiEwG45ZZbcN1112H//v0YNmwYPDw88OyzzwIAPv30U4wePRohISFQKpWIiorC4sWLYTAYLNZx+bgI0/iH5cuX480330RUVBSUSiUGDhyIffv2Wby3sTEsMpkM06ZNw9atW3HddddBqVSib9++SEtLa1D/jIwMDBgwACqVClFRUfj3v//d4nExP/74I+6//35069YNSqUSoaGh+Oc//4mqqqoG++fl5YW8vDyMHTsWXl5eCAwMxKxZsxr8LEpKSjBp0iSo1Wr4+vpi4sSJLeoa+fnnnyGTybBx48YGy7755hvIZDJ88cUXAIDTp0/jscceQ8+ePeHu7g5/f3/cf//9LRqf0dgYlpbW+fDhw5g0aRIiIyOhUqmg0Wjw17/+FcXFxeYyCxYswFNPPQUAiIiIMHc7murW2BiWP/74A/fffz86deoEDw8P3Hjjjfjyyy8typjG43z44Yf417/+ha5du0KlUmHEiBHIzs5udr+v5I033kDfvn2hVCoREhKCxx9/vMG+nzhxAvfeey80Gg1UKhW6du2KcePGobS01Fxm27ZtuPnmm+Hr6wsvLy/07NnT/HtEdLX4Lx3Rn4qLizFq1CiMGzcODz30EIKCggBIAxW9vLyQkpICLy8vfPfdd5g3bx50Oh2WLVvW7Hrff/99lJWV4e9//ztkMhmWLl2Ke+65B3/88Uez/+n/9NNP2LJlCx577DF4e3vjtddew7333ovc3Fz4+/sDAA4ePIikpCQEBwdj4cKFMBgMWLRoEQIDA1u03x999BEqKyvx6KOPwt/fH3v37sWqVatw9uxZfPTRRxZlDQYDEhMTER8fj+XLl2P79u14+eWXERUVhUcffRQAIITAXXfdhZ9++gn/+Mc/0Lt3b3zyySeYOHFis3UZMGAAIiMj8eGHHzYov3nzZvj5+SExMREAsG/fPuzatQvjxo1D165dcerUKaxZswa33HILjh07ZlXrmDV13rZtG/744w9MnjwZGo0GR48exZtvvomjR49i9+7dkMlkuOeee/D7779j06ZNeOWVVxAQEAAAVzwm+fn5uOmmm1BZWYkZM2bA398fGzduxJ133on//e9/uPvuuy3KL1myBHK5HLNmzUJpaSmWLl2KBx98EHv27GnxPpssWLAACxcuREJCAh599FFkZWVhzZo12LdvH3bu3AlXV1fo9XokJiaipqYG06dPh0ajQV5eHr744guUlJRArVbj6NGjuOOOO9CvXz8sWrQISqUS2dnZ2Llzp9V1ImqUILrGPP744+Lyj/7w4cMFALF27doG5SsrKxvM+/vf/y48PDxEdXW1ed7EiRNFWFiY+XVOTo4AIPz9/cWFCxfM8z/99FMBQHz++efmefPnz29QJwDCzc1NZGdnm+f98ssvAoBYtWqVed6YMWOEh4eHyMvLM887ceKEcHFxabDOxjS2f6mpqUImk4nTp09b7B8AsWjRIouy119/vejfv7/59datWwUAsXTpUvO8uro6MXToUAFArF+/vsn6zJkzR7i6ulr8zGpqaoSvr6/461//2mS9MzMzBQDx9ttvm+ft2LFDABA7duyw2Jf6x8qaOje23U2bNgkA4ocffjDPW7ZsmQAgcnJyGpQPCwsTEydONL+eOXOmACB+/PFH87yysjIREREhwsPDhcFgsNiX3r17i5qaGnPZV199VQAQR44cabCt+tavX29Rp4KCAuHm5iZGjhxp3oYQQrz++usCgFi3bp0QQoiDBw8KAOKjjz664rpfeeUVAUAUFhY2WQei1mKXENGflEolJk+e3GC+u7u7+XlZWRmKioowdOhQVFZW4rfffmt2vcnJyfDz8zO/Hjp0KACpC6A5CQkJiIqKMr/u168ffHx8zO81GAzYvn07xo4di5CQEHO57t27Y9SoUc2uH7Dcv4qKChQVFeGmm26CEAIHDx5sUP4f//iHxeuhQ4da7MtXX30FFxcXc4sLACgUCkyfPr1F9UlOTkZtbS22bNlinvftt9+ipKQEycnJjda7trYWxcXF6N69O3x9fXHgwIEWbas1da6/3erqahQVFeHGG28EAKu3W3/7gwYNws0332ye5+XlhalTp+LUqVM4duyYRfnJkyfDzc3N/Nqaz1R927dvh16vx8yZMy0GAU+ZMgU+Pj7mLim1Wg1A6parrKxsdF2mgcWffvppmw9opmsTAwvRn7p06WLxJWBy9OhR3H333VCr1fDx8UFgYKB5wG79/vsr6datm8VrU3i5ePGi1e81vd/03oKCAlRVVaF79+4NyjU2rzG5ubmYNGkSOnXqZB6XMnz4cAAN90+lUjXo1qhfH0AaWxIcHAwvLy+Lcj179mxRfWJjY9GrVy9s3rzZPG/z5s0ICAjArbfeap5XVVWFefPmITQ0FEqlEgEBAQgMDERJSUmLjkt91tT5woULeOKJJxAUFAR3d3cEBgYiIiICQMs+D1fafmPbMp25dvr0aYv5V/OZuny7QMP9dHNzQ2RkpHl5REQEUlJS8J///AcBAQFITEzE6tWrLfY3OTkZQ4YMwd/+9jcEBQVh3Lhx+PDDDxleyGY4hoXoT/X/czYpKSnB8OHD4ePjg0WLFiEqKgoqlQoHDhzA7NmzW/THWKFQNDpfCNGm720Jg8GA2267DRcuXMDs2bPRq1cveHp6Ii8vD5MmTWqwf1eqj60lJyfjX//6F4qKiuDt7Y3PPvsM48ePtziTavr06Vi/fj1mzpyJwYMHQ61WQyaTYdy4cW36JfnAAw9g165deOqppxAXFwcvLy8YjUYkJSXZ7cu5rT8XjXn55ZcxadIkfPrpp/j2228xY8YMpKamYvfu3ejatSvc3d3xww8/YMeOHfjyyy+RlpaGzZs349Zbb8W3335rt88OdVwMLERNyMjIQHFxMbZs2YJhw4aZ5+fk5DiwVpd07twZKpWq0TNEWnLWyJEjR/D7779j48aNmDBhgnn+tm3bWl2nsLAwpKeno7y83KLFIisrq8XrSE5OxsKFC/Hxxx8jKCgIOp0O48aNsyjzv//9DxMnTsTLL79snlddXd2qC7W1tM4XL15Eeno6Fi5ciHnz5pnnnzhxosE6rblycVhYWKM/H1OXY1hYWIvXZQ3TerOyshAZGWmer9frkZOTg4SEBIvyMTExiImJwfPPP49du3ZhyJAhWLt2LV544QUAgFwux4gRIzBixAisWLECL774Ip577jns2LGjwbqIrMUuIaImmP4rrP+fq16vxxtvvOGoKllQKBRISEjA1q1bce7cOfP87OxsfP311y16P2C5f0IIvPrqq62u0+233466ujqsWbPGPM9gMGDVqlUtXkfv3r0RExODzZs3Y/PmzQgODrYIjKa6X96isGrVqganWNuyzo39vABg5cqVDdbp6ekJAC0KULfffjv27t2LzMxM87yKigq8+eabCA8PR58+fVq6K1ZJSEiAm5sbXnvtNYt9+u9//4vS0lKMHj0aAKDT6VBXV2fx3piYGMjlctTU1ACQusouFxcXBwDmMkRXgy0sRE246aab4Ofnh4kTJ2LGjBmQyWR455132rTp3VoLFizAt99+iyFDhuDRRx+FwWDA66+/juuuuw6HDh1q8r29evVCVFQUZs2ahby8PPj4+ODjjz+2eixEfWPGjMGQIUPwzDPP4NSpU+jTpw+2bNli9fiO5ORkzJs3DyqVCo888kiDK8PecccdeOedd6BWq9GnTx9kZmZi+/bt5tO926LOPj4+GDZsGJYuXYra2lp06dIF3377baMtbv379wcAPPfccxg3bhxcXV0xZswYc5Cp75lnnsGmTZswatQozJgxA506dcLGjRuRk5ODjz/+uM2uihsYGIg5c+Zg4cKFSEpKwp133omsrCy88cYbGDhwoHms1nfffYdp06bh/vvvR48ePVBXV4d33nkHCoUC9957LwBg0aJF+OGHHzB69GiEhYWhoKAAb7zxBrp27WoxmJiotRhYiJrg7++PL774Ak8++SSef/55+Pn54aGHHsKIESPM1wNxtP79++Prr7/GrFmzMHfuXISGhmLRokU4fvx4s2cxubq64vPPPzePR1CpVLj77rsxbdo0xMbGtqo+crkcn332GWbOnIl3330XMpkMd955J15++WVcf/31LV5PcnIynn/+eVRWVlqcHWTy6quvQqFQ4L333kN1dTWGDBmC7du3t+q4WFPn999/H9OnT8fq1ashhMDIkSPx9ddfW5ylBQADBw7E4sWLsXbtWqSlpcFoNCInJ6fRwBIUFIRdu3Zh9uzZWLVqFaqrq9GvXz98/vnn5laOtrJgwQIEBgbi9ddfxz//+U906tQJU6dOxYsvvmi+TlBsbCwSExPx+eefIy8vDx4eHoiNjcXXX39tPkPqzjvvxKlTp7Bu3ToUFRUhICAAw4cPx8KFC81nGRFdDZlwpn8Vichmxo4di6NHjzY6voKIqL3hGBaiDuDyy+ifOHECX331FW655RbHVIiIyMbYwkLUAQQHB5vvb3P69GmsWbMGNTU1OHjwIKKjox1dPSKiq8YxLEQdQFJSEjZt2gStVgulUonBgwfjxRdfZFghog6DLSxERETk9DiGhYiIiJweAwsRERE5vVaNYVm9ejWWLVsGrVaL2NhYrFq1CoMGDWq07JYtW/Diiy8iOzsbtbW1iI6OxpNPPomHH37YXGbSpEnYuHGjxfsSExORlpbWovoYjUacO3cO3t7eVl0Om4iIiBxHCIGysjKEhIQ0f4FEYaUPPvhAuLm5iXXr1omjR4+KKVOmCF9fX5Gfn99o+R07dogtW7aIY8eOiezsbLFy5UqhUChEWlqauczEiRNFUlKSOH/+vHm6cOFCi+t05swZAYATJ06cOHHi1A6nM2fONPtdb/Wg2/j4eAwcOBCvv/46AKl1IzQ0FNOnT8czzzzTonXccMMNGD16NBYvXgxAamEpKSnB1q1bramKWWlpKXx9fXHmzBn4+Pi0ah1ERERkXzqdDqGhoSgpKWn2ishWdQnp9Xrs378fc+bMMc+Ty+VISEiwuGnXlQgh8N133yErKwsvvfSSxbKMjAx07twZfn5+uPXWW/HCCy9c8Z4gNTU1FjfTKisrAyDd54OBhYiIqH1pyXAOqwJLUVERDAYDgoKCLOYHBQU1ec+S0tJSdOnSBTU1NVAoFHjjjTdw2223mZcnJSXhnnvuQUREBE6ePIlnn30Wo0aNQmZmpvnuqPWlpqZi4cKF1lSdiIiI2jG7XDjO29sbhw4dQnl5OdLT05GSkoLIyEjzZcPHjRtnLhsTE4N+/fohKioKGRkZGDFiRIP1zZkzBykpKebXpiYlIiIi6pisCiwBAQFQKBTIz8+3mJ+fnw+NRnPF98nlcnTv3h0AEBcXh+PHjyM1NfWK9zmJjIxEQEAAsrOzGw0sSqUSSqXSmqoTERFRO2ZVYHFzc0P//v2Rnp6OsWPHApAG3aanp2PatGktXo/RaLQYg3K5s2fPori4GMHBwdZUj4iIbMRgMKC2ttbR1aAOQKFQwMXF5aovO2J1l1BKSgomTpyIAQMGYNCgQVi5ciUqKiowefJkAMCECRPQpUsXpKamApDGmwwYMABRUVGoqanBV199hXfeeQdr1qwBAJSXl2PhwoW49957odFocPLkSTz99NPo3r07EhMTr2rniIjIeuXl5Th79iysPImU6Io8PDwQHBwMNze3Vq/D6sCSnJyMwsJCzJs3D1qtFnFxcUhLSzMPxM3NzbW4+EtFRQUee+wxnD17Fu7u7ujVqxfeffddJCcnA5CS1+HDh7Fx40aUlJQgJCQEI0eOxOLFi9ntQ0RkZwaDAWfPnoWHhwcCAwN5MU66KkII6PV6FBYWIicnB9HR0c1fIO4KOsTND3U6HdRqNUpLS3laMxHRVaiurkZOTg7Cw8Ph7u7u6OpQB1FZWYnTp08jIiICKpXKPN+a72/eS4iIiBpgywrZUmtbVSzWYYN6EBEREbUpBhYiIiJyegwsREREjQgPD8fKlStbXD4jIwMymQwlJSVtVicA2LBhA3x9fdt0G86IgYWIiNo1mUzW5LRgwYJWrXffvn2YOnVqi8vfdNNNOH/+fLM38aPWscul+durykpgwQKgtBRYswawwZghIiKysfPnz5ufb968GfPmzUNWVpZ5npeXl/m5EAIGgwEuLs1//QUGBlpVDzc3tyav+k5Xh1/BTZDLgWXLgDffBP68ITQR0bVFCKCiwjFTC6+6odFozJNarYZMJjO//u233+Dt7Y2vv/4a/fv3h1KpxE8//YSTJ0/irrvuQlBQELy8vDBw4EBs377dYr2XdwnJZDL85z//wd133w0PDw9ER0fjs88+My+/vEvI1HXzzTffoHfv3vDy8kJSUpJFwKqrq8OMGTPg6+sLf39/zJ49GxMnTjRfTb6l1qxZg6ioKLi5uaFnz55455136h1CgQULFqBbt25QKpUICQnBjBkzzMvfeOMNREdHQ6VSISgoCPfdd59V27YXBpYmqFTSBABt3CVJROScKisBLy/HTJWVNtuNZ555BkuWLMHx48fRr18/lJeX4/bbb0d6ejoOHjyIpKQkjBkzBrm5uU2uZ+HChXjggQdw+PBh3H777XjwwQdx4cKFJn58lVi+fDneeecd/PDDD8jNzcWsWbPMy1966SW89957WL9+PXbu3AmdToetW7datW+ffPIJnnjiCTz55JP49ddf8fe//x2TJ0/Gjh07AAAff/wxXnnlFfz73//GiRMnsHXrVsTExAAAfv75Z8yYMQOLFi1CVlYW0tLSMGzYMKu2bzeiAygtLRUARGlpqc3XHRQkBCDEoUM2XzURkdOpqqoSx44dE1VVVdKM8nLpj6AjpvJyq+u/fv16oVarza937NghAIitW7c2+96+ffuKVatWmV+HhYWJV155xfwagHj++efNr8vLywUA8fXXX1ts6+LFi+a6ABDZ2dnm96xevVoEBQWZXwcFBYlly5aZX9fV1Ylu3bqJu+66q8X7eNNNN4kpU6ZYlLn//vvF7bffLoQQ4uWXXxY9evQQer2+wbo+/vhj4ePjI3Q63RW3ZwsNPld/sub7my0szTANxGYLCxFdkzw8gPJyx0weHjbbjQEDBli8Li8vx6xZs9C7d2/4+vrCy8sLx48fb7aFpV+/fubnnp6e8PHxQUFBwRXLe3h4ICoqyvw6ODjYXL60tBT5+fkYNGiQeblCoUD//v2t2rfjx49jyJAhFvOGDBmC48ePAwDuv/9+VFVVITIyElOmTMEnn3yCuro6AMBtt92GsLAwREZG4uGHH8Z7772HShu2bNkSA0szTIGltNSh1SAicgyZDPD0dMxkw6vtenp6WryeNWsWPvnkE7z44ov48ccfcejQIcTExECv1ze5HldX18t+PDIYjUarygs73xEnNDQUWVlZeOONN+Du7o7HHnsMw4YNQ21tLby9vXHgwAFs2rQJwcHBmDdvHmJjY9v81OzWYGBpBltYiIg6np07d2LSpEm4++67ERMTA41Gg1OnTtm1Dmq1GkFBQdi3b595nsFgwIEDB6xaT+/evbFz506LeTt37kSfPn3Mr93d3TFmzBi89tpryMjIQGZmJo4cOQIAcHFxQUJCApYuXYrDhw/j1KlT+O67765iz9oGT2tuhul0egYWIqKOIzo6Glu2bMGYMWMgk8kwd+7cJltK2sr06dORmpqK7t27o1evXli1ahUuXrxo1b2cnnrqKTzwwAO4/vrrkZCQgM8//xxbtmwxn/W0YcMGGAwGxMfHw8PDA++++y7c3d0RFhaGL774An/88QeGDRsGPz8/fPXVVzAajejZs2db7XKrMbA0g11CREQdz4oVK/DXv/4VN910EwICAjB79mzodDq712P27NnQarWYMGECFAoFpk6disTERCgUihavY+zYsXj11VexfPlyPPHEE4iIiMD69etxyy23AAB8fX2xZMkSpKSkwGAwICYmBp9//jn8/f3h6+uLLVu2YMGCBaiurkZ0dDQ2bdqEvn37ttEet55M2LszrQ1Yc3tqa82eDSxdCqSkAC+/bNNVExE5nerqauTk5CAiIgIq03UdyG6MRiN69+6NBx54AIsXL3Z0dWzmSp8ra76/2cLSDHYJERFRWzl9+jS+/fZbDB8+HDU1NXj99deRk5ODv/zlL46umtPhoNtmcNAtERG1Fblcjg0bNmDgwIEYMmQIjhw5gu3bt6N3796OrprTYQtLMziGhYiI2kpoaGiDM3yocWxhaQa7hIiIiByPgaUZ7BIiIiJyPAaWZjCwEBEROR4DSzPqj2Fp/yeAExERtU8MLM0wjWGpq7Ppnc6JiIjICgwszfD0BEwXHGS3EBERkWMwsDRDJuOpzURE14JbbrkFM2fONL8ODw/HypUrm3yPTCbD1q1br3rbtlpPUxYsWIC4uLg23UZbYmBpAZ7aTETkvMaMGYOkpKRGl/3444+QyWQ4fPiw1evdt28fpk6derXVs3Cl0HD+/HmMGjXKptvqaBhYWoBnChEROa9HHnkE27Ztw9mzZxssW79+PQYMGIB+/fpZvd7AwEB4eHjYoorN0mg0UCqVdtlWe8XA0gIMLER0rRICqKhwzNTSMzPvuOMOBAYGYsOGDRbzy8vL8dFHH+GRRx5BcXExxo8fjy5dusDDwwMxMTHYtGlTk+u9vEvoxIkTGDZsGFQqFfr06YNt27Y1eM/s2bPRo0cPeHh4IDIyEnPnzkVtbS0AYMOGDVi4cCF++eUXyGQyyGQyc50v7xI6cuQIbr31Vri7u8Pf3x9Tp05FeXm5efmkSZMwduxYLF++HMHBwfD398fjjz9u3lZLGI1GLFq0CF27doVSqURcXBzS0tLMy/V6PaZNm4bg4GCoVCqEhYUhNTUVACCEwIIFC9CtWzcolUqEhIRgxowZLd52a/DS/C1g6hLiGBYiutZUVgJeXo7Zdnm5dOJDc1xcXDBhwgRs2LABzz33HGQyGQDgo48+gsFgwPjx41FeXo7+/ftj9uzZ8PHxwZdffomHH34YUVFRGDRoULPbMBqNuOeeexAUFIQ9e/agtLTUYryLibe3NzZs2ICQkBAcOXIEU6ZMgbe3N55++mkkJyfj119/RVpaGrZv3w4AUJu+YOqpqKhAYmIiBg8ejH379qGgoAB/+9vfMG3aNItQtmPHDgQHB2PHjh3Izs5GcnIy4uLiMGXKlOZ/aABeffVVvPzyy/j3v/+N66+/HuvWrcOdd96Jo0ePIjo6Gq+99ho+++wzfPjhh+jWrRvOnDmDM2fOAAA+/vhjvPLKK/jggw/Qt29faLVa/PLLLy3abquJDqC0tFQAEKWlpW2y/smThQCEePHFNlk9EZHTqKqqEseOHRNVVVVCCCHKy6W/f46YystbXu/jx48LAGLHjh3meUOHDhUPPfTQFd8zevRo8eSTT5pfDx8+XDzxxBPm12FhYeKVV14RQgjxzTffCBcXF5GXl2de/vXXXwsA4pNPPrniNpYtWyb69+9vfj1//nwRGxvboFz99bz55pvCz89PlNf7AXz55ZdCLpcLrVYrhBBi4sSJIiwsTNTV1ZnL3H///SI5OfmKdbl82yEhIeJf//qXRZmBAweKxx57TAghxPTp08Wtt94qjEZjg3W9/PLLokePHkKv119xe/Vd/rkyseb7my0sLcAuISK6Vnl4SC0djtp2S/Xq1Qs33XQT1q1bh1tuuQXZ2dn48ccfsWjRIgCAwWDAiy++iA8//BB5eXnQ6/Woqalp8RiV48ePIzQ0FCEhIeZ5gwcPblBu8+bNeO2113Dy5EmUl5ejrq4OPj4+Ld+RP7cVGxsLz3rNS0OGDIHRaERWVhaCgoIAAH379oXCdN0NAMHBwThy5EiLtqHT6XDu3DkMGTLEYv6QIUPMLSWTJk3Cbbfdhp49eyIpKQl33HEHRo4cCQC4//77sXLlSkRGRiIpKQm33347xowZAxeXtosVHMPSAgwsRHStksmkbhlHTH/27LTYI488go8//hhlZWVYv349oqKiMHz4cADAsmXL8Oqrr2L27NnYsWMHDh06hMTEROj1epv9rDIzM/Hggw/i9ttvxxdffIGDBw/iueees+k26nN1dbV4LZPJYDQabbb+G264ATk5OVi8eDGqqqrwwAMP4L777gMg3WU6KysLb7zxBtzd3fHYY49h2LBhVo2hsRYDSwtwDAsRkfN74IEHIJfL8f777+Ptt9/GX//6V/N4lp07d+Kuu+7CQw89hNjYWERGRuL3339v8bp79+6NM2fO4Pz58+Z5u3fvtiiza9cuhIWF4bnnnsOAAQMQHR2N06dPW5Rxc3ODwWBodlu//PILKioqzPN27twJuVyOnj17trjOTfHx8UFISAh27txpMX/nzp3o06ePRbnk5GS89dZb2Lx5Mz7++GNcuHABAODu7o4xY8bgtddeQ0ZGBjIzM1vcwtMa7BJqAbawEBE5Py8vLyQnJ2POnDnQ6XSYNGmSeVl0dDT+97//YdeuXfDz88OKFSuQn59v8eXclISEBPTo0QMTJ07EsmXLoNPp8Nxzz1mUiY6ORm5uLj744AMMHDgQX375JT755BOLMuHh4cjJycGhQ4fQtWtXeHt7Nzid+cEHH8T8+fMxceJELFiwAIWFhZg+fToefvhhc3eQLTz11FOYP38+oqKiEBcXh/Xr1+PQoUN47733AAArVqxAcHAwrr/+esjlcnz00UfQaDTw9fXFhg0bYDAYEB8fDw8PD7z77rtwd3dHWFiYzep3ObawtAADCxFR+/DII4/g4sWLSExMtBhv8vzzz+OGG25AYmIibrnlFmg0GowdO7bF65XL5fjkk09QVVWFQYMG4W9/+xv+9a9/WZS588478c9//hPTpk1DXFwcdu3ahblz51qUuffee5GUlIT/+7//Q2BgYKOnVnt4eOCbb77BhQsXMHDgQNx3330YMWIEXn/9det+GM2YMWMGUlJS8OSTTyImJgZpaWn47LPPEB0dDUA642np0qUYMGAABg4ciFOnTuGrr76CXC6Hr68v3nrrLQwZMgT9+vXD9u3b8fnnn8Pf39+mdaxPJkT7vwexTqeDWq1GaWmp1YObWiIjA/i//wN69QKOH7f56omInEZ1dTVycnIQEREBlUrl6OpQB3Glz5U1399sYWkBtrAQERE5FgNLCzCwEBERORYDSwuYzhKqrgZqahxbFyIiomtRqwLL6tWrER4eDpVKhfj4eOzdu/eKZbds2YIBAwbA19cXnp6eiIuLwzvvvGNRRgiBefPmITg4GO7u7khISMCJEydaU7U2Ub9bjac2ExER2Z/VgWXz5s1ISUnB/PnzceDAAcTGxiIxMREFBQWNlu/UqROee+45ZGZm4vDhw5g8eTImT56Mb775xlxm6dKleO2117B27Vrs2bMHnp6eSExMRHV1dev3zIYUikuhhd1CRHQt6ADnY5ATscXnyeqzhOLj4zFw4EDz6VVGoxGhoaGYPn06nnnmmRat44YbbsDo0aOxePFiCCEQEhKCJ598ErNmzQIAlJaWIigoCBs2bMC4ceOaXV9bnyUEAGFhQG4usGcP0IL7ZBERtUu1tbXIzs5GSEhIozfmI2qN4uJiFBQUoEePHha3E7Dm+9uqC8fp9Xrs378fc+bMMc+Ty+VISEhAZmZms+8XQuC7775DVlYWXnrpJQBATk4OtFotEhISzOXUajXi4+ORmZnZaGCpqalBTb3BJDqdzprdaBXT7y1bWIioI3NxcYGHhwcKCwvh6uoKuZxDHan1hBCorKxEQUEBfH19LcKKtawKLEVFRTAYDA2utBcUFITffvvtiu8rLS1Fly5dUFNTA4VCgTfeeAO33XYbAECr1ZrXcfk6Tcsul5qaioULF1pT9avGM4WI6Fogk8kQHByMnJycBpeVJ2otX19faDSaq1qHXS7N7+3tjUOHDqG8vBzp6elISUlBZGQkbrnlllatb86cOUhJSTG/1ul0CA0NtVFtG+fnJz1evNimmyEicjg3NzdER0e32U376Nri6up6VS0rJlYFloCAACgUCuTn51vMz8/PbzI5yeVydO/eHQAQFxeH48ePIzU11Xx5ZNM6goODLdYZFxfX6PqUSmWDey+0NQYWIrqWyOVyXumWnIpVnZNubm7o378/0tPTzfOMRiPS09MxePDgFq/HaDSax6BERERAo9FYrFOn02HPnj1WrbOtMbAQERE5jtVdQikpKZg4cSIGDBiAQYMGYeXKlaioqMDkyZMBABMmTECXLl2QmpoKQBpvMmDAAERFRaGmpgZfffUV3nnnHaxZswaA1F86c+ZMvPDCC4iOjkZERATmzp2LkJAQq25M1dZMY1gYWIiIiOzP6sCSnJyMwsJCzJs3D1qtFnFxcUhLSzMPms3NzbUYVV5RUYHHHnsMZ8+ehbu7O3r16oV3330XycnJ5jJPP/00KioqMHXqVJSUlODmm29GWlqaUzVHsoWFiIjIcXi35hZ65x1gwgQgIQHYtq1NNkFERHRN4d2a2wBbWIiIiByHgaWFGFiIiIgch4GlhRhYiIiIHIeBpYVMgaWkBDAaHVoVIiKiaw4DSwuZAosQgB1uXURERET1MLC0kEolTQC7hYiIiOyNgcUKHMdCRETkGAwsVmBgISIicgwGFiswsBARETkGA4sVGFiIiIgcg4HFCgwsREREjsHAYgUGFiIiIsdgYLECAwsREZFjMLBYgYGFiIjIMRhYrMDAQkRE5BgMLFZgYCEiInIMBhYrMLAQERE5BgOLFRhYiIiIHIOBxQqmwFJSIt21mYiIiOyDgcUKpsBiNAJlZY6tCxER0bWEgcUK7u6AUik9Z7cQERGR/TCwWInjWIiIiOyPgcVKDCxERET2x8BiJQYWIiIi+2NgsRIDCxERkf0xsFiJgYWIiMj+GFisxMBCRERkfwwsVmJgISIisj8GFit16iQ9Xrjg2HoQERFdSxhYrGQKLMXFjq0HERHRtYSBxUr+/tIjW1iIiIjsh4HFSmxhISIisj8GFiuxhYWIiMj+GFisZGphKSsD9HrH1oWIiOhawcBiJV9fQCaTnvPUZiIiIvtgYLGSQiGFFoDjWIiIiOyFgaUVOI6FiIjIvhhYWoFnChEREdkXA0srsIWFiIjIvloVWFavXo3w8HCoVCrEx8dj7969Vyz71ltvYejQofDz84Ofnx8SEhIalJ80aRJkMpnFlJSU1Jqq2QVbWIiIiOzL6sCyefNmpKSkYP78+Thw4ABiY2ORmJiIgoKCRstnZGRg/Pjx2LFjBzIzMxEaGoqRI0ciLy/PolxSUhLOnz9vnjZt2tS6PbIDtrAQERHZl9WBZcWKFZgyZQomT56MPn36YO3atfDw8MC6desaLf/ee+/hscceQ1xcHHr16oX//Oc/MBqNSE9PtyinVCqh0WjMk5/ptshOiC0sRERE9mVVYNHr9di/fz8SEhIurUAuR0JCAjIzM1u0jsrKStTW1qKT6Vv/TxkZGejcuTN69uyJRx99FMVNpIGamhrodDqLyZ7YwkJERGRfVgWWoqIiGAwGBAUFWcwPCgqCVqtt0Tpmz56NkJAQi9CTlJSEt99+G+np6XjppZfw/fffY9SoUTAYDI2uIzU1FWq12jyFhoZasxtXjS0sRERE9uViz40tWbIEH3zwATIyMqBSqczzx40bZ34eExODfv36ISoqChkZGRgxYkSD9cyZMwcpKSnm1zqdzq6hhS0sRERE9mVVC0tAQAAUCgXy8/Mt5ufn50Oj0TT53uXLl2PJkiX49ttv0a9fvybLRkZGIiAgANnZ2Y0uVyqV8PHxsZjsydTCwsBCRERkH1YFFjc3N/Tv399iwKxpAO3gwYOv+L6lS5di8eLFSEtLw4ABA5rdztmzZ1FcXIzg4GBrqmc3phYWdgkRERHZh9VnCaWkpOCtt97Cxo0bcfz4cTz66KOoqKjA5MmTAQATJkzAnDlzzOVfeuklzJ07F+vWrUN4eDi0Wi20Wi3Ky8sBAOXl5Xjqqaewe/dunDp1Cunp6bjrrrvQvXt3JCYm2mg3bcvUwlJZCVRXO7YuRERE1wKrx7AkJyejsLAQ8+bNg1arRVxcHNLS0swDcXNzcyGXX8pBa9asgV6vx3333Wexnvnz52PBggVQKBQ4fPgwNm7ciJKSEoSEhGDkyJFYvHgxlErlVe5e21CrpZsgGgxSt1BIiKNrRERE1LHJhBDC0ZW4WjqdDmq1GqWlpXYbzxIYCBQVAUeOANddZ5dNEhERdSjWfH/zXkKtxHEsRERE9sPA0ko8U4iIiMh+GFhaiS0sRERE9sPA0kpsYSEiIrIfBpZWYgsLERGR/TCwtBJbWIiIiOyHgaWV2MJCRERkPwwsrcQWFiIiIvthYGkltrAQERHZDwNLKzGwEBER2Q8DSysFBEiPRUVA+7+5ARERkXNjYGklUwtLbS2g0zm2LkRERB0dA0sreXhIEyC1shAREVHbYWC5CoGB0iMDCxERUdtiYLkK9cexEBERUdthYLkKDCxERET2wcByFUyBpbDQsfUgIiLq6BhYmlJVBfz3v0BqaqOL2cJCRERkHy6OroBTMxqBv/1Nej5tGuDtbbGYg26JiIjsgy0sTfH0BHx8pOfnzjVYzBYWIiIi+2BgaU5IiPTIwEJEROQwDCzNCQ6WHs+fb7CIg26JiIjsg4GlOWxhISIicjgGluY0EVhMg24vXgTq6uxYJyIiomsMA0tzmugS6tRJehRCCi1ERETUNhhYmtNEC4uLC+DnJz1ntxAREVHbYWBpThOBBeDAWyIiIntgYGmOqUvo3Dmp7+cyHHhLRETU9hhYmmMKLJWVQFlZg8W82i0REVHbY2BpjqcnoFZLz3lqMxERkUMwsLRE/W6hyzCwEBERtT0GlpZowcXjOOiWiIio7TCwtIQpsDRxeX62sBAREbUdBpaWaKJLiINuiYiI2h4DS0vwfkJEREQOxcDSEuwSIiIicigGlpZowVlC5eVAdbUd60RERHQNYWBpifpdQpdd7VatBhQK6TlbWYiIiNoGA0tLmFpYqqqA0lKLRTIZu4WIiIjaGgNLS3h4AL6+0vNGxrHwTCEiIqK21arAsnr1aoSHh0OlUiE+Ph579+69Ytm33noLQ4cOhZ+fH/z8/JCQkNCgvBAC8+bNQ3BwMNzd3ZGQkIATJ060pmpth1e7JSIichirA8vmzZuRkpKC+fPn48CBA4iNjUViYiIKCgoaLZ+RkYHx48djx44dyMzMRGhoKEaOHIm8vDxzmaVLl+K1117D2rVrsWfPHnh6eiIxMRHVzjSK1TSOpV69TXi1WyIiorZldWBZsWIFpkyZgsmTJ6NPnz5Yu3YtPDw8sG7dukbLv/fee3jssccQFxeHXr164T//+Q+MRiPS09MBSK0rK1euxPPPP4+77roL/fr1w9tvv41z585h69atja6zpqYGOp3OYmpzXbtKj00EFrawEBERtQ2rAoter8f+/fuRkJBwaQVyORISEpCZmdmidVRWVqK2thadOnUCAOTk5ECr1VqsU61WIz4+/orrTE1NhVqtNk+hoaHW7EbrdOkiPZ4922ARAwsREVHbsiqwFBUVwWAwICgoyGJ+UFAQtFpti9Yxe/ZshISEmAOK6X3WrHPOnDkoLS01T2fOnLFmN1qniRYWDrolIiJqWy723NiSJUvwwQcfICMjAyqVqtXrUSqVUCqVNqxZC5gCC1tYiIiI7M6qFpaAgAAoFArk5+dbzM/Pz4dGo2nyvcuXL8eSJUvw7bffol+/fub5pve1Zp121YIuIQ66JSIiahtWBRY3Nzf079/fPGAWgHkA7eDBg6/4vqVLl2Lx4sVIS0vDgAEDLJZFRERAo9FYrFOn02HPnj1NrtPuTC0sBQWAXm+xiC0sREREbcvqLqGUlBRMnDgRAwYMwKBBg7By5UpUVFRg8uTJAIAJEyagS5cuSE1NBQC89NJLmDdvHt5//32Eh4ebx6V4eXnBy8sLMpkMM2fOxAsvvIDo6GhERERg7ty5CAkJwdixY223p1crIABwdQVqa6WLx4WFWSwCpMAihHT1WyIiIrIdqwNLcnIyCgsLMW/ePGi1WsTFxSEtLc08aDY3Nxdy+aWGmzVr1kCv1+O+++6zWM/8+fOxYMECAMDTTz+NiooKTJ06FSUlJbj55puRlpZ2VeNcbE4ul7qFTp2SBt42Elhqa4GyMsDHxzFVJCIi6qhkQlx2N792SKfTQa1Wo7S0FD5tmRaGDgV++gnYvBl44AGLRZ6eQGUlcPIkEBnZdlUgIiLqKKz5/ua9hKxhGnjLq90SERHZFQOLNXhqMxERkUMwsFiDV7slIiJyCAYWa/Bqt0RERA7BwGINdgkRERE5BAOLNUxdQufOAUajxSIOuiUiImo7DCzWCA6WrgpXW9sgmbCFhYiIqO0wsFjD1RUw3VX6snEsDCxERERth4HFWlcYx8JBt0RERG2HgcVaVwgsbGEhIiJqOwws1rrC1W5NgeXCBcBgsHOdiIiIOjgGFmuZWljOnLGY3amT9CiEFFqIiIjIdhhYrNWtm/R4WWBxdQV8faXn7BYiIiKyLQYWa4WGSo+5uQ0WceAtERFR22BgsZapheXs2StePI6BhYiIyLYYWKwVEgLI5YBeDxQUWCzi1W6JiIjaBgOLtVxdpSveAg26hUxdQgwsREREtsXA0hpXGHjLwEJERNQ2GFhawxRY2MJCRERkFwwsrWE6U4gtLERERHbBwNIabGEhIiKyKwaW1mBgISIisisGltZoQZeQEHauExERUQfGwNIaphYWrRaoqTHPNgUWvR4oK3NAvYiIiDooBpbW8PcH3N2l5/Xu2uzhIU0Au4WIiIhsiYGlNWSyK95TiONYiIiIbI+BpbU48JaIiMhuGFhai9diISIishsGltZiCwsREZHdMLC0FgMLERGR3TCwtBYH3RIREdkNA0trhYdLj6dPW1wljoGFiIjI9hhYWsvUJVRRARQXm2czsBAREdkeA0trKZVASIj0/NQp82wGFiIiIttjYLkapm4hBhYiIqI2xcByNUyBJSfHPMsUWCorpYmIiIiuHgPL1WikhcXbG3Bzk56zlYWIiMg2GFiuRkSE9FgvsMhk7BYiIiKyNQaWq9FICwvAwEJERGRrrQosq1evRnh4OFQqFeLj47F3794rlj169CjuvfdehIeHQyaTYeXKlQ3KLFiwADKZzGLq1atXa6pmX/UDC6/FQkRE1GasDiybN29GSkoK5s+fjwMHDiA2NhaJiYkoKChotHxlZSUiIyOxZMkSaDSaK663b9++OH/+vHn66aefrK2a/YWGSn1AlZVAUZF5timwXOFHQkRERFayOrCsWLECU6ZMweTJk9GnTx+sXbsWHh4eWLduXaPlBw4ciGXLlmHcuHFQKpVXXK+Liws0Go15CggIsLZq9lf/WiyNnCnEFhYiIiLbsCqw6PV67N+/HwkJCZdWIJcjISEBmZmZV1WREydOICQkBJGRkXjwwQeRe9k9euqrqamBTqezmByG12IhIiJqc1YFlqKiIhgMBgQFBVnMDwoKglarbXUl4uPjsWHDBqSlpWHNmjXIycnB0KFDUVZW1mj51NRUqNVq8xRquhGhIzQSWDp3lh7ZJURERGQbTnGW0KhRo3D//fejX79+SExMxFdffYWSkhJ8+OGHjZafM2cOSktLzdOZM2fsXON6Gjm1mYGFiIjItlysKRwQEACFQoH8/HyL+fn5+U0OqLWWr68vevTogezs7EaXK5XKJsfD2FUjLSymBqjLfkxERETUSla1sLi5uaF///5IT083zzMajUhPT8fgwYNtVqny8nKcPHkSwcHBNltnm2nk8vz1A0u9s52JiIiolaxqYQGAlJQUTJw4EQMGDMCgQYOwcuVKVFRUYPLkyQCACRMmoEuXLkhNTQUgDdQ9duyY+XleXh4OHToELy8vdO/eHQAwa9YsjBkzBmFhYTh37hzmz58PhUKB8ePH22o/287l12KRycyBpaYG0OkAtdpRlSMiIuoYrA4sycnJKCwsxLx586DVahEXF4e0tDTzQNzc3FzI5Zcabs6dO4frr7/e/Hr58uVYvnw5hg8fjoyMDADA2bNnMX78eBQXFyMwMBA333wzdu/ejUDT6TbOzHQtlupqqUlFo4GHB+DlBZSXS7MYWIiIiK6OTIj232mh0+mgVqtRWloKHx8f+1egWzfgzBlg1y7gz66x7t2BkyeBH34Ahg61f5WIiIicnTXf305xllC7FxUlPf7xh3kWB94SERHZDgOLLURGSo8nT5pnMbAQERHZDgOLLZgCC1tYiIiI2gQDiy2wS4iIiKhNMbDYAltYiIiI2hQDiy2YAkteHlBVBYCBhYiIyJYYWGzB3x8wnY715yX6TfcTYmAhIiK6egwstiCTNegWMrWw8AaIREREV4+BxVYuO7XZFFgqKqSJiIiIWo+BxVYuO1PI2xtQqaRZ7BYiIiK6OgwstnJZl5BMxoG3REREtsLAYiu82i0REVGbYWCxlfpdQn/eT5KBhYiIyDYYWGylWzdALgeqqwGtFgADCxERka0wsNiKq6sUWoAGZwoxsBAREV0dBhZbMnULMbAQERHZFAOLLTGwEBERtQkGFluKjpYeT5wAwMvzExER2QoDiy1dFljYwkJERGQbDCy21L279JidDQhhDiw6nXTyEBEREbUOA4stRUVJl7gtLQWKiuDnJ508BPAmiERERFeDgcWWVCogNFR6fuIEZDKOYyEiIrIFBhZbq98tBI5jISIisgUGFlvjwFsiIiKbY2CxNVMLCwMLERGRzTCw2JqphYVdQkRERDbDwGJr9buE6p3azMBCRETUegwsthYZKZ3arNMBhYUMLERERDbAwGJr9U9tzs5mYCEiIrIBBpa2UK9biNdhISIiunoMLG2h3plCphaWCxeA2lrHVYmIiKg9Y2BpC/XOFPL3B+R//pQLCx1XJSIiovaMgaUt1OsSUiiAwEDpJbuFiIiIWoeBpS3Uv3gcT20mIiK6agwsbcF0anNZGU9tJiIisgEGlragUgHduknP6w28ZWAhIiJqHQaWttLImUIMLERERK3DwNJW6p0pxMBCRER0dRhY2gpbWIiIiGymVYFl9erVCA8Ph0qlQnx8PPbu3XvFskePHsW9996L8PBwyGQyrFy58qrX2S7UO7WZgYWIiOjqWB1YNm/ejJSUFMyfPx8HDhxAbGwsEhMTUVBQ0Gj5yspKREZGYsmSJdBoNDZZZ7tQr0uoc6AAwMBCRETUWjIhhLDmDfHx8Rg4cCBef/11AIDRaERoaCimT5+OZ555psn3hoeHY+bMmZg5c6bN1gkAOp0OarUapaWl8PHxsWZ32k51NeDhAQiBc4cK0CUuEHI5oNcDCoWjK0dEROR41nx/W9XCotfrsX//fiQkJFxagVyOhIQEZGZmtqqyrVlnTU0NdDqdxeR06p3aHHjxdwCA0QgUFTmyUkRERO2TVYGlqKgIBoMBQaZBGX8KCgqCVqttVQVas87U1FSo1WrzFBoa2qptt7k/u4VcT52Av780i91CRERE1muXZwnNmTMHpaWl5unMmTOOrlLj6p0pZBq+w8BCRERkPRdrCgcEBEChUCD/sm/d/Pz8Kw6obYt1KpVKKJXKVm3PruqdKaTRAEePAq1siCIiIrqmWdXC4ubmhv79+yM9Pd08z2g0Ij09HYMHD25VBdpinU6j3plCpuzFwEJERGQ9q1pYACAlJQUTJ07EgAEDMGjQIKxcuRIVFRWYPHkyAGDChAno0qULUlNTAUiDao8dO2Z+npeXh0OHDsHLywvd/+wyaW6d7Vb9LqH/EwBkDCxEREStYHVgSU5ORmFhIebNmwetVou4uDikpaWZB83m5uZCLr/UcHPu3Dlcf/315tfLly/H8uXLMXz4cGRkZLRone1WZCQglwPl5dB4lQHw4RgWIiKiVrD6OizOyCmvw2ISEQGcOoV3n/8ND7/QEyNGANu3O7pSREREjtdm12GhVvizW0hTlQOAY1iIiIhag4Glrf058FZTmgWAgYWIiKg1GFjamqmFpeAwAKC4WLo8PxEREbUcA0tb+7OFpdPpg+Z7CLXnezoSERE5AgNLW/szsMizf0dQkDS+md1CRERE1mFgaWsREdKpzRUV0PjXAWBgISIishYDS1tTKs13bdZ4lgFgYCEiIrIWA4s9mM4UcikCwBsgEhERWYuBxR5MZwoZ8wCwhYWIiMhaDCz2YGphqfwDAAMLERGRtRhY7MEUWC7+BoCBhYiIyFoMLPZg6hLK/wUAAwsREZG1GFjs4c9TmzXVvJ8QERFRazCw2INSCYSHIwjS6UHl5dJERERELcPAYi99+sAbZXB3rQXAU5uJiIiswcBiL717QwZA414KgN1CRERE1mBgsZc+fQAAmj+7hRhYiIiIWo6BxV5MgaXmFAB2CREREVmDgcVeevcGAGhqTgNgCwsREZE1GFjsxdsbCA2FBlJSYWAhIiJqOQYWe+rTh4GFiIioFRhY7ImBhYiIqFUYWOyJgYWIiKhVGFjs6bLAIoSD60NERNROMLDYU+/e6IwCAEBtLXDxooPrQ0RE1E4wsNiTnx9UwZ3gCympsFuIiIioZRhY7K1etxAvHkdERNQyDCz2xoG3REREVmNgsTcGFiIiIqsxsNgbAwsREZHVGFjsrX5gOVvr4MoQERG1Dwws9hYQAI13JQBA+0elgytDRETUPjCwOIAmXAUA0OYZHFwTIiKi9oGBxQE0vXwBANpiV8dWhIiIqJ1gYHEATZwGAFBY6Ym6OgdXhoiIqB1gYHGAgIERkMMAATkKCx1dGyIiIufHwOIAipg+CISUVPJzaxxcGyIiIufHwOIIQUHQKKTAoj2Q5+DKEBEROT8GFkeQyS6d2nyYfUJERETNaVVgWb16NcLDw6FSqRAfH4+9e/c2Wf6jjz5Cr169oFKpEBMTg6+++spi+aRJkyCTySympKSk1lSt3dAESqc0a3/XObgmREREzs/qwLJ582akpKRg/vz5OHDgAGJjY5GYmIiCgoJGy+/atQvjx4/HI488goMHD2Ls2LEYO3Ysfv31V4tySUlJOH/+vHnatGlT6/aondCESqc0azmGhYiIqFlWB5YVK1ZgypQpmDx5Mvr06YO1a9fCw8MD69ata7T8q6++iqSkJDz11FPo3bs3Fi9ejBtuuAGvv/66RTmlUgmNRmOe/Pz8WrdH7YQm2hsAoC1grxwREVFzrPq21Ov12L9/PxISEi6tQC5HQkICMjMzG31PZmamRXkASExMbFA+IyMDnTt3Rs+ePfHoo4+iuLj4ivWoqamBTqezmNobTUwAAOCczhvQ6x1cGyIiIudmVWApKiqCwWBAUFCQxfygoCBor3DrYa1W22z5pKQkvP3220hPT8dLL72E77//HqNGjYLB0Pil61NTU6FWq81TaGioNbvhFLrE+AMA8hACZGc7uDZERETOzcXRFQCAcePGmZ/HxMSgX79+iIqKQkZGBkaMGNGg/Jw5c5CSkmJ+rdPp2l1o6RoqAwDkoQvE0S8g69PHwTUiIiJyXla1sAQEBEChUCA/P99ifn5+PjQaTaPv0Wg0VpUHgMjISAQEBCD7Ci0PSqUSPj4+FlN7ExIiPdZAhQv7cxxbGSIiIidnVWBxc3ND//79kZ6ebp5nNBqRnp6OwYMHN/qewYMHW5QHgG3btl2xPACcPXsWxcXFCA4OtqZ67YpSCQR4StdiyTvEa7EQERE1xepTVFJSUvDWW29h48aNOH78OB599FFUVFRg8uTJAIAJEyZgzpw55vJPPPEE0tLS8PLLL+O3337DggUL8PPPP2PatGkAgPLycjz11FPYvXs3Tp06hfT0dNx1113o3r07EhMTbbSbzqlL51oAwNnfyh1cEyIiIudm9RiW5ORkFBYWYt68edBqtYiLi0NaWpp5YG1ubi7k8ks56KabbsL777+P559/Hs8++yyio6OxdetWXHfddQAAhUKBw4cPY+PGjSgpKUFISAhGjhyJxYsXQ6lU2mg3nVPXcFf8kgPknRVAXR3g4hRDioiIiJyOTAghHF2Jq6XT6aBWq1FaWtquxrP8farAm2/JMB8LsCDrL0CPHo6uEhERkd1Y8/3Nq5Y5UJeu0plCZ9EVOH7cwbUhIiJyXgwsDtSli/SYhy7AsWOOrQwREZETY2BxoK5dpUcGFiIioqYxsDgQW1iIiIhahoHFgUyB5QL8UXUsB7jCrQiIiIiudQwsDuTrC7i7Sydp5VV3Ak6ccGyFiIiInBQDiwPJZEDXrpfuKYRDhxxbISIiIifFwOJgFuNYGFiIiIgaxcDiYKbAchZdgYMHHVsZIiIiJ8XA4mAWLSwHDwLt/8LDRERENsfA4mCXrsXSFSgsBLRax1aIiIjICTGwOJi5hcU9SnrCbiEiIqIGGFgczDyGRRYqPeHAWyIiogYYWBzM1CV0vtoPdVAwsBARETWCgcXBgoMBpRIwGOXIRTd2CRERETWCgcXB5HIgMlJ6no3uQHY2UFbm2EoRERE5GQYWJxD153jbk+r+0pP9+x1XGSIiIifEwOIEzIEleIj0JD3dcZUhIiJyQgwsTqB7d+nxpEeM9OSbbxxXGSIiIifEwOIEzC0slSHSk59/BoqLHVchIiIiJ8PA4gTMgSXXFaLvddLl+dktREREZMbA4gTCw6WzhSorAe2Qe6WZ7BYiIiIyY2BxAm5uQLdu0vOTPW+Xnnz7LW+ESERE9CcGFidh7hbyjpOuJHf2LHD8uEPrRERE5CwYWJyEKbBk57oBw4ZJLz780HEVIiIiciIMLE7C3MJyEsCECdKL1FTg8GGH1YmIiMhZMLA4CfO1WE4CePBB4M47Ab1eel5d7dC6ERERORoDi5OwaGGRyYC33gI6dwZ+/VU6jSgyEkhKAj7/HDAaHVlVIiIiu2NgcRKmGyAWFwMlJZDCyrp10vnO+flATo50qvOddwK9egGbNjG4EBHRNYOBxUl4e0sZBZBu2AwAGD1aanLZuxf46Sfg6acBX1/gxAngL38BBg4EVq0Cjh3jKdBERNShMbA4keuvlx6//77ezPBwKZgMGQK89JJ0uvMLL0gJ58ABYMYMoG9foEsX4OGHgfXrpcTDAENERB2ITIj2/82m0+mgVqtRWloKHx8fR1en1VauBP75T2DECGD79mYKFxZK4WT7duDHHxsOzA0OBoYOlaZ+/YCePaUmHJmsrapPRERkFWu+vxlYnEhWljQ8xc1NGsvi5dXCN1ZXA5mZUnj5/ntg3z7pDKPL+fpKG+jZU3rs0QPo2lVqnQkKAlxcbLk7RERETWJgaaeEkM4WyskBPvsMGDOmlSuqqpLGvfz4oxRkjh8HTp1quptILgc0GiAkRAowwcFAp05NT0plKytIRERk3fc3/6V2IjIZMGoU8MYbwNdfXwos+fnAq69K+eDZZwFX12ZW5O4ODB8uTSZVVdLYlt9+k5pyfvtNep2XB5w/DxgMwLlzwLlzqPz5KM4jGOcQgiIEwAV1cIMeFfDERfihBkrIICB3UUCmcoPRzR1aRRecl4VA5qqAt3sdfDwM8PY0ws3DBcUGXxTVqlGk90ZxtSc6qY3o16MKYd0Ag6sKdS7SVCtXok7uijqDHB4eQGDgpalTJylvGQyAWg14el7atdpaoKJCalQKDGSvFxFRR8TA4mTqB5ayMulit6++Kt3JGQAyMoD//Q/w9296PUYjcOaMlEkKCoALF9xRVRUDozEGlXVAoRooCgMKPYDiTgLVFQbUVhtwoUSO0ormEtGf6gCUt24/N/3UuveZeLjUwE1eh4o6JWqNlz7G3u616Btega4aA/w6yeClVkChcoWLhysUbi5wdZPB21sKPeHhQGwsEBAgvVcIoK5OCkVKJYMPEZEzYZeQk6mokFoT9HppjGxBgTS/f3+pYaS8XLqz8/jxwM03S1+uxcVAUZE0nT4tlfv9d6lRpbXc3aXeocBAKfzo9YCHB+DnB6iUAqK2DkKvh6g1AIY6dPaqQohPGVCjR1mZQFmZDLpyOfR6AX+XUgTILyJAVoxOxiKcL/fCL6XhyK/2hauxGi6GGrgYauCKWrigDgoYUAFPFCIQhQhEATpDBzUAQIE6GGycs90VNag1KlAnLq1X7V6DyM4V6BtRgRv7VcFf44qfT/oiK9cDbh4u8FLLMXy4DMnJlq09RETUchzD0s7ddtuls4QiI4EVK6TrxR09Kj3m5LRsPa6u0vuDg6UWGQ8PaaiKSnWpqyUgQJrc3aXyarUUVHx87NzCYDRKzUhVVZZTRQVQVgZjaRnkFWUQZeWoqADyS5Soq9TDs+YCPKuL4VlZCFlpCU4Ud8LRC8HIr/DCxWoVKmpcYagzog4uqIMLauGKMnjjIvzwO3rgJLpfVbXVch36exxHhcwLFfBEudEDeuGG2OB8DO1ZgIDOCuhV3pB5esK9kzvUGnd0j/VE995SK1ZVlTQWWqGwwc+QnF5dnXQZpZ49pd9FomsdA0s7t3078PjjwAMPSGNW3N0vLdPpgE8/BXbskE4G8vCQAoe/v/QYEnLpJKCICJ74A0BqhqqslJqnTFNZGVBejrL8ShSdr4VrZal5kpeVIq/QFX8U+uBAYSgyS3qhRO+JG3AA/Wp/hoAMWmjwLh7CH4i66ur5u5RglOYgAnz0+K4gBn/o/HFTZD5uu74ItS4qnC5RQ6F0RUgXGdQBrqiGElUGJapq5KipkQJpz55Sl9bJk4BWK7WI1dVJZ5p5e0tfjgaD9Hnw8pJCqxDS+B+tVhrKpFZLnxsfH2lYk04nfaY8PYEffpA+lwEBwMSJwHXXAd99B/zyi/S+gABpXWVl0jb8/aXtGAzSPmo0Uj1PngR275YOQUiINC8kRGpNrK6Wtmk0SuG5sFC69+fZs1Ko8/e/NOn1UotidbX0O1B/8vS8FM5ra6VHUyg8fBg4ckTqDrz9dmkd69ZJIWL4cCAxUSp/4cKlSaWSug4jI6XxZHl5Up3OnZNaQ2+4Qdq/c+ek1k5PT2kqL5fWHxAA3Hij1D07e7Y0Bj42Fli0CIiOllpFT5+WxsVXVkrzoqOldSgUUmvpgQPSPvfpIy1TqaSfkWk6f14qV1Jy6ZjXf/Tykup+9KhUr759pfWUlkrH/8QJ6f1yOdC7NxAaCtTUSNsMCpJeu7hI8+pPer30M/bwkLZVWSm1ChsM0nH185OOY2GhNJa/Tx/pf5AjR6TPSkyMtO6ff5b+ngUHA4MGSfU9dUqqa9eu0jE/fly6U4npig0eHtL7zpyRfsadO0vvc3e/NBUVSWXOnpX2t3dvaZ15eVLZXr2kcsePS2W6dpX+bspkln8u5HLppMqAAOl3ISNDOj69e0st3t7e0nqEkD6/RqPlc9Nk+iya9i8rS+p+7tFDmn/unPSZc3eX1mmaamul+WVl0npNEyD949mtm3QssrKkz0CPHkBYmPT7kZ8vTVqttN4bb5S2+cknwLffSp/FceOk43zhgvR+uVz6XPn7S2WPHZP2+cIFYN68q/6TZ6HNA8vq1auxbNkyaLVaxMbGYtWqVRg0aNAVy3/00UeYO3cuTp06hejoaLz00ku4/fbbzcuFEJg/fz7eeustlJSUYMiQIVizZg2io6NbVJ+OFljIiRmN0l8wnQ7GEh2+/x7IyzXAGzp4GXTwMpTCqCvH3hN+yDwdgqoqwLW2EkJfi2q9HMV1PvgdPXARnRy9J0TUgbm4SP+0XE4ul8JeebnlPKWy8WEErq5SYAKkwHPxom1PEG3Ts4Q2b96MlJQUrF27FvHx8Vi5ciUSExORlZWFzqZry9eza9cujB8/Hqmpqbjjjjvw/vvvY+zYsThw4ACuu+46AMDSpUvx2muvYePGjYiIiMDcuXORmJiIY8eOQaVSWVtForYjl0tNED4+kHcF/u+6xosNBvBEYwsMBohSHUpzT8Gl7CJcyy5gz14ZvtjVCeU6I4Z1/g3Rij+Q8UcofjzXHWpRgjBZLoRej3PV/tDVquAuKqFCNdxRBVfU4iy6Igs9IYNAd2SjC/KgQjXkMKICniiDNwRkUMCAOrigAp6ogjvkMEIBA4KQj2CcxwV0QhZ6ogKeCMZ5qFGKIgTgAjohDocwCl8jG92xAZOghQbD8ANuwi5UwR3F8Icb9PBCOerggmL4oxxecEEdjJDjHEKQhy4IxRnciN0IRKH5TLRzCEEhAuGOKvhAZ66nN8oQgyOIQA508EERAlAMfxTDH66oRSAK4YFKVMEdFfBEJTzMUwU8ISCDC+pggAKlUKMK7uiLo4jBERxBDL5BIpSowcOy93Cj+y/YVjscO+vi4S6q0AnF6IQL6IQLKIUahxCHMwiFBlp0xVl0QR5CcA7nEYz96I+L8EMX5CEARaiCO8rhBS9UoBMu4Ay6Yh8GQkCGJxSrMdX/Y7xZ9TDeKH8YABDuchZhLnkIczkHD1kVfq+LwMnaMFQLJeqgQKjLefRXHoW7rBrHarvjVG1X6IU04qtWuKBWuCJAcRE93E4hQHER5cID5UZPlBmlLsqyP5/7yUvR1/V3eMkr8GtdL+TUdkUnhQ5BLkWIdD2Dnq5/wGCU4VhNFLR1gXBX6OEiN+K8oTPO1gXBKORQyvVQymotHl1kBlQaVdAZPOEhr0Zn14uQQ+CcPgAlBi8EupTA37UUuTVByKoOg1KmRz+Pk/BSVOKXymjk1/qjt+oP3Oj1K7R6f+yr7AO9cEW42zl4KypxRh+Ewjo/RKvOIMbjD5yuCcLe8j6oEwr09TiF7u55uFDnjYJaP1QaVKgyuqHSqEKVUQkvRRX6e59AmCofv1d1RVZFV6hdKtFFVQxdnQeyKrqgxuiK7p7nEepehLxqf5yq7AyFzAgvl2ppUlSjxuiK7AoN6oQLuqiKkRh0CAYhx/GyLtDW+KG8ToUqgxvkMEIuE5DLhHQmpUxADgG5TJpfZ1SgpNYDdcIF/m469PI+h2qDC34vD0FFnRIaVQn8lRWoNrigrFaF8jp3lBvc4SKrQye3cni51Ejr/nP9AjJoq31RVis1wwe7X4SfWyWyyzpDXyd1OysVtdC4lyJIpUNxjRdOlnVGeTkQ6lmMeyIOYk9BBHYXRJnDipdrNYQAaoyuqDMqpBY0lxoMCc7B8G5/QK+/3WFXtLC6hSU+Ph4DBw7E66+/DgAwGo0IDQ3F9OnT8cwzzzQon5ycjIqKCnzxxRfmeTfeeCPi4uKwdu1aCCEQEhKCJ598ErNmzQIAlJaWIigoCBs2bMC4ceOarRNbWOiaIYTUFl9efqlN/vJ2+raYr1BcGmhTV2c5mepV/7GxeS1Z1pZlAOmqjK6uUr/ExYuNX2CxDdT++b+hKy79y2uq1bV0MpoBcumLHJeOSQ3coIR1x6EGbjBCDndUN1lOoOmfrxEyGKCwOC5XUgsXFCEAGmiv6pgJANVQWdRdADBCDgUa3tDWCBlkEE1uswRqyGGED8oAAHVQ4BxCoEYpfKCzeO85BOM8ghGHQ+btnUUX6OGGLsgzHwsBQAcfXEAndMVZ6WekVDa8qvpVarMWFr1ej/3792POnDnmeXK5HAkJCcjMzGz0PZmZmUhJSbGYl5iYiK1btwIAcnJyoNVqkZCQYF6uVqsRHx+PzMzMRgNLTU0NampqzK91Op01u0HUfslk0gAGtjxePSGkNvCLF6UA4+IiTa6uUjirPyr28v/rrhSIGlvm6gpXd3epO7G4WBoIIARk9Qci2CPAKRSXLuJUVSUFUbn8Uhit/xyQwmht7aWBKjbQ2NhyJSB9rk2j/E2P9QdrXPazUl5h/uWTrIllEALyP6dGt1P/5wfAVQgENzK/Rc/rkQFwrz/DxQUyNzcoTP+M6PWXBicBkBsM0mfH9NjIsfW9bJ6LEOhWf7/rLQsRAiFCALhHmieToWsjP3eZ0Qi1EFDXH5Tj4JHiVgWWoqIiGAwGBAUFWcwPCgrCb7/91uh7tFpto+W1Wq15uWnelcpcLjU1FQsXLrSm6kRElmSySyN17cXbWxrxS0RWa5cn1s2ZMwelpaXm6cyZM46uEhEREbUhqwJLQEAAFAoF8vPzLebn5+dDo9E0+h6NRtNkedOjNetUKpXw8fGxmIiIiKjjsiqwuLm5oX///khPTzfPMxqNSE9Px+DBgxt9z+DBgy3KA8C2bdvM5SMiIqDRaCzK6HQ67Nmz54rrJCIiomuL1ac1p6SkYOLEiRgwYAAGDRqElStXoqKiApMnTwYATJgwAV26dEFqaioA4IknnsDw4cPx8ssvY/To0fjggw/w888/48033wQAyGQyzJw5Ey+88AKio6PNpzWHhIRg7NixtttTIiIiaresDizJyckoLCzEvHnzoNVqERcXh7S0NPOg2dzcXMjrjSS+6aab8P777+P555/Hs88+i+joaGzdutV8DRYAePrpp1FRUYGpU6eipKQEN998M9LS0ngNFiIiIgLAS/MTERGRg1jz/d0uzxIiIiKiawsDCxERETk9BhYiIiJyegwsRERE5PQYWIiIiMjpMbAQERGR02NgISIiIqdn9YXjnJHpUjI6nc7BNSEiIqKWMn1vt+SScB0isJSVlQEAQkNDHVwTIiIislZZWRnUanWTZTrElW6NRiPOnTsHb29vyGQym65bp9MhNDQUZ86c6bBX0e3o+9jR9w/gPnYEHX3/AO5jR2Dr/RNCoKysDCEhIRa39WlMh2hhkcvl6Nq1a5tuw8fHp0N++Orr6PvY0fcP4D52BB19/wDuY0dgy/1rrmXFhINuiYiIyOkxsBAREZHTY2BphlKpxPz586FUKh1dlTbT0fexo+8fwH3sCDr6/gHcx47AkfvXIQbdEhERUcfGFhYiIiJyegwsRERE5PQYWIiIiMjpMbAQERGR02NgISIiIqfHwNKM1atXIzw8HCqVCvHx8di7d6+jq9QqqampGDhwILy9vdG5c2eMHTsWWVlZFmVuueUWyGQyi+kf//iHg2psvQULFjSof69evczLq6ur8fjjj8Pf3x9eXl649957kZ+f78AaWyc8PLzB/slkMjz++OMA2ufx++GHHzBmzBiEhIRAJpNh69atFsuFEJg3bx6Cg4Ph7u6OhIQEnDhxwqLMhQsX8OCDD8LHxwe+vr545JFHUF5ebse9aFpT+1hbW4vZs2cjJiYGnp6eCAkJwYQJE3Du3DmLdTR27JcsWWLnPWlcc8dw0qRJDeqelJRkUaY9H0MAjf5eymQyLFu2zFzGmY9hS74fWvL3Mzc3F6NHj4aHhwc6d+6Mp556CnV1dTarJwNLEzZv3oyUlBTMnz8fBw4cQGxsLBITE1FQUODoqlnt+++/x+OPP47du3dj27ZtqK2txciRI1FRUWFRbsqUKTh//rx5Wrp0qYNq3Dp9+/a1qP9PP/1kXvbPf/4Tn3/+OT766CN8//33OHfuHO655x4H1tY6+/bts9i3bdu2AQDuv/9+c5n2dvwqKioQGxuL1atXN7p86dKleO2117B27Vrs2bMHnp6eSExMRHV1tbnMgw8+iKNHj2Lbtm344osv8MMPP2Dq1Kn22oVmNbWPlZWVOHDgAObOnYsDBw5gy5YtyMrKwp133tmg7KJFiyyO7fTp0+1R/WY1dwwBICkpyaLumzZtsljeno8hAIt9O3/+PNatWweZTIZ7773XopyzHsOWfD809/fTYDBg9OjR0Ov12LVrFzZu3IgNGzZg3rx5tquooCsaNGiQePzxx82vDQaDCAkJEampqQ6slW0UFBQIAOL77783zxs+fLh44oknHFepqzR//nwRGxvb6LKSkhLh6uoqPvroI/O848ePCwAiMzPTTjW0rSeeeEJERUUJo9EohGj/xw+A+OSTT8yvjUaj0Gg0YtmyZeZ5JSUlQqlUik2bNgkhhDh27JgAIPbt22cu8/XXXwuZTCby8vLsVveWunwfG7N3714BQJw+fdo8LywsTLzyyittWzkbaGz/Jk6cKO66664rvqcjHsO77rpL3HrrrRbz2ssxFKLh90NL/n5+9dVXQi6XC61Way6zZs0a4ePjI2pqamxSL7awXIFer8f+/fuRkJBgnieXy5GQkIDMzEwH1sw2SktLAQCdOnWymP/ee+8hICAA1113HebMmYPKykpHVK/VTpw4gZCQEERGRuLBBx9Ebm4uAGD//v2ora21OJ69evVCt27d2uXx1Ov1ePfdd/HXv/7V4g7l7f341ZeTkwOtVmtxzNRqNeLj483HLDMzE76+vhgwYIC5TEJCAuRyOfbs2WP3OttCaWkpZDIZfH19LeYvWbIE/v7+uP7667Fs2TKbNrW3tYyMDHTu3Bk9e/bEo48+iuLiYvOyjnYM8/Pz8eWXX+KRRx5psKy9HMPLvx9a8vczMzMTMTExCAoKMpdJTEyETqfD0aNHbVKvDnG35rZQVFQEg8Fg8cMHgKCgIPz2228OqpVtGI1GzJw5E0OGDMF1111nnv+Xv/wFYWFhCAkJweHDhzF79mxkZWVhy5YtDqxty8XHx2PDhg3o2bMnzp8/j4ULF2Lo0KH49ddfodVq4ebm1uBLICgoCFqt1jEVvgpbt25FSUkJJk2aZJ7X3o/f5UzHpbHfQdMyrVaLzp07Wyx3cXFBp06d2uVxra6uxuzZszF+/HiLO+HOmDEDN9xwAzp16oRdu3Zhzpw5OH/+PFasWOHA2rZMUlIS7rnnHkRERODkyZN49tlnMWrUKGRmZkKhUHS4Y7hx40Z4e3s36G5uL8ewse+Hlvz91Gq1jf6umpbZAgPLNejxxx/Hr7/+ajG+A4BFn3FMTAyCg4MxYsQInDx5ElFRUfauptVGjRplft6vXz/Ex8cjLCwMH374Idzd3R1YM9v773//i1GjRiEkJMQ8r70fv2tdbW0tHnjgAQghsGbNGotlKSkp5uf9+vWDm5sb/v73vyM1NdXp71kzbtw48/OYmBj069cPUVFRyMjIwIgRIxxYs7axbt06PPjgg1CpVBbz28sxvNL3gzNgl9AVBAQEQKFQNBgFnZ+fD41G46BaXb1p06bhiy++wI4dO9C1a9cmy8bHxwMAsrOz7VE1m/P19UWPHj2QnZ0NjUYDvV6PkpISizLt8XiePn0a27dvx9/+9rcmy7X342c6Lk39Dmo0mgaD4Ovq6nDhwoV2dVxNYeX06dPYtm2bRetKY+Lj41FXV4dTp07Zp4I2FBkZiYCAAPPnsqMcQwD48ccfkZWV1ezvJuCcx/BK3w8t+fup0Wga/V01LbMFBpYrcHNzQ//+/ZGenm6eZzQakZ6ejsGDBzuwZq0jhMC0adPwySef4LvvvkNERESz7zl06BAAIDg4uI1r1zbKy8tx8uRJBAcHo3///nB1dbU4nllZWcjNzW13x3P9+vXo3LkzRo8e3WS59n78IiIioNFoLI6ZTqfDnj17zMds8ODBKCkpwf79+81lvvvuOxiNRnNgc3amsHLixAls374d/v7+zb7n0KFDkMvlDbpS2oOzZ8+iuLjY/LnsCMfQ5L///S/69++P2NjYZss60zFs7vuhJX8/Bw8ejCNHjliET1P47tOnj80qSlfwwQcfCKVSKTZs2CCOHTsmpk6dKnx9fS1GQbcXjz76qFCr1SIjI0OcP3/ePFVWVgohhMjOzhaLFi0SP//8s8jJyRGffvqpiIyMFMOGDXNwzVvuySefFBkZGSInJ0fs3LlTJCQkiICAAFFQUCCEEOIf//iH6Natm/juu+/Ezz//LAYPHiwGDx7s4Fpbx2AwiG7duonZs2dbzG+vx6+srEwcPHhQHDx4UAAQK1asEAcPHjSfIbNkyRLh6+srPv30U3H48GFx1113iYiICFFVVWVeR1JSkrj++uvFnj17xE8//SSio6PF+PHjHbVLDTS1j3q9Xtx5552ia9eu4tChQxa/m6YzK3bt2iVeeeUVcejQIXHy5Enx7rvvisDAQDFhwgQH75mkqf0rKysTs2bNEpmZmSInJ0ds375d3HDDDSI6OlpUV1eb19Gej6FJaWmp8PDwEGvWrGnwfmc/hs19PwjR/N/Puro6cd1114mRI0eKQ4cOibS0NBEYGCjmzJljs3oysDRj1apVolu3bsLNzU0MGjRI7N6929FVahUAjU7r168XQgiRm5srhg0bJjp16iSUSqXo3r27eOqpp0RpaaljK26F5ORkERwcLNzc3ESXLl1EcnKyyM7ONi+vqqoSjz32mPDz8xMeHh7i7rvvFufPn3dgja33zTffCAAiKyvLYn57PX47duxo9HM5ceJEIYR0avPcuXNFUFCQUCqVYsSIEQ32vbi4WIwfP154eXkJHx8fMXnyZFFWVuaAvWlcU/uYk5Nzxd/NHTt2CCGE2L9/v4iPjxdqtVqoVCrRu3dv8eKLL1p84TtSU/tXWVkpRo4cKQIDA4Wrq6sICwsTU6ZMafBPX3s+hib//ve/hbu7uygpKWnwfmc/hs19PwjRsr+fp06dEqNGjRLu7u4iICBAPPnkk6K2ttZm9ZT9WVkiIiIip8UxLEREROT0GFiIiIjI6TGwEBERkdNjYCEiIiKnx8BCRERETo+BhYiIiJweAwsRERE5PQYWIiIicnoMLEREROT0GFiIiIjI6TGwEBERkdP7f+m0l/tONVJzAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6COQ-814dWIk",
        "outputId": "aeef99b5-c599-4502-a27a-eac5d3677b00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "((214, 1), (138, 1))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Lets Do the prediction and check performance metrics\n",
        "train_predict=model.predict(X_train)\n",
        "test_predict=model.predict(X_test)\n",
        "train_predict.shape, test_predict.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxB7rMV4dZPq"
      },
      "outputs": [],
      "source": [
        "# Transform back to original form\n",
        "\n",
        "train_predict = scaler.inverse_transform(train_predict)\n",
        "test_predict = scaler.inverse_transform(test_predict)\n",
        "original_ytrain = scaler.inverse_transform(y_train.reshape(-1,1))\n",
        "original_ytest = scaler.inverse_transform(y_test.reshape(-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzCY0492dbcp",
        "outputId": "908e7815-4e02-4017-9cf1-62c07ada49cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data RMSE:  2234.3970001873795\n",
            "Train data MSE:  4992529.9544463605\n",
            "Train data MAE:  1540.4418525481901\n",
            "-------------------------------------------------------------------------------------\n",
            "Test data RMSE:  3222.411878937541\n",
            "Test data MSE:  10383938.317517774\n",
            "Test data MAE:  2619.612049365941\n"
          ]
        }
      ],
      "source": [
        "# Evaluation metrices RMSE and MAE\n",
        "print(\"Train data RMSE: \", math.sqrt(mean_squared_error(original_ytrain,train_predict)))\n",
        "print(\"Train data MSE: \", mean_squared_error(original_ytrain,train_predict))\n",
        "print(\"Train data MAE: \", mean_absolute_error(original_ytrain,train_predict))\n",
        "print(\"-------------------------------------------------------------------------------------\")\n",
        "print(\"Test data RMSE: \", math.sqrt(mean_squared_error(original_ytest,test_predict)))\n",
        "print(\"Test data MSE: \", mean_squared_error(original_ytest,test_predict))\n",
        "print(\"Test data MAE: \", mean_absolute_error(original_ytest,test_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-_ICw8Ad85c",
        "outputId": "6b5f4bc7-775b-4357-ee8a-b494a3a2ee3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data explained variance regression score: 0.9829280350561804\n",
            "Test data explained variance regression score: 0.8870211190250825\n"
          ]
        }
      ],
      "source": [
        "print(\"Train data explained variance regression score:\",\n",
        "      explained_variance_score(original_ytrain, train_predict))\n",
        "print(\"Test data explained variance regression score:\",\n",
        "      explained_variance_score(original_ytest, test_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_SuFJQEeAQB",
        "outputId": "8acc907e-ca30-4b93-c270-277841ce70ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data R2 score: 0.9827536598078128\n",
            "Test data R2 score: 0.8331800882780037\n"
          ]
        }
      ],
      "source": [
        "print(\"Train data R2 score:\", r2_score(original_ytrain, train_predict))\n",
        "print(\"Test data R2 score:\", r2_score(original_ytest, test_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjz4IyuheC7O",
        "outputId": "755c541e-8c03-49e3-953f-bf832b4fd3cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data MGD:  0.004273993985943193\n",
            "Test data MGD:  0.006072666127539531\n",
            "----------------------------------------------------------------------\n",
            "Train data MPD:  131.28385387567909\n",
            "Test data MPD:  248.1051207391666\n"
          ]
        }
      ],
      "source": [
        "print(\"Train data MGD: \", mean_gamma_deviance(original_ytrain, train_predict))\n",
        "print(\"Test data MGD: \", mean_gamma_deviance(original_ytest, test_predict))\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"Train data MPD: \", mean_poisson_deviance(original_ytrain, train_predict))\n",
        "print(\"Test data MPD: \", mean_poisson_deviance(original_ytest, test_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "8epnZMJVeNp7",
        "outputId": "bbd6bc1a-90ba-4146-a5f4-560c866ac5d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train predicted data:  (384, 1)\n",
            "Test predicted data:  (384, 1)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"c0ca51e2-4e27-4519-ba01-7f9c6d6803d3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c0ca51e2-4e27-4519-ba01-7f9c6d6803d3\")) {                    Plotly.newPlot(                        \"c0ca51e2-4e27-4519-ba01-7f9c6d6803d3\",                        [{\"hovertemplate\":\"variable=original_close\\u003cbr\\u003eDate=%{x}\\u003cbr\\u003eStock price=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"original_close\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Original close price\",\"showlegend\":true,\"x\":[\"2020-08-23T00:00:00\",\"2020-08-24T00:00:00\",\"2020-08-25T00:00:00\",\"2020-08-26T00:00:00\",\"2020-08-27T00:00:00\",\"2020-08-28T00:00:00\",\"2020-08-29T00:00:00\",\"2020-08-30T00:00:00\",\"2020-08-31T00:00:00\",\"2020-09-01T00:00:00\",\"2020-09-02T00:00:00\",\"2020-09-03T00:00:00\",\"2020-09-04T00:00:00\",\"2020-09-05T00:00:00\",\"2020-09-06T00:00:00\",\"2020-09-07T00:00:00\",\"2020-09-08T00:00:00\",\"2020-09-09T00:00:00\",\"2020-09-10T00:00:00\",\"2020-09-11T00:00:00\",\"2020-09-12T00:00:00\",\"2020-09-13T00:00:00\",\"2020-09-14T00:00:00\",\"2020-09-15T00:00:00\",\"2020-09-16T00:00:00\",\"2020-09-17T00:00:00\",\"2020-09-18T00:00:00\",\"2020-09-19T00:00:00\",\"2020-09-20T00:00:00\",\"2020-09-21T00:00:00\",\"2020-09-22T00:00:00\",\"2020-09-23T00:00:00\",\"2020-09-24T00:00:00\",\"2020-09-25T00:00:00\",\"2020-09-26T00:00:00\",\"2020-09-27T00:00:00\",\"2020-09-28T00:00:00\",\"2020-09-29T00:00:00\",\"2020-09-30T00:00:00\",\"2020-10-01T00:00:00\",\"2020-10-02T00:00:00\",\"2020-10-03T00:00:00\",\"2020-10-04T00:00:00\",\"2020-10-05T00:00:00\",\"2020-10-06T00:00:00\",\"2020-10-07T00:00:00\",\"2020-10-08T00:00:00\",\"2020-10-09T00:00:00\",\"2020-10-10T00:00:00\",\"2020-10-11T00:00:00\",\"2020-10-12T00:00:00\",\"2020-10-13T00:00:00\",\"2020-10-14T00:00:00\",\"2020-10-15T00:00:00\",\"2020-10-16T00:00:00\",\"2020-10-17T00:00:00\",\"2020-10-18T00:00:00\",\"2020-10-19T00:00:00\",\"2020-10-20T00:00:00\",\"2020-10-21T00:00:00\",\"2020-10-22T00:00:00\",\"2020-10-23T00:00:00\",\"2020-10-24T00:00:00\",\"2020-10-25T00:00:00\",\"2020-10-26T00:00:00\",\"2020-10-27T00:00:00\",\"2020-10-28T00:00:00\",\"2020-10-29T00:00:00\",\"2020-10-30T00:00:00\",\"2020-10-31T00:00:00\",\"2020-11-01T00:00:00\",\"2020-11-02T00:00:00\",\"2020-11-03T00:00:00\",\"2020-11-04T00:00:00\",\"2020-11-05T00:00:00\",\"2020-11-06T00:00:00\",\"2020-11-07T00:00:00\",\"2020-11-08T00:00:00\",\"2020-11-09T00:00:00\",\"2020-11-10T00:00:00\",\"2020-11-11T00:00:00\",\"2020-11-12T00:00:00\",\"2020-11-13T00:00:00\",\"2020-11-14T00:00:00\",\"2020-11-15T00:00:00\",\"2020-11-16T00:00:00\",\"2020-11-17T00:00:00\",\"2020-11-18T00:00:00\",\"2020-11-19T00:00:00\",\"2020-11-20T00:00:00\",\"2020-11-21T00:00:00\",\"2020-11-22T00:00:00\",\"2020-11-23T00:00:00\",\"2020-11-24T00:00:00\",\"2020-11-25T00:00:00\",\"2020-11-26T00:00:00\",\"2020-11-27T00:00:00\",\"2020-11-28T00:00:00\",\"2020-11-29T00:00:00\",\"2020-11-30T00:00:00\",\"2020-12-01T00:00:00\",\"2020-12-02T00:00:00\",\"2020-12-03T00:00:00\",\"2020-12-04T00:00:00\",\"2020-12-05T00:00:00\",\"2020-12-06T00:00:00\",\"2020-12-07T00:00:00\",\"2020-12-08T00:00:00\",\"2020-12-09T00:00:00\",\"2020-12-10T00:00:00\",\"2020-12-11T00:00:00\",\"2020-12-12T00:00:00\",\"2020-12-13T00:00:00\",\"2020-12-14T00:00:00\",\"2020-12-15T00:00:00\",\"2020-12-16T00:00:00\",\"2020-12-17T00:00:00\",\"2020-12-18T00:00:00\",\"2020-12-19T00:00:00\",\"2020-12-20T00:00:00\",\"2020-12-21T00:00:00\",\"2020-12-22T00:00:00\",\"2020-12-23T00:00:00\",\"2020-12-24T00:00:00\",\"2020-12-25T00:00:00\",\"2020-12-26T00:00:00\",\"2020-12-27T00:00:00\",\"2020-12-28T00:00:00\",\"2020-12-29T00:00:00\",\"2020-12-30T00:00:00\",\"2020-12-31T00:00:00\",\"2021-01-01T00:00:00\",\"2021-01-02T00:00:00\",\"2021-01-03T00:00:00\",\"2021-01-04T00:00:00\",\"2021-01-05T00:00:00\",\"2021-01-06T00:00:00\",\"2021-01-07T00:00:00\",\"2021-01-08T00:00:00\",\"2021-01-09T00:00:00\",\"2021-01-10T00:00:00\",\"2021-01-11T00:00:00\",\"2021-01-12T00:00:00\",\"2021-01-13T00:00:00\",\"2021-01-14T00:00:00\",\"2021-01-15T00:00:00\",\"2021-01-16T00:00:00\",\"2021-01-17T00:00:00\",\"2021-01-18T00:00:00\",\"2021-01-19T00:00:00\",\"2021-01-20T00:00:00\",\"2021-01-21T00:00:00\",\"2021-01-22T00:00:00\",\"2021-01-23T00:00:00\",\"2021-01-24T00:00:00\",\"2021-01-25T00:00:00\",\"2021-01-26T00:00:00\",\"2021-01-27T00:00:00\",\"2021-01-28T00:00:00\",\"2021-01-29T00:00:00\",\"2021-01-30T00:00:00\",\"2021-01-31T00:00:00\",\"2021-02-01T00:00:00\",\"2021-02-02T00:00:00\",\"2021-02-03T00:00:00\",\"2021-02-04T00:00:00\",\"2021-02-05T00:00:00\",\"2021-02-06T00:00:00\",\"2021-02-07T00:00:00\",\"2021-02-08T00:00:00\",\"2021-02-09T00:00:00\",\"2021-02-10T00:00:00\",\"2021-02-11T00:00:00\",\"2021-02-12T00:00:00\",\"2021-02-13T00:00:00\",\"2021-02-14T00:00:00\",\"2021-02-15T00:00:00\",\"2021-02-16T00:00:00\",\"2021-02-17T00:00:00\",\"2021-02-18T00:00:00\",\"2021-02-19T00:00:00\",\"2021-02-20T00:00:00\",\"2021-02-21T00:00:00\",\"2021-02-22T00:00:00\",\"2021-02-23T00:00:00\",\"2021-02-24T00:00:00\",\"2021-02-25T00:00:00\",\"2021-02-26T00:00:00\",\"2021-02-27T00:00:00\",\"2021-02-28T00:00:00\",\"2021-03-01T00:00:00\",\"2021-03-02T00:00:00\",\"2021-03-03T00:00:00\",\"2021-03-04T00:00:00\",\"2021-03-05T00:00:00\",\"2021-03-06T00:00:00\",\"2021-03-07T00:00:00\",\"2021-03-08T00:00:00\",\"2021-03-09T00:00:00\",\"2021-03-10T00:00:00\",\"2021-03-11T00:00:00\",\"2021-03-12T00:00:00\",\"2021-03-13T00:00:00\",\"2021-03-14T00:00:00\",\"2021-03-15T00:00:00\",\"2021-03-16T00:00:00\",\"2021-03-17T00:00:00\",\"2021-03-18T00:00:00\",\"2021-03-19T00:00:00\",\"2021-03-20T00:00:00\",\"2021-03-21T00:00:00\",\"2021-03-22T00:00:00\",\"2021-03-23T00:00:00\",\"2021-03-24T00:00:00\",\"2021-03-25T00:00:00\",\"2021-03-26T00:00:00\",\"2021-03-27T00:00:00\",\"2021-03-28T00:00:00\",\"2021-03-29T00:00:00\",\"2021-03-30T00:00:00\",\"2021-03-31T00:00:00\",\"2021-04-01T00:00:00\",\"2021-04-02T00:00:00\",\"2021-04-03T00:00:00\",\"2021-04-04T00:00:00\",\"2021-04-05T00:00:00\",\"2021-04-06T00:00:00\",\"2021-04-07T00:00:00\",\"2021-04-08T00:00:00\",\"2021-04-09T00:00:00\",\"2021-04-10T00:00:00\",\"2021-04-11T00:00:00\",\"2021-04-12T00:00:00\",\"2021-04-13T00:00:00\",\"2021-04-14T00:00:00\",\"2021-04-15T00:00:00\",\"2021-04-16T00:00:00\",\"2021-04-17T00:00:00\",\"2021-04-18T00:00:00\",\"2021-04-19T00:00:00\",\"2021-04-20T00:00:00\",\"2021-04-21T00:00:00\",\"2021-04-22T00:00:00\",\"2021-04-23T00:00:00\",\"2021-04-24T00:00:00\",\"2021-04-25T00:00:00\",\"2021-04-26T00:00:00\",\"2021-04-27T00:00:00\",\"2021-04-28T00:00:00\",\"2021-04-29T00:00:00\",\"2021-04-30T00:00:00\",\"2021-05-01T00:00:00\",\"2021-05-02T00:00:00\",\"2021-05-03T00:00:00\",\"2021-05-04T00:00:00\",\"2021-05-05T00:00:00\",\"2021-05-06T00:00:00\",\"2021-05-07T00:00:00\",\"2021-05-08T00:00:00\",\"2021-05-09T00:00:00\",\"2021-05-10T00:00:00\",\"2021-05-11T00:00:00\",\"2021-05-12T00:00:00\",\"2021-05-13T00:00:00\",\"2021-05-14T00:00:00\",\"2021-05-15T00:00:00\",\"2021-05-16T00:00:00\",\"2021-05-17T00:00:00\",\"2021-05-18T00:00:00\",\"2021-05-19T00:00:00\",\"2021-05-20T00:00:00\",\"2021-05-21T00:00:00\",\"2021-05-22T00:00:00\",\"2021-05-23T00:00:00\",\"2021-05-24T00:00:00\",\"2021-05-25T00:00:00\",\"2021-05-26T00:00:00\",\"2021-05-27T00:00:00\",\"2021-05-28T00:00:00\",\"2021-05-29T00:00:00\",\"2021-05-30T00:00:00\",\"2021-05-31T00:00:00\",\"2021-06-01T00:00:00\",\"2021-06-02T00:00:00\",\"2021-06-03T00:00:00\",\"2021-06-04T00:00:00\",\"2021-06-05T00:00:00\",\"2021-06-06T00:00:00\",\"2021-06-07T00:00:00\",\"2021-06-08T00:00:00\",\"2021-06-09T00:00:00\",\"2021-06-10T00:00:00\",\"2021-06-11T00:00:00\",\"2021-06-12T00:00:00\",\"2021-06-13T00:00:00\",\"2021-06-14T00:00:00\",\"2021-06-15T00:00:00\",\"2021-06-16T00:00:00\",\"2021-06-17T00:00:00\",\"2021-06-18T00:00:00\",\"2021-06-19T00:00:00\",\"2021-06-20T00:00:00\",\"2021-06-21T00:00:00\",\"2021-06-22T00:00:00\",\"2021-06-23T00:00:00\",\"2021-06-24T00:00:00\",\"2021-06-25T00:00:00\",\"2021-06-26T00:00:00\",\"2021-06-27T00:00:00\",\"2021-06-28T00:00:00\",\"2021-06-29T00:00:00\",\"2021-06-30T00:00:00\",\"2021-07-01T00:00:00\",\"2021-07-02T00:00:00\",\"2021-07-03T00:00:00\",\"2021-07-04T00:00:00\",\"2021-07-05T00:00:00\",\"2021-07-06T00:00:00\",\"2021-07-07T00:00:00\",\"2021-07-08T00:00:00\",\"2021-07-09T00:00:00\",\"2021-07-10T00:00:00\",\"2021-07-11T00:00:00\",\"2021-07-12T00:00:00\",\"2021-07-13T00:00:00\",\"2021-07-14T00:00:00\",\"2021-07-15T00:00:00\",\"2021-07-16T00:00:00\",\"2021-07-17T00:00:00\",\"2021-07-18T00:00:00\",\"2021-07-19T00:00:00\",\"2021-07-20T00:00:00\",\"2021-07-21T00:00:00\",\"2021-07-22T00:00:00\",\"2021-07-23T00:00:00\",\"2021-07-24T00:00:00\",\"2021-07-25T00:00:00\",\"2021-07-26T00:00:00\",\"2021-07-27T00:00:00\",\"2021-07-28T00:00:00\",\"2021-07-29T00:00:00\",\"2021-07-30T00:00:00\",\"2021-07-31T00:00:00\",\"2021-08-01T00:00:00\",\"2021-08-02T00:00:00\",\"2021-08-03T00:00:00\",\"2021-08-04T00:00:00\",\"2021-08-05T00:00:00\",\"2021-08-06T00:00:00\",\"2021-08-07T00:00:00\",\"2021-08-08T00:00:00\",\"2021-08-09T00:00:00\",\"2021-08-10T00:00:00\",\"2021-08-11T00:00:00\",\"2021-08-12T00:00:00\",\"2021-08-13T00:00:00\",\"2021-08-14T00:00:00\",\"2021-08-15T00:00:00\",\"2021-08-16T00:00:00\",\"2021-08-17T00:00:00\",\"2021-08-18T00:00:00\",\"2021-08-19T00:00:00\",\"2021-08-20T00:00:00\",\"2021-08-21T00:00:00\",\"2021-08-22T00:00:00\",\"2021-08-23T00:00:00\",\"2021-08-24T00:00:00\",\"2021-08-25T00:00:00\",\"2021-08-26T00:00:00\",\"2021-08-27T00:00:00\",\"2021-08-28T00:00:00\",\"2021-08-29T00:00:00\",\"2021-08-30T00:00:00\",\"2021-08-31T00:00:00\",\"2021-09-01T00:00:00\",\"2021-09-02T00:00:00\",\"2021-09-03T00:00:00\",\"2021-09-04T00:00:00\",\"2021-09-05T00:00:00\",\"2021-09-06T00:00:00\",\"2021-09-07T00:00:00\",\"2021-09-08T00:00:00\",\"2021-09-09T00:00:00\",\"2021-09-10T00:00:00\"],\"xaxis\":\"x\",\"y\":[11683.44,11653.02,11763.93,11337.4,11467.37,11302.01,11534.75,11481.64,11707.78,11659.57,11923.25,11397.44,10187.51,10467.89,10159.62,10254.93,10367.74,10121.52,10227.83,10352.66,10395.44,10446.44,10330.77,10674.64,10785.62,10948.43,10943.89,10931.79,11081.43,10919.65,10430.46,10532.22,10234.48,10732.43,10692.84,10732.4,10774.24,10692.33,10840.8,10777.92,10619.24,10575.06,10551.77,10673.46,10788.56,10603.74,10670.8,10923.3,11063.19,11302.67,11376.61,11540.04,11428.24,11431.32,11503.73,11327.57,11366.51,11508.2,11758.16,11925.46,12831.56,12990.25,12944.52,13128.46,13036.77,13076.37,13651.47,13289.0,13458.66,13564.72,13810.32,13758.88,13575.17,14023.31,14155.59,15591.39,15595.77,14839.84,15490.6,15328.53,15317.04,15708.65,16295.57,16339.33,16091.07,15968.16,16725.15,17679.72,17798.45,17820.57,18687.45,18699.75,18422.28,18398.91,19172.52,18739.8,17151.44,17138.87,17732.42,18191.6,19709.73,18792.52,19226.97,19454.54,18670.49,19155.06,19377.66,19181.41,18318.87,18554.15,18247.76,18029.36,18803.44,19164.48,19276.59,19439.75,21379.48,22847.46,23150.79,23869.92,23490.58,22745.48,23824.99,23253.37,23715.53,24693.58,26443.21,26246.58,27036.69,27376.37,28856.59,28982.56,29393.75,32195.46,33000.78,32035.03,34046.67,36860.41,39486.04,40670.25,40240.72,38240.09,35544.94,34011.82,37393.13,39158.47,36828.52,36065.2,35793.01,36632.35,36020.13,35538.98,30797.88,33002.38,32099.74,32276.84,32243.26,32541.8,30419.17,33403.17,34314.26,34318.1,33136.46,33522.9,35529.66,37676.25,37002.09,38278.61,39323.26,38928.1,46364.3,46589.58,44878.17,48013.38,47471.4,47185.19,48720.37,47951.85,49160.1,52118.23,51608.15,55916.5,56001.2,57487.86,54123.4,48880.43,50624.84,46800.42,46340.31,46155.87,45113.92,49618.43,48356.04,50477.7,48448.91,48861.38,48881.59,51169.7,52299.33,54881.52,55997.23,57764.0,57253.28,61258.73,59133.47,55754.72,56872.38,58913.0,57665.9,58075.1,58085.8,57411.17,54204.96,54477.46,52508.23,51415.92,55074.47,55863.93,55783.71,57627.67,58730.13,58735.25,58736.92,59031.32,57076.49,58206.55,59054.1,58020.46,55947.27,58048.59,58102.58,59774.0,59964.87,59834.74,63554.44,62969.12,63252.63,61455.98,60087.09,56251.48,55703.14,56507.91,53808.8,51731.71,51153.13,50110.53,49075.58,54056.64,55071.46,54884.1,53584.15,57796.62,57857.5,56610.46,57213.33,53241.72,57473.23,56428.16,57380.27,58928.81,58280.73,55883.5,56750.0,49007.09,49702.27,49922.52,46736.58,46441.64,43596.24,42912.19,36964.27,40784.32,37280.35,37528.3,34754.54,38728.59,38410.5,39266.04,38445.29,35689.62,34647.67,35684.59,37310.54,36662.64,37585.24,39188.59,36885.51,35530.38,35816.17,33514.87,33450.19,37338.36,36704.57,37313.18,35494.9,39066.82,40525.8,40188.56,38324.87,38068.04,35729.82,35524.17,35592.35,31686.55,32447.59,33674.66,34639.38,31640.58,32160.91,34644.45,34456.67,35847.7,35047.36,33536.88,33856.86,34688.98,35309.3,33747.97,34211.01,33839.04,32877.41,33818.52,33515.57,34227.64,33158.25,32686.56,32814.61,31738.59,31421.25,31520.66,31783.49,30815.94,29790.24,32118.06,32297.89,33581.63,34279.34,35365.2,37318.14,39405.95,40002.53,40005.93,42214.15,41659.06,40000.46,39193.94,38138.0,39750.14,40882.0,42825.95,44634.13,43816.14,46333.46,45608.37,45611.46,44417.78,47833.98,47112.19,47056.41,45982.55,44648.57,44777.86,46734.65,49327.75,48932.02,49335.68,49523.5,47744.58,48972.09,46962.8,49056.86,48897.65,48806.78,47074.77,47155.87,48862.76,49329.01,50035.33,49947.38,51769.06,52677.4,46809.17,46078.38,46368.69],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"variable=train_predicted_close\\u003cbr\\u003eDate=%{x}\\u003cbr\\u003eStock price=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"train_predicted_close\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Train predicted close price\",\"showlegend\":true,\"x\":[\"2020-08-23T00:00:00\",\"2020-08-24T00:00:00\",\"2020-08-25T00:00:00\",\"2020-08-26T00:00:00\",\"2020-08-27T00:00:00\",\"2020-08-28T00:00:00\",\"2020-08-29T00:00:00\",\"2020-08-30T00:00:00\",\"2020-08-31T00:00:00\",\"2020-09-01T00:00:00\",\"2020-09-02T00:00:00\",\"2020-09-03T00:00:00\",\"2020-09-04T00:00:00\",\"2020-09-05T00:00:00\",\"2020-09-06T00:00:00\",\"2020-09-07T00:00:00\",\"2020-09-08T00:00:00\",\"2020-09-09T00:00:00\",\"2020-09-10T00:00:00\",\"2020-09-11T00:00:00\",\"2020-09-12T00:00:00\",\"2020-09-13T00:00:00\",\"2020-09-14T00:00:00\",\"2020-09-15T00:00:00\",\"2020-09-16T00:00:00\",\"2020-09-17T00:00:00\",\"2020-09-18T00:00:00\",\"2020-09-19T00:00:00\",\"2020-09-20T00:00:00\",\"2020-09-21T00:00:00\",\"2020-09-22T00:00:00\",\"2020-09-23T00:00:00\",\"2020-09-24T00:00:00\",\"2020-09-25T00:00:00\",\"2020-09-26T00:00:00\",\"2020-09-27T00:00:00\",\"2020-09-28T00:00:00\",\"2020-09-29T00:00:00\",\"2020-09-30T00:00:00\",\"2020-10-01T00:00:00\",\"2020-10-02T00:00:00\",\"2020-10-03T00:00:00\",\"2020-10-04T00:00:00\",\"2020-10-05T00:00:00\",\"2020-10-06T00:00:00\",\"2020-10-07T00:00:00\",\"2020-10-08T00:00:00\",\"2020-10-09T00:00:00\",\"2020-10-10T00:00:00\",\"2020-10-11T00:00:00\",\"2020-10-12T00:00:00\",\"2020-10-13T00:00:00\",\"2020-10-14T00:00:00\",\"2020-10-15T00:00:00\",\"2020-10-16T00:00:00\",\"2020-10-17T00:00:00\",\"2020-10-18T00:00:00\",\"2020-10-19T00:00:00\",\"2020-10-20T00:00:00\",\"2020-10-21T00:00:00\",\"2020-10-22T00:00:00\",\"2020-10-23T00:00:00\",\"2020-10-24T00:00:00\",\"2020-10-25T00:00:00\",\"2020-10-26T00:00:00\",\"2020-10-27T00:00:00\",\"2020-10-28T00:00:00\",\"2020-10-29T00:00:00\",\"2020-10-30T00:00:00\",\"2020-10-31T00:00:00\",\"2020-11-01T00:00:00\",\"2020-11-02T00:00:00\",\"2020-11-03T00:00:00\",\"2020-11-04T00:00:00\",\"2020-11-05T00:00:00\",\"2020-11-06T00:00:00\",\"2020-11-07T00:00:00\",\"2020-11-08T00:00:00\",\"2020-11-09T00:00:00\",\"2020-11-10T00:00:00\",\"2020-11-11T00:00:00\",\"2020-11-12T00:00:00\",\"2020-11-13T00:00:00\",\"2020-11-14T00:00:00\",\"2020-11-15T00:00:00\",\"2020-11-16T00:00:00\",\"2020-11-17T00:00:00\",\"2020-11-18T00:00:00\",\"2020-11-19T00:00:00\",\"2020-11-20T00:00:00\",\"2020-11-21T00:00:00\",\"2020-11-22T00:00:00\",\"2020-11-23T00:00:00\",\"2020-11-24T00:00:00\",\"2020-11-25T00:00:00\",\"2020-11-26T00:00:00\",\"2020-11-27T00:00:00\",\"2020-11-28T00:00:00\",\"2020-11-29T00:00:00\",\"2020-11-30T00:00:00\",\"2020-12-01T00:00:00\",\"2020-12-02T00:00:00\",\"2020-12-03T00:00:00\",\"2020-12-04T00:00:00\",\"2020-12-05T00:00:00\",\"2020-12-06T00:00:00\",\"2020-12-07T00:00:00\",\"2020-12-08T00:00:00\",\"2020-12-09T00:00:00\",\"2020-12-10T00:00:00\",\"2020-12-11T00:00:00\",\"2020-12-12T00:00:00\",\"2020-12-13T00:00:00\",\"2020-12-14T00:00:00\",\"2020-12-15T00:00:00\",\"2020-12-16T00:00:00\",\"2020-12-17T00:00:00\",\"2020-12-18T00:00:00\",\"2020-12-19T00:00:00\",\"2020-12-20T00:00:00\",\"2020-12-21T00:00:00\",\"2020-12-22T00:00:00\",\"2020-12-23T00:00:00\",\"2020-12-24T00:00:00\",\"2020-12-25T00:00:00\",\"2020-12-26T00:00:00\",\"2020-12-27T00:00:00\",\"2020-12-28T00:00:00\",\"2020-12-29T00:00:00\",\"2020-12-30T00:00:00\",\"2020-12-31T00:00:00\",\"2021-01-01T00:00:00\",\"2021-01-02T00:00:00\",\"2021-01-03T00:00:00\",\"2021-01-04T00:00:00\",\"2021-01-05T00:00:00\",\"2021-01-06T00:00:00\",\"2021-01-07T00:00:00\",\"2021-01-08T00:00:00\",\"2021-01-09T00:00:00\",\"2021-01-10T00:00:00\",\"2021-01-11T00:00:00\",\"2021-01-12T00:00:00\",\"2021-01-13T00:00:00\",\"2021-01-14T00:00:00\",\"2021-01-15T00:00:00\",\"2021-01-16T00:00:00\",\"2021-01-17T00:00:00\",\"2021-01-18T00:00:00\",\"2021-01-19T00:00:00\",\"2021-01-20T00:00:00\",\"2021-01-21T00:00:00\",\"2021-01-22T00:00:00\",\"2021-01-23T00:00:00\",\"2021-01-24T00:00:00\",\"2021-01-25T00:00:00\",\"2021-01-26T00:00:00\",\"2021-01-27T00:00:00\",\"2021-01-28T00:00:00\",\"2021-01-29T00:00:00\",\"2021-01-30T00:00:00\",\"2021-01-31T00:00:00\",\"2021-02-01T00:00:00\",\"2021-02-02T00:00:00\",\"2021-02-03T00:00:00\",\"2021-02-04T00:00:00\",\"2021-02-05T00:00:00\",\"2021-02-06T00:00:00\",\"2021-02-07T00:00:00\",\"2021-02-08T00:00:00\",\"2021-02-09T00:00:00\",\"2021-02-10T00:00:00\",\"2021-02-11T00:00:00\",\"2021-02-12T00:00:00\",\"2021-02-13T00:00:00\",\"2021-02-14T00:00:00\",\"2021-02-15T00:00:00\",\"2021-02-16T00:00:00\",\"2021-02-17T00:00:00\",\"2021-02-18T00:00:00\",\"2021-02-19T00:00:00\",\"2021-02-20T00:00:00\",\"2021-02-21T00:00:00\",\"2021-02-22T00:00:00\",\"2021-02-23T00:00:00\",\"2021-02-24T00:00:00\",\"2021-02-25T00:00:00\",\"2021-02-26T00:00:00\",\"2021-02-27T00:00:00\",\"2021-02-28T00:00:00\",\"2021-03-01T00:00:00\",\"2021-03-02T00:00:00\",\"2021-03-03T00:00:00\",\"2021-03-04T00:00:00\",\"2021-03-05T00:00:00\",\"2021-03-06T00:00:00\",\"2021-03-07T00:00:00\",\"2021-03-08T00:00:00\",\"2021-03-09T00:00:00\",\"2021-03-10T00:00:00\",\"2021-03-11T00:00:00\",\"2021-03-12T00:00:00\",\"2021-03-13T00:00:00\",\"2021-03-14T00:00:00\",\"2021-03-15T00:00:00\",\"2021-03-16T00:00:00\",\"2021-03-17T00:00:00\",\"2021-03-18T00:00:00\",\"2021-03-19T00:00:00\",\"2021-03-20T00:00:00\",\"2021-03-21T00:00:00\",\"2021-03-22T00:00:00\",\"2021-03-23T00:00:00\",\"2021-03-24T00:00:00\",\"2021-03-25T00:00:00\",\"2021-03-26T00:00:00\",\"2021-03-27T00:00:00\",\"2021-03-28T00:00:00\",\"2021-03-29T00:00:00\",\"2021-03-30T00:00:00\",\"2021-03-31T00:00:00\",\"2021-04-01T00:00:00\",\"2021-04-02T00:00:00\",\"2021-04-03T00:00:00\",\"2021-04-04T00:00:00\",\"2021-04-05T00:00:00\",\"2021-04-06T00:00:00\",\"2021-04-07T00:00:00\",\"2021-04-08T00:00:00\",\"2021-04-09T00:00:00\",\"2021-04-10T00:00:00\",\"2021-04-11T00:00:00\",\"2021-04-12T00:00:00\",\"2021-04-13T00:00:00\",\"2021-04-14T00:00:00\",\"2021-04-15T00:00:00\",\"2021-04-16T00:00:00\",\"2021-04-17T00:00:00\",\"2021-04-18T00:00:00\",\"2021-04-19T00:00:00\",\"2021-04-20T00:00:00\",\"2021-04-21T00:00:00\",\"2021-04-22T00:00:00\",\"2021-04-23T00:00:00\",\"2021-04-24T00:00:00\",\"2021-04-25T00:00:00\",\"2021-04-26T00:00:00\",\"2021-04-27T00:00:00\",\"2021-04-28T00:00:00\",\"2021-04-29T00:00:00\",\"2021-04-30T00:00:00\",\"2021-05-01T00:00:00\",\"2021-05-02T00:00:00\",\"2021-05-03T00:00:00\",\"2021-05-04T00:00:00\",\"2021-05-05T00:00:00\",\"2021-05-06T00:00:00\",\"2021-05-07T00:00:00\",\"2021-05-08T00:00:00\",\"2021-05-09T00:00:00\",\"2021-05-10T00:00:00\",\"2021-05-11T00:00:00\",\"2021-05-12T00:00:00\",\"2021-05-13T00:00:00\",\"2021-05-14T00:00:00\",\"2021-05-15T00:00:00\",\"2021-05-16T00:00:00\",\"2021-05-17T00:00:00\",\"2021-05-18T00:00:00\",\"2021-05-19T00:00:00\",\"2021-05-20T00:00:00\",\"2021-05-21T00:00:00\",\"2021-05-22T00:00:00\",\"2021-05-23T00:00:00\",\"2021-05-24T00:00:00\",\"2021-05-25T00:00:00\",\"2021-05-26T00:00:00\",\"2021-05-27T00:00:00\",\"2021-05-28T00:00:00\",\"2021-05-29T00:00:00\",\"2021-05-30T00:00:00\",\"2021-05-31T00:00:00\",\"2021-06-01T00:00:00\",\"2021-06-02T00:00:00\",\"2021-06-03T00:00:00\",\"2021-06-04T00:00:00\",\"2021-06-05T00:00:00\",\"2021-06-06T00:00:00\",\"2021-06-07T00:00:00\",\"2021-06-08T00:00:00\",\"2021-06-09T00:00:00\",\"2021-06-10T00:00:00\",\"2021-06-11T00:00:00\",\"2021-06-12T00:00:00\",\"2021-06-13T00:00:00\",\"2021-06-14T00:00:00\",\"2021-06-15T00:00:00\",\"2021-06-16T00:00:00\",\"2021-06-17T00:00:00\",\"2021-06-18T00:00:00\",\"2021-06-19T00:00:00\",\"2021-06-20T00:00:00\",\"2021-06-21T00:00:00\",\"2021-06-22T00:00:00\",\"2021-06-23T00:00:00\",\"2021-06-24T00:00:00\",\"2021-06-25T00:00:00\",\"2021-06-26T00:00:00\",\"2021-06-27T00:00:00\",\"2021-06-28T00:00:00\",\"2021-06-29T00:00:00\",\"2021-06-30T00:00:00\",\"2021-07-01T00:00:00\",\"2021-07-02T00:00:00\",\"2021-07-03T00:00:00\",\"2021-07-04T00:00:00\",\"2021-07-05T00:00:00\",\"2021-07-06T00:00:00\",\"2021-07-07T00:00:00\",\"2021-07-08T00:00:00\",\"2021-07-09T00:00:00\",\"2021-07-10T00:00:00\",\"2021-07-11T00:00:00\",\"2021-07-12T00:00:00\",\"2021-07-13T00:00:00\",\"2021-07-14T00:00:00\",\"2021-07-15T00:00:00\",\"2021-07-16T00:00:00\",\"2021-07-17T00:00:00\",\"2021-07-18T00:00:00\",\"2021-07-19T00:00:00\",\"2021-07-20T00:00:00\",\"2021-07-21T00:00:00\",\"2021-07-22T00:00:00\",\"2021-07-23T00:00:00\",\"2021-07-24T00:00:00\",\"2021-07-25T00:00:00\",\"2021-07-26T00:00:00\",\"2021-07-27T00:00:00\",\"2021-07-28T00:00:00\",\"2021-07-29T00:00:00\",\"2021-07-30T00:00:00\",\"2021-07-31T00:00:00\",\"2021-08-01T00:00:00\",\"2021-08-02T00:00:00\",\"2021-08-03T00:00:00\",\"2021-08-04T00:00:00\",\"2021-08-05T00:00:00\",\"2021-08-06T00:00:00\",\"2021-08-07T00:00:00\",\"2021-08-08T00:00:00\",\"2021-08-09T00:00:00\",\"2021-08-10T00:00:00\",\"2021-08-11T00:00:00\",\"2021-08-12T00:00:00\",\"2021-08-13T00:00:00\",\"2021-08-14T00:00:00\",\"2021-08-15T00:00:00\",\"2021-08-16T00:00:00\",\"2021-08-17T00:00:00\",\"2021-08-18T00:00:00\",\"2021-08-19T00:00:00\",\"2021-08-20T00:00:00\",\"2021-08-21T00:00:00\",\"2021-08-22T00:00:00\",\"2021-08-23T00:00:00\",\"2021-08-24T00:00:00\",\"2021-08-25T00:00:00\",\"2021-08-26T00:00:00\",\"2021-08-27T00:00:00\",\"2021-08-28T00:00:00\",\"2021-08-29T00:00:00\",\"2021-08-30T00:00:00\",\"2021-08-31T00:00:00\",\"2021-09-01T00:00:00\",\"2021-09-02T00:00:00\",\"2021-09-03T00:00:00\",\"2021-09-04T00:00:00\",\"2021-09-05T00:00:00\",\"2021-09-06T00:00:00\",\"2021-09-07T00:00:00\",\"2021-09-08T00:00:00\",\"2021-09-09T00:00:00\",\"2021-09-10T00:00:00\"],\"xaxis\":\"x\",\"y\":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,11763.048828125,11638.3046875,11545.525390625,11432.505859375,11353.533203125,11298.9794921875,11257.57421875,11228.0673828125,11189.1494140625,11216.837890625,11264.0078125,11331.689453125,11389.92578125,11440.2197265625,11503.9658203125,11537.2109375,11498.998046875,11477.2666015625,11414.6630859375,11426.279296875,11429.3759765625,11435.57421875,11444.5400390625,11440.1201171875,11455.2255859375,11459.2275390625,11445.052734375,11432.0927734375,11416.89453125,11416.3935546875,11431.6328125,11421.435546875,11420.5888671875,11454.712890625,11503.40234375,11576.4248046875,11651.7451171875,11744.068359375,11810.046875,11865.3271484375,11923.802734375,11949.609375,11978.95703125,12024.08203125,12093.970703125,12176.8154296875,12374.4599609375,12567.916015625,12727.0185546875,12890.650390625,13018.2841796875,13135.134765625,13319.548828125,13429.1708984375,13551.564453125,13673.5048828125,13814.8212890625,13930.4091796875,14001.0986328125,14123.6513671875,14247.3125,14547.708984375,14808.9375,14930.8662109375,15131.224609375,15285.1181640625,15418.7294921875,15582.8955078125,15820.759765625,16033.462890625,16182.9384765625,16292.369140625,16502.41796875,16832.998046875,17133.646484375,17401.716796875,17741.11328125,18044.505859375,18289.720703125,18491.22265625,18792.40234375,18993.146484375,18920.2734375,18841.921875,18859.185546875,18946.896484375,19258.33984375,19374.1171875,19524.47265625,19690.5703125,19717.1875,19797.1484375,19902.02734375,19972.060546875,19902.171875,19859.279296875,19782.978515625,19718.642578125,19779.099609375,19870.908203125,19957.81640625,20024.14453125,20409.046875,20972.3203125,21523.73046875,22160.064453125,22651.955078125,22965.9765625,23418.302734375,23740.625,24094.48046875,24563.859375,25262.1484375,25818.294921875,26425.552734375,27003.27734375,27740.67578125,28354.271484375,28921.55078125,29898.984375,30866.947265625,31541.396484375,32499.408203125,33796.328125,35414.0859375,37050.89453125,38357.50390625,39050.05078125,39070.7109375,38722.47265625,38987.5078125,39502.578125,39462.1875,39249.05859375,38999.30859375,38951.6015625,38797.05078125,38586.26953125,37518.37890625,37008.24609375,36373.015625,35852.578125,35459.91796875,35228.36328125,34660.56640625,34724.36328125,34954.96484375,35206.21484375,35230.359375,35342.20703125,35762.09765625,36526.015625,37072.17578125,37829.9453125,38659.35546875,39277.49609375,41232.33984375,43009.32421875,44220.37890625,45869.96484375,47097.48046875,48021.34375,49057.53515625,49687.4296875,50349.53125,51439.78515625,52194.92578125,53686.046875,54958.1015625,56275.82421875,56548.30859375,55581.60546875,54955.5,53414.51953125,51953.6640625,50630.1171875,49308.74609375,49214.93359375,48989.38671875,49373.30078125,49492.796875,49740.59765625,50138.5625,50964.171875,51954.59375,53278.86328125,54554.19921875,56037.765625,57096.5234375,58758.95703125,59625.98046875,59356.765625,59269.8828125,59403.37890625,59135.24609375,58744.2265625,58303.43359375,57664.46484375,56441.890625,55467.47265625,54379.984375,53283.015625,53304.83984375,53567.27734375,54069.79296875,54881.59375,55770.09765625,56634.45703125,57473.55859375,58171.7734375,58270.5546875,58497.578125,58738.1796875,58496.31640625,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"variable=test_predicted_close\\u003cbr\\u003eDate=%{x}\\u003cbr\\u003eStock price=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"test_predicted_close\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Test predicted close price\",\"showlegend\":true,\"x\":[\"2020-08-23T00:00:00\",\"2020-08-24T00:00:00\",\"2020-08-25T00:00:00\",\"2020-08-26T00:00:00\",\"2020-08-27T00:00:00\",\"2020-08-28T00:00:00\",\"2020-08-29T00:00:00\",\"2020-08-30T00:00:00\",\"2020-08-31T00:00:00\",\"2020-09-01T00:00:00\",\"2020-09-02T00:00:00\",\"2020-09-03T00:00:00\",\"2020-09-04T00:00:00\",\"2020-09-05T00:00:00\",\"2020-09-06T00:00:00\",\"2020-09-07T00:00:00\",\"2020-09-08T00:00:00\",\"2020-09-09T00:00:00\",\"2020-09-10T00:00:00\",\"2020-09-11T00:00:00\",\"2020-09-12T00:00:00\",\"2020-09-13T00:00:00\",\"2020-09-14T00:00:00\",\"2020-09-15T00:00:00\",\"2020-09-16T00:00:00\",\"2020-09-17T00:00:00\",\"2020-09-18T00:00:00\",\"2020-09-19T00:00:00\",\"2020-09-20T00:00:00\",\"2020-09-21T00:00:00\",\"2020-09-22T00:00:00\",\"2020-09-23T00:00:00\",\"2020-09-24T00:00:00\",\"2020-09-25T00:00:00\",\"2020-09-26T00:00:00\",\"2020-09-27T00:00:00\",\"2020-09-28T00:00:00\",\"2020-09-29T00:00:00\",\"2020-09-30T00:00:00\",\"2020-10-01T00:00:00\",\"2020-10-02T00:00:00\",\"2020-10-03T00:00:00\",\"2020-10-04T00:00:00\",\"2020-10-05T00:00:00\",\"2020-10-06T00:00:00\",\"2020-10-07T00:00:00\",\"2020-10-08T00:00:00\",\"2020-10-09T00:00:00\",\"2020-10-10T00:00:00\",\"2020-10-11T00:00:00\",\"2020-10-12T00:00:00\",\"2020-10-13T00:00:00\",\"2020-10-14T00:00:00\",\"2020-10-15T00:00:00\",\"2020-10-16T00:00:00\",\"2020-10-17T00:00:00\",\"2020-10-18T00:00:00\",\"2020-10-19T00:00:00\",\"2020-10-20T00:00:00\",\"2020-10-21T00:00:00\",\"2020-10-22T00:00:00\",\"2020-10-23T00:00:00\",\"2020-10-24T00:00:00\",\"2020-10-25T00:00:00\",\"2020-10-26T00:00:00\",\"2020-10-27T00:00:00\",\"2020-10-28T00:00:00\",\"2020-10-29T00:00:00\",\"2020-10-30T00:00:00\",\"2020-10-31T00:00:00\",\"2020-11-01T00:00:00\",\"2020-11-02T00:00:00\",\"2020-11-03T00:00:00\",\"2020-11-04T00:00:00\",\"2020-11-05T00:00:00\",\"2020-11-06T00:00:00\",\"2020-11-07T00:00:00\",\"2020-11-08T00:00:00\",\"2020-11-09T00:00:00\",\"2020-11-10T00:00:00\",\"2020-11-11T00:00:00\",\"2020-11-12T00:00:00\",\"2020-11-13T00:00:00\",\"2020-11-14T00:00:00\",\"2020-11-15T00:00:00\",\"2020-11-16T00:00:00\",\"2020-11-17T00:00:00\",\"2020-11-18T00:00:00\",\"2020-11-19T00:00:00\",\"2020-11-20T00:00:00\",\"2020-11-21T00:00:00\",\"2020-11-22T00:00:00\",\"2020-11-23T00:00:00\",\"2020-11-24T00:00:00\",\"2020-11-25T00:00:00\",\"2020-11-26T00:00:00\",\"2020-11-27T00:00:00\",\"2020-11-28T00:00:00\",\"2020-11-29T00:00:00\",\"2020-11-30T00:00:00\",\"2020-12-01T00:00:00\",\"2020-12-02T00:00:00\",\"2020-12-03T00:00:00\",\"2020-12-04T00:00:00\",\"2020-12-05T00:00:00\",\"2020-12-06T00:00:00\",\"2020-12-07T00:00:00\",\"2020-12-08T00:00:00\",\"2020-12-09T00:00:00\",\"2020-12-10T00:00:00\",\"2020-12-11T00:00:00\",\"2020-12-12T00:00:00\",\"2020-12-13T00:00:00\",\"2020-12-14T00:00:00\",\"2020-12-15T00:00:00\",\"2020-12-16T00:00:00\",\"2020-12-17T00:00:00\",\"2020-12-18T00:00:00\",\"2020-12-19T00:00:00\",\"2020-12-20T00:00:00\",\"2020-12-21T00:00:00\",\"2020-12-22T00:00:00\",\"2020-12-23T00:00:00\",\"2020-12-24T00:00:00\",\"2020-12-25T00:00:00\",\"2020-12-26T00:00:00\",\"2020-12-27T00:00:00\",\"2020-12-28T00:00:00\",\"2020-12-29T00:00:00\",\"2020-12-30T00:00:00\",\"2020-12-31T00:00:00\",\"2021-01-01T00:00:00\",\"2021-01-02T00:00:00\",\"2021-01-03T00:00:00\",\"2021-01-04T00:00:00\",\"2021-01-05T00:00:00\",\"2021-01-06T00:00:00\",\"2021-01-07T00:00:00\",\"2021-01-08T00:00:00\",\"2021-01-09T00:00:00\",\"2021-01-10T00:00:00\",\"2021-01-11T00:00:00\",\"2021-01-12T00:00:00\",\"2021-01-13T00:00:00\",\"2021-01-14T00:00:00\",\"2021-01-15T00:00:00\",\"2021-01-16T00:00:00\",\"2021-01-17T00:00:00\",\"2021-01-18T00:00:00\",\"2021-01-19T00:00:00\",\"2021-01-20T00:00:00\",\"2021-01-21T00:00:00\",\"2021-01-22T00:00:00\",\"2021-01-23T00:00:00\",\"2021-01-24T00:00:00\",\"2021-01-25T00:00:00\",\"2021-01-26T00:00:00\",\"2021-01-27T00:00:00\",\"2021-01-28T00:00:00\",\"2021-01-29T00:00:00\",\"2021-01-30T00:00:00\",\"2021-01-31T00:00:00\",\"2021-02-01T00:00:00\",\"2021-02-02T00:00:00\",\"2021-02-03T00:00:00\",\"2021-02-04T00:00:00\",\"2021-02-05T00:00:00\",\"2021-02-06T00:00:00\",\"2021-02-07T00:00:00\",\"2021-02-08T00:00:00\",\"2021-02-09T00:00:00\",\"2021-02-10T00:00:00\",\"2021-02-11T00:00:00\",\"2021-02-12T00:00:00\",\"2021-02-13T00:00:00\",\"2021-02-14T00:00:00\",\"2021-02-15T00:00:00\",\"2021-02-16T00:00:00\",\"2021-02-17T00:00:00\",\"2021-02-18T00:00:00\",\"2021-02-19T00:00:00\",\"2021-02-20T00:00:00\",\"2021-02-21T00:00:00\",\"2021-02-22T00:00:00\",\"2021-02-23T00:00:00\",\"2021-02-24T00:00:00\",\"2021-02-25T00:00:00\",\"2021-02-26T00:00:00\",\"2021-02-27T00:00:00\",\"2021-02-28T00:00:00\",\"2021-03-01T00:00:00\",\"2021-03-02T00:00:00\",\"2021-03-03T00:00:00\",\"2021-03-04T00:00:00\",\"2021-03-05T00:00:00\",\"2021-03-06T00:00:00\",\"2021-03-07T00:00:00\",\"2021-03-08T00:00:00\",\"2021-03-09T00:00:00\",\"2021-03-10T00:00:00\",\"2021-03-11T00:00:00\",\"2021-03-12T00:00:00\",\"2021-03-13T00:00:00\",\"2021-03-14T00:00:00\",\"2021-03-15T00:00:00\",\"2021-03-16T00:00:00\",\"2021-03-17T00:00:00\",\"2021-03-18T00:00:00\",\"2021-03-19T00:00:00\",\"2021-03-20T00:00:00\",\"2021-03-21T00:00:00\",\"2021-03-22T00:00:00\",\"2021-03-23T00:00:00\",\"2021-03-24T00:00:00\",\"2021-03-25T00:00:00\",\"2021-03-26T00:00:00\",\"2021-03-27T00:00:00\",\"2021-03-28T00:00:00\",\"2021-03-29T00:00:00\",\"2021-03-30T00:00:00\",\"2021-03-31T00:00:00\",\"2021-04-01T00:00:00\",\"2021-04-02T00:00:00\",\"2021-04-03T00:00:00\",\"2021-04-04T00:00:00\",\"2021-04-05T00:00:00\",\"2021-04-06T00:00:00\",\"2021-04-07T00:00:00\",\"2021-04-08T00:00:00\",\"2021-04-09T00:00:00\",\"2021-04-10T00:00:00\",\"2021-04-11T00:00:00\",\"2021-04-12T00:00:00\",\"2021-04-13T00:00:00\",\"2021-04-14T00:00:00\",\"2021-04-15T00:00:00\",\"2021-04-16T00:00:00\",\"2021-04-17T00:00:00\",\"2021-04-18T00:00:00\",\"2021-04-19T00:00:00\",\"2021-04-20T00:00:00\",\"2021-04-21T00:00:00\",\"2021-04-22T00:00:00\",\"2021-04-23T00:00:00\",\"2021-04-24T00:00:00\",\"2021-04-25T00:00:00\",\"2021-04-26T00:00:00\",\"2021-04-27T00:00:00\",\"2021-04-28T00:00:00\",\"2021-04-29T00:00:00\",\"2021-04-30T00:00:00\",\"2021-05-01T00:00:00\",\"2021-05-02T00:00:00\",\"2021-05-03T00:00:00\",\"2021-05-04T00:00:00\",\"2021-05-05T00:00:00\",\"2021-05-06T00:00:00\",\"2021-05-07T00:00:00\",\"2021-05-08T00:00:00\",\"2021-05-09T00:00:00\",\"2021-05-10T00:00:00\",\"2021-05-11T00:00:00\",\"2021-05-12T00:00:00\",\"2021-05-13T00:00:00\",\"2021-05-14T00:00:00\",\"2021-05-15T00:00:00\",\"2021-05-16T00:00:00\",\"2021-05-17T00:00:00\",\"2021-05-18T00:00:00\",\"2021-05-19T00:00:00\",\"2021-05-20T00:00:00\",\"2021-05-21T00:00:00\",\"2021-05-22T00:00:00\",\"2021-05-23T00:00:00\",\"2021-05-24T00:00:00\",\"2021-05-25T00:00:00\",\"2021-05-26T00:00:00\",\"2021-05-27T00:00:00\",\"2021-05-28T00:00:00\",\"2021-05-29T00:00:00\",\"2021-05-30T00:00:00\",\"2021-05-31T00:00:00\",\"2021-06-01T00:00:00\",\"2021-06-02T00:00:00\",\"2021-06-03T00:00:00\",\"2021-06-04T00:00:00\",\"2021-06-05T00:00:00\",\"2021-06-06T00:00:00\",\"2021-06-07T00:00:00\",\"2021-06-08T00:00:00\",\"2021-06-09T00:00:00\",\"2021-06-10T00:00:00\",\"2021-06-11T00:00:00\",\"2021-06-12T00:00:00\",\"2021-06-13T00:00:00\",\"2021-06-14T00:00:00\",\"2021-06-15T00:00:00\",\"2021-06-16T00:00:00\",\"2021-06-17T00:00:00\",\"2021-06-18T00:00:00\",\"2021-06-19T00:00:00\",\"2021-06-20T00:00:00\",\"2021-06-21T00:00:00\",\"2021-06-22T00:00:00\",\"2021-06-23T00:00:00\",\"2021-06-24T00:00:00\",\"2021-06-25T00:00:00\",\"2021-06-26T00:00:00\",\"2021-06-27T00:00:00\",\"2021-06-28T00:00:00\",\"2021-06-29T00:00:00\",\"2021-06-30T00:00:00\",\"2021-07-01T00:00:00\",\"2021-07-02T00:00:00\",\"2021-07-03T00:00:00\",\"2021-07-04T00:00:00\",\"2021-07-05T00:00:00\",\"2021-07-06T00:00:00\",\"2021-07-07T00:00:00\",\"2021-07-08T00:00:00\",\"2021-07-09T00:00:00\",\"2021-07-10T00:00:00\",\"2021-07-11T00:00:00\",\"2021-07-12T00:00:00\",\"2021-07-13T00:00:00\",\"2021-07-14T00:00:00\",\"2021-07-15T00:00:00\",\"2021-07-16T00:00:00\",\"2021-07-17T00:00:00\",\"2021-07-18T00:00:00\",\"2021-07-19T00:00:00\",\"2021-07-20T00:00:00\",\"2021-07-21T00:00:00\",\"2021-07-22T00:00:00\",\"2021-07-23T00:00:00\",\"2021-07-24T00:00:00\",\"2021-07-25T00:00:00\",\"2021-07-26T00:00:00\",\"2021-07-27T00:00:00\",\"2021-07-28T00:00:00\",\"2021-07-29T00:00:00\",\"2021-07-30T00:00:00\",\"2021-07-31T00:00:00\",\"2021-08-01T00:00:00\",\"2021-08-02T00:00:00\",\"2021-08-03T00:00:00\",\"2021-08-04T00:00:00\",\"2021-08-05T00:00:00\",\"2021-08-06T00:00:00\",\"2021-08-07T00:00:00\",\"2021-08-08T00:00:00\",\"2021-08-09T00:00:00\",\"2021-08-10T00:00:00\",\"2021-08-11T00:00:00\",\"2021-08-12T00:00:00\",\"2021-08-13T00:00:00\",\"2021-08-14T00:00:00\",\"2021-08-15T00:00:00\",\"2021-08-16T00:00:00\",\"2021-08-17T00:00:00\",\"2021-08-18T00:00:00\",\"2021-08-19T00:00:00\",\"2021-08-20T00:00:00\",\"2021-08-21T00:00:00\",\"2021-08-22T00:00:00\",\"2021-08-23T00:00:00\",\"2021-08-24T00:00:00\",\"2021-08-25T00:00:00\",\"2021-08-26T00:00:00\",\"2021-08-27T00:00:00\",\"2021-08-28T00:00:00\",\"2021-08-29T00:00:00\",\"2021-08-30T00:00:00\",\"2021-08-31T00:00:00\",\"2021-09-01T00:00:00\",\"2021-09-02T00:00:00\",\"2021-09-03T00:00:00\",\"2021-09-04T00:00:00\",\"2021-09-05T00:00:00\",\"2021-09-06T00:00:00\",\"2021-09-07T00:00:00\",\"2021-09-08T00:00:00\",\"2021-09-09T00:00:00\",\"2021-09-10T00:00:00\"],\"xaxis\":\"x\",\"y\":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,51761.05078125,50614.98828125,49708.01953125,50232.984375,51074.3125,52104.68359375,52819.17578125,54414.19140625,55782.37109375,56630.1796875,57317.48046875,56913.3046875,57404.328125,57408.91796875,57483.55078125,57786.9296875,57795.66796875,57144.72265625,56966.9765625,55083.13671875,53593.8515625,52368.25390625,50871.8359375,49666.2890625,48124.1171875,46878.23046875,44661.71484375,43877.96875,42625.1328125,41799.296875,40691.88671875,40577.57421875,40446.58203125,40583.86328125,40542.2890625,40035.33984375,39426.85546875,39107.61328125,39110.71875,39027.41796875,39122.2265625,39553.03125,39446.54296875,39151.83203125,38948.44921875,38350.09375,37807.52734375,38052.609375,38132.30078125,38344.91015625,38225.703125,38811.94140625,39611.609375,40273.0390625,40475.78515625,40566.95703125,40161.390625,39718.39453125,39317.875,38234.09375,37447.8671875,37012.55078125,36799.640625,36123.48046875,35684.26171875,35815.91015625,35896.10546875,36254.87890625,36422.62109375,36308.890625,36283.15234375,36377.6171875,36568.640625,36459.9140625,36471.6640625,36368.86328125,36097.421875,36041.1171875,35970.4140625,36039.6953125,35889.58203125,35665.5546875,35489.95703125,35157.12890625,34833.38671875,34561.87890625,34382.33203125,34043.15234375,33587.671875,33628.140625,33714.64453125,34051.609375,34468.19140625,35045.55859375,35907.4140625,37056.265625,38175.2109375,39145.625,40402.05859375,41336.6171875,41774.86328125,41933.80859375,41801.828125,41949.484375,42234.2734375,42834.4296875,43706.1171875,44301.22265625,45324.265625,46058.46484375,46689.64453125,46959.9140625,47825.51171875,48371.75390625,48755.73046875,48806.8359375,48516.515625,48232.24609375,48355.59765625,48990.78515625,49460.95703125,49983.046875,50454.3984375,50506.8828125,50752.93359375,50507.98828125,50677.17578125,50804.55078125,50845.69921875,50492.41796875,50181.6171875,50221.4140625,50365.2578125,50687.58203125,51008.68359375,51669.80078125,52443.24609375,51845.2265625,null],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Date\"},\"showgrid\":false},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Stock price\"},\"showgrid\":false},\"legend\":{\"title\":{\"text\":\"Close Price\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"Comparision between original close price vs predicted close price\"},\"font\":{\"size\":15,\"color\":\"black\"},\"plot_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c0ca51e2-4e27-4519-ba01-7f9c6d6803d3');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# shift train predictions for plotting\n",
        "\n",
        "look_back=time_step\n",
        "trainPredictPlot = np.empty_like(closedf)\n",
        "trainPredictPlot[:, :] = np.nan\n",
        "trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n",
        "print(\"Train predicted data: \", trainPredictPlot.shape)\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = np.empty_like(closedf)\n",
        "testPredictPlot[:, :] = np.nan\n",
        "testPredictPlot[len(train_predict)+(look_back*2)+1:len(closedf)-1, :] = test_predict\n",
        "print(\"Test predicted data: \", testPredictPlot.shape)\n",
        "\n",
        "names = cycle(['Original close price','Train predicted close price','Test predicted close price'])\n",
        "\n",
        "\n",
        "plotdf = pd.DataFrame({'date': close_stock['Date'],\n",
        "                       'original_close': close_stock['Value'],\n",
        "                      'train_predicted_close': trainPredictPlot.reshape(1,-1)[0].tolist(),\n",
        "                      'test_predicted_close': testPredictPlot.reshape(1,-1)[0].tolist()})\n",
        "\n",
        "fig = px.line(plotdf,x=plotdf['date'], y=[plotdf['original_close'],plotdf['train_predicted_close'],\n",
        "                                          plotdf['test_predicted_close']],\n",
        "              labels={'value':'Stock price','date': 'Date'})\n",
        "fig.update_layout(title_text='Comparision between original close price vs predicted close price',\n",
        "                  plot_bgcolor='white', font_size=15, font_color='black', legend_title_text='Close Price')\n",
        "fig.for_each_trace(lambda t:  t.update(name = next(names)))\n",
        "\n",
        "fig.update_xaxes(showgrid=False)\n",
        "fig.update_yaxes(showgrid=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU6nUrwceW7O",
        "outputId": "aa0247e8-6c45-4968-e9d7-eaf65c372f93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output of predicted next days:  30\n"
          ]
        }
      ],
      "source": [
        "x_input=test_data[len(test_data)-time_step:].reshape(1,-1)\n",
        "temp_input=list(x_input)\n",
        "temp_input=temp_input[0].tolist()\n",
        "\n",
        "from numpy import array\n",
        "\n",
        "lst_output=[]\n",
        "n_steps=time_step\n",
        "i=0\n",
        "pred_days = 30\n",
        "while(i<pred_days):\n",
        "\n",
        "    if(len(temp_input)>time_step):\n",
        "\n",
        "        x_input=np.array(temp_input[1:])\n",
        "        #print(\"{} day input {}\".format(i,x_input))\n",
        "        x_input = x_input.reshape(1,-1)\n",
        "        x_input = x_input.reshape((1, n_steps, 1))\n",
        "\n",
        "        yhat = model.predict(x_input, verbose=0)\n",
        "        #print(\"{} day output {}\".format(i,yhat))\n",
        "        temp_input.extend(yhat[0].tolist())\n",
        "        temp_input=temp_input[1:]\n",
        "        #print(temp_input)\n",
        "\n",
        "        lst_output.extend(yhat.tolist())\n",
        "        i=i+1\n",
        "\n",
        "    else:\n",
        "\n",
        "        x_input = x_input.reshape((1, n_steps,1))\n",
        "        yhat = model.predict(x_input, verbose=0)\n",
        "        temp_input.extend(yhat[0].tolist())\n",
        "\n",
        "        lst_output.extend(yhat.tolist())\n",
        "        i=i+1\n",
        "\n",
        "print(\"Output of predicted next days: \", len(lst_output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYg7SdcKebd0",
        "outputId": "55cf9fb7-79e1-4dbb-c894-199314d69b80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
            "[16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39\n",
            " 40 41 42 43 44 45]\n"
          ]
        }
      ],
      "source": [
        "last_days=np.arange(1,time_step+1)\n",
        "day_pred=np.arange(time_step+1,time_step+pred_days+1)\n",
        "print(last_days)\n",
        "print(day_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "pkYtGXivefKc",
        "outputId": "a5c270e5-4854-4bc6-ecb2-c5c4e2e18dd6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"bda5391c-8e82-44e6-95a8-2c9a6cd2eed2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bda5391c-8e82-44e6-95a8-2c9a6cd2eed2\")) {                    Plotly.newPlot(                        \"bda5391c-8e82-44e6-95a8-2c9a6cd2eed2\",                        [{\"hovertemplate\":\"variable=last_original_days_value\\u003cbr\\u003eTimestamp=%{x}\\u003cbr\\u003eStock price=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"last_original_days_value\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Last 15 days close price\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45],\"xaxis\":\"x\",\"y\":[46962.8,49056.86000000001,48897.65,48806.78,47074.77,47155.87,48862.76,49329.01,50035.33,49947.380000000005,51769.060000000005,52677.4,46809.17,46078.38,46368.69000000001,null,50480.699930386545,50758.96347860098,51092.28353249788,51467.4588879323,51887.25081004143,52315.88706120014,52774.2347800231,53311.69099335909,53879.56571110725,54457.82941025972,55020.079682526586,55588.79838558197,56178.454293956755,56577.95235049009,56897.724051568504,57152.83373871326,57462.49672520637,57714.43111668587,57903.48701035261,58026.326683180334,58081.532876822945,58068.783921408656,57990.509857652185,57853.99762284756,57666.94499996662,57436.320441281794,57171.27083661318,56882.510022087095,56583.43984741211,56276.66870491505],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=next_predicted_days_value\\u003cbr\\u003eTimestamp=%{x}\\u003cbr\\u003eStock price=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"next_predicted_days_value\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Predicted next 30 days close price\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45],\"xaxis\":\"x\",\"y\":[46962.8,49056.86000000001,48897.65,48806.78,47074.77,47155.87,48862.76,49329.01,50035.33,49947.380000000005,51769.060000000005,52677.4,46809.17,46078.38,46368.69000000001,null,50480.699930386545,50758.96347860098,51092.28353249788,51467.4588879323,51887.25081004143,52315.88706120014,52774.2347800231,53311.69099335909,53879.56571110725,54457.82941025972,55020.079682526586,55588.79838558197,56178.454293956755,56577.95235049009,56897.724051568504,57152.83373871326,57462.49672520637,57714.43111668587,57903.48701035261,58026.326683180334,58081.532876822945,58068.783921408656,57990.509857652185,57853.99762284756,57666.94499996662,57436.320441281794,57171.27083661318,56882.510022087095,56583.43984741211,56276.66870491505],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Timestamp\"},\"showgrid\":false},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Stock price\"},\"showgrid\":false},\"legend\":{\"title\":{\"text\":\"Close Price\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"Compare last 15 days vs next 30 days\"},\"font\":{\"size\":15,\"color\":\"black\"},\"plot_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('bda5391c-8e82-44e6-95a8-2c9a6cd2eed2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "temp_mat = np.empty((len(last_days)+pred_days+1,1))\n",
        "temp_mat[:] = np.nan\n",
        "temp_mat = temp_mat.reshape(1,-1).tolist()[0]\n",
        "\n",
        "last_original_days_value = temp_mat\n",
        "next_predicted_days_value = temp_mat\n",
        "\n",
        "last_original_days_value[0:time_step+1] = scaler.inverse_transform(closedf[len(closedf)-time_step:]).reshape(1,-1).tolist()[0]\n",
        "next_predicted_days_value[time_step+1:] = scaler.inverse_transform(np.array(lst_output).reshape(-1,1)).reshape(1,-1).tolist()[0]\n",
        "\n",
        "new_pred_plot = pd.DataFrame({\n",
        "    'last_original_days_value':last_original_days_value,\n",
        "    'next_predicted_days_value':next_predicted_days_value\n",
        "})\n",
        "\n",
        "names = cycle(['Last 15 days close price','Predicted next 30 days close price'])\n",
        "\n",
        "fig = px.line(new_pred_plot,x=new_pred_plot.index, y=[new_pred_plot['last_original_days_value'],\n",
        "                                                      new_pred_plot['next_predicted_days_value']],\n",
        "              labels={'value': 'Stock price','index': 'Timestamp'})\n",
        "fig.update_layout(title_text='Compare last 15 days vs next 30 days',\n",
        "                  plot_bgcolor='white', font_size=15, font_color='black',legend_title_text='Close Price')\n",
        "\n",
        "fig.for_each_trace(lambda t:  t.update(name = next(names)))\n",
        "fig.update_xaxes(showgrid=False)\n",
        "fig.update_yaxes(showgrid=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "8Lm-ZmkvejRm",
        "outputId": "7f369980-95eb-4e64-967d-1926adb1ad76"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"57821410-9e0d-4aab-948a-98c753c2bdca\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"57821410-9e0d-4aab-948a-98c753c2bdca\")) {                    Plotly.newPlot(                        \"57821410-9e0d-4aab-948a-98c753c2bdca\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eTimestamp=%{x}\\u003cbr\\u003eStock price=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Close price\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413],\"xaxis\":\"x\",\"y\":[11683.44,11653.02,11763.93,11337.4,11467.37,11302.01,11534.75,11481.64,11707.78,11659.57,11923.25,11397.44,10187.51,10467.89,10159.62,10254.93,10367.74,10121.52,10227.83,10352.66,10395.44,10446.44,10330.77,10674.64,10785.62,10948.43,10943.89,10931.79,11081.43,10919.65,10430.46,10532.22,10234.48,10732.43,10692.84,10732.4,10774.24,10692.33,10840.8,10777.92,10619.24,10575.06,10551.77,10673.46,10788.56,10603.74,10670.8,10923.3,11063.19,11302.67,11376.61,11540.04,11428.24,11431.32,11503.73,11327.57,11366.51,11508.2,11758.16,11925.46,12831.56,12990.25,12944.52,13128.46,13036.77,13076.37,13651.47,13289.0,13458.660000000002,13564.72,13810.32,13758.88,13575.169999999998,14023.310000000001,14155.589999999998,15591.39,15595.77,14839.84,15490.6,15328.53,15317.04,15708.649999999998,16295.57,16339.330000000002,16091.070000000002,15968.159999999998,16725.15,17679.72,17798.45,17820.57,18687.45,18699.75,18422.28,18398.91,19172.52,18739.8,17151.44,17138.87,17732.42,18191.6,19709.73,18792.52,19226.97,19454.54,18670.49,19155.06,19377.66,19181.41,18318.87,18554.15,18247.76,18029.36,18803.44,19164.48,19276.59,19439.75,21379.48,22847.46,23150.79,23869.92,23490.58,22745.48,23824.99,23253.37,23715.53,24693.58,26443.21,26246.58,27036.689999999995,27376.37,28856.59,28982.56,29393.749999999996,32195.46,33000.78,32035.03,34046.67,36860.41000000001,39486.04000000001,40670.25000000001,40240.72,38240.090000000004,35544.94,34011.82,37393.130000000005,39158.47000000001,36828.52,36065.2,35793.01,36632.35,36020.13,35538.98,30797.88,33002.38,32099.74,32276.840000000004,32243.259999999995,32541.8,30419.170000000002,33403.17,34314.26,34318.1,33136.46,33522.9,35529.66,37676.25,37002.09,38278.61000000001,39323.26,38928.1,46364.30000000001,46589.58000000001,44878.170000000006,48013.38,47471.40000000001,47185.19000000001,48720.37000000001,47951.85,49160.1,52118.23000000001,51608.15000000001,55916.5,56001.2,57487.85999999999,54123.4,48880.43,50624.840000000004,46800.420000000006,46340.31,46155.87,45113.920000000006,49618.43000000001,48356.04,50477.7,48448.91,48861.380000000005,48881.59,51169.7,52299.33,54881.520000000004,55997.23000000001,57764.0,57253.27999999999,61258.73,59133.47,55754.72,56872.37999999999,58913.0,57665.9,58075.1,58085.8,57411.16999999999,54204.96,54477.46000000001,52508.23000000001,51415.920000000006,55074.47000000001,55863.93000000001,55783.71,57627.670000000006,58730.13,58735.25,58736.91999999999,59031.32,57076.490000000005,58206.55,59054.100000000006,58020.46,55947.27,58048.590000000004,58102.58,59774.00000000001,59964.869999999995,59834.74,63554.44,62969.119999999995,63252.63,61455.98,60087.09,56251.48000000001,55703.14,56507.91000000001,53808.8,51731.71,51153.130000000005,50110.53,49075.58000000001,54056.64,55071.45999999999,54884.1,53584.149999999994,57796.62,57857.5,56610.45999999999,57213.33,53241.72000000001,57473.23000000001,56428.16,57380.27,58928.81,58280.73,55883.5,56750.0,49007.090000000004,49702.27,49922.520000000004,46736.58000000001,46441.64,43596.24,42912.19000000001,36964.270000000004,40784.32,37280.350000000006,37528.30000000001,34754.54,38728.59,38410.5,39266.04000000001,38445.29,35689.62,34647.67,35684.59,37310.54,36662.64,37585.240000000005,39188.59,36885.51000000001,35530.38,35816.17,33514.87,33450.19,37338.36000000001,36704.57,37313.18000000001,35494.9,39066.82000000001,40525.8,40188.56,38324.87,38068.04,35729.82,35524.17,35592.35,31686.55,32447.59,33674.66,34639.38,31640.58,32160.909999999996,34644.45,34456.67,35847.7,35047.36,33536.88,33856.86,34688.98,35309.3,33747.97,34211.01,33839.04,32877.41,33818.52,33515.57,34227.64,33158.25,32686.56,32814.61,31738.589999999997,31421.250000000004,31520.66,31783.49,30815.94,29790.24,32118.06,32297.89,33581.63,34279.34,35365.2,37318.14,39405.950000000004,40002.53,40005.93,42214.15,41659.060000000005,40000.46,39193.94000000001,38138.00000000001,39750.14000000001,40882.00000000001,42825.950000000004,44634.13,43816.14,46333.46000000001,45608.37,45611.46000000001,44417.78,47833.98000000001,47112.19,47056.41,45982.55,44648.57,44777.86000000001,46734.65,49327.75,48932.02,49335.68000000001,49523.50000000001,47744.58000000001,48972.090000000004,46962.8,49056.86000000001,48897.65,48806.78,47074.77,47155.87,48862.76,49329.01,50035.33,49947.380000000005,51769.060000000005,52677.4,46809.17,46078.38,46368.69000000001,50480.699930386545,50758.96347860098,51092.28353249788,51467.4588879323,51887.25081004143,52315.88706120014,52774.2347800231,53311.69099335909,53879.56571110725,54457.82941025972,55020.079682526586,55588.79838558197,56178.454293956755,56577.95235049009,56897.724051568504,57152.83373871326,57462.49672520637,57714.43111668587,57903.48701035261,58026.326683180334,58081.532876822945,58068.783921408656,57990.509857652185,57853.99762284756,57666.94499996662,57436.320441281794,57171.27083661318,56882.510022087095,56583.43984741211,56276.66870491505],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Timestamp\"},\"showgrid\":false},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Stock price\"},\"showgrid\":false},\"legend\":{\"title\":{\"text\":\"Stock\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"Plotting whole closing stock price with prediction\"},\"font\":{\"size\":15,\"color\":\"black\"},\"plot_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('57821410-9e0d-4aab-948a-98c753c2bdca');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "lstmdf=closedf.tolist()\n",
        "lstmdf.extend((np.array(lst_output).reshape(-1,1)).tolist())\n",
        "lstmdf=scaler.inverse_transform(lstmdf).reshape(1,-1).tolist()[0]\n",
        "\n",
        "names = cycle(['Close price'])\n",
        "\n",
        "fig = px.line(lstmdf,labels={'value': 'Stock price','index': 'Timestamp'})\n",
        "fig.update_layout(title_text='Plotting whole closing stock price with prediction',\n",
        "                  plot_bgcolor='white', font_size=15, font_color='black',legend_title_text='Stock')\n",
        "\n",
        "fig.for_each_trace(lambda t:  t.update(name = next(names)))\n",
        "\n",
        "fig.update_xaxes(showgrid=False)\n",
        "fig.update_yaxes(showgrid=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78IpnCEaVlWs",
        "outputId": "c9360c11-94af-4efd-8d4e-a80936afa3b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "46368.69000000001\n",
            "50480.699930386545\n",
            "50758.96347860098\n",
            "51092.28353249788\n",
            "51467.4588879323\n",
            "51887.25081004143\n",
            "52315.88706120014\n",
            "52774.2347800231\n",
            "53311.69099335909\n",
            "53879.56571110725\n",
            "54457.82941025972\n",
            "55020.079682526586\n",
            "55588.79838558197\n",
            "56178.454293956755\n",
            "56577.95235049009\n",
            "56897.724051568504\n",
            "57152.83373871326\n",
            "57462.49672520637\n",
            "57714.43111668587\n",
            "57903.48701035261\n",
            "58026.326683180334\n",
            "58081.532876822945\n",
            "58068.783921408656\n",
            "57990.509857652185\n",
            "57853.99762284756\n",
            "57666.94499996662\n",
            "57436.320441281794\n",
            "57171.27083661318\n",
            "56882.510022087095\n",
            "56583.43984741211\n",
            "56276.66870491505\n"
          ]
        }
      ],
      "source": [
        "predict = lstmdf[(len(lstmdf)-31):]\n",
        "for day in predict:\n",
        "  print(day)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_JEq__86huY"
      },
      "source": [
        "# Simulation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4V2PlI2sAJX"
      },
      "source": [
        "## Simple Bitcoin Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Bv4lshw6ka_",
        "outputId": "d9c97a59-8f2c-4575-f6e0-916b38e58a63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000.0 0.0\n",
            "Buying 0.050267928056541365 amount of bitcoin with 50.0 for 994.67 dolallars on day 116\n",
            "950.0 0.050267928056541365\n",
            "950.0 0.050267928056541365\n",
            "Buying 0.06049260079977586 amount of bitcoin with 47.5 for 785.22 dolallars on day 122\n",
            "902.5 0.11076052885631722\n",
            "Selling 0.04992587183572313 bitcoin for 45.125 dollars on day 128\n",
            "Selling 0.046771951784108416 bitcoin for 47.38125 dollars on day 145\n",
            "995.00625 0.014062705236485673\n",
            "Buying 0.0429847939330563 amount of bitcoin with 49.75031250000001 for 1157.3933 dolallars on day 178\n",
            "945.2559375000001 0.05704749916954197\n",
            "Selling 0.0375877294181633 bitcoin for 47.26279687500001 dollars on day 185\n",
            "992.5187343750001 0.01945976975137867\n",
            "Buying 0.04547948610526338 amount of bitcoin with 49.62593671875001 for 1091.171888 dolallars on day 187\n",
            "942.8927976562501 0.06493925585664205\n",
            "942.8927976562501 0.06493925585664205\n",
            "Buying 0.049509596333229544 amount of bitcoin with 47.144639882812506 for 952.2323625 dolallars on day 188\n",
            "895.7481577734376 0.1144488521898716\n",
            "Selling 0.04003773022987894 bitcoin for 44.78740788867188 dollars on day 191\n",
            "940.5355656621094 0.07441112195999265\n",
            "Buying 0.049926525126458406 amount of bitcoin with 47.02677828310547 for 941.9197143 dolallars on day 194\n",
            "893.5087873790039 0.12433764708645106\n",
            "Selling 0.04110242334086815 bitcoin for 44.6754393689502 dollars on day 202\n",
            "Selling 0.039404463747680946 bitcoin for 46.90921133739771 dollars on day 208\n",
            "Selling 0.03762455149478388 bitcoin for 49.25467190426759 dollars on day 227\n",
            "1034.3481099896194 0.006206208503118087\n",
            "Buying 0.01940673219138702 amount of bitcoin with 51.717405499480975 for 2664.920863 dolallars on day 269\n",
            "982.6307044901384 0.025612940694505107\n",
            "Selling 0.01737637007919597 bitcoin for 49.131535224506926 dollars on day 271\n",
            "1031.7622397146454 0.008236570615309136\n",
            "Buying 0.019410992978012628 amount of bitcoin with 51.58811198573227 for 2657.675063 dolallars on day 274\n",
            "980.1741277289132 0.027647563593321763\n",
            "980.1741277289132 0.027647563593321763\n",
            "Buying 0.020065139272444743 amount of bitcoin with 49.00870638644566 for 2442.48025 dolallars on day 277\n",
            "931.1654213424675 0.04771270286576651\n",
            "Selling 0.01778927422275792 bitcoin for 46.55827106712338 dollars on day 281\n",
            "Selling 0.017924833896330976 bitcoin for 48.886184620479554 dollars on day 284\n",
            "1026.6098770300705 0.011998594746677613\n",
            "Buying 0.019825115846968618 amount of bitcoin with 51.330493851503526 for 2589.164888 dolallars on day 286\n",
            "975.279383178567 0.031823710593646234\n",
            "975.279383178567 0.031823710593646234\n",
            "Buying 0.01968160915093224 amount of bitcoin with 48.76396915892835 for 2477.641375 dolallars on day 292\n",
            "926.5154140196386 0.051505319744578476\n",
            "Selling 0.017819455707990352 bitcoin for 46.325770700981934 dollars on day 296\n",
            "972.8411847206205 0.033685864036588124\n",
            "Buying 0.019525544128139234 amount of bitcoin with 48.64205923603103 for 2491.201214 dolallars on day 299\n",
            "924.1991254845894 0.05321140816472736\n",
            "924.1991254845894 0.05321140816472736\n",
            "Buying 0.019529430886842815 amount of bitcoin with 46.20995627422948 for 2366.170143 dolallars on day 302\n",
            "877.9891692103599 0.07274083905157017\n",
            "877.9891692103599 0.07274083905157017\n",
            "Buying 0.02132081217682932 amount of bitcoin with 43.899458460518 for 2058.9956 dolallars on day 307\n",
            "834.089710749842 0.09406165122839949\n",
            "834.089710749842 0.09406165122839949\n",
            "Buying 0.021594954810293245 amount of bitcoin with 41.704485537492104 for 1931.2143 dolallars on day 308\n",
            "792.3852252123498 0.11565660603869274\n",
            "Selling 0.017076367963204307 bitcoin for 39.61926126061749 dollars on day 310\n",
            "Selling 0.015509766699886868 bitcoin for 41.60022432364837 dollars on day 313\n",
            "Selling 0.015557801035256441 bitcoin for 43.68023553983079 dollars on day 314\n",
            "917.2849463364464 0.06751267034034512\n",
            "Buying 0.01790874057818389 amount of bitcoin with 45.86424731682232 for 2560.997917 dolallars on day 317\n",
            "871.4206990196241 0.085421410918529\n",
            "Selling 0.015663812885287037 bitcoin for 43.57103495098121 dollars on day 320\n",
            "Selling 0.014216268361091416 bitcoin for 45.749586698530265 dollars on day 328\n",
            "Selling 0.013894088810387653 bitcoin for 48.03706603345678 dollars on day 331\n",
            "1008.7783867025923 0.0416472408617629\n",
            "Buying 0.015023537950341458 amount of bitcoin with 50.43891933512962 for 3357.326317 dolallars on day 332\n",
            "958.3394673674627 0.05667077881210436\n",
            "Selling 0.013191159097843201 bitcoin for 47.916973368373135 dollars on day 334\n",
            "Selling 0.01305875830138344 bitcoin for 50.3128220367918 dollars on day 335\n",
            "Selling 0.012334476258333284 bitcoin for 52.82846313863139 dollars on day 337\n",
            "1109.3977259112592 0.018086385154544436\n",
            "Buying 0.0132704029683378 amount of bitcoin with 55.46988629556296 for 4179.97 dolallars on day 339\n",
            "1053.9278396156963 0.031356788122882236\n",
            "Selling 0.012173650036044733 bitcoin for 52.69639198078482 dollars on day 340\n",
            "1106.624231596481 0.019183138086837505\n",
            "Buying 0.013395960401868735 amount of bitcoin with 55.33121157982405 for 4130.440067 dolallars on day 341\n",
            "1051.293020016657 0.03257909848870624\n",
            "1051.293020016657 0.03257909848870624\n",
            "Buying 0.01318639207502555 amount of bitcoin with 52.56465100083285 for 3986.28 dolallars on day 345\n",
            "998.7283690158241 0.04576549056373179\n",
            "Selling 0.011960962035662992 bitcoin for 49.93641845079121 dollars on day 346\n",
            "Selling 0.012080509970150819 bitcoin for 52.433239373330764 dollars on day 347\n",
            "Selling 0.011947715968156388 bitcoin for 55.0549013419973 dollars on day 352\n",
            "1156.1529281819433 0.009776302589761593\n",
            "Buying 0.01262068911451508 amount of bitcoin with 57.80764640909717 for 4580.38748 dolallars on day 356\n",
            "1098.3452817728462 0.022396991704276673\n",
            "1098.3452817728462 0.022396991704276673\n",
            "Buying 0.01264180966478856 amount of bitcoin with 54.91726408864231 for 4344.098317 dolallars on day 358\n",
            "1043.428017684204 0.03503880136906523\n",
            "Selling 0.011622778711307718 bitcoin for 52.1714008842102 dollars on day 359\n",
            "1095.5994185684142 0.023416022657757513\n",
            "Buying 0.012492524761214477 amount of bitcoin with 54.77997092842071 for 4385.02 dolallars on day 360\n",
            "1040.8194476399935 0.03590854741897199\n",
            "Selling 0.011180406120448941 bitcoin for 52.04097238199968 dollars on day 361\n",
            "1092.8604200219931 0.02472814129852305\n",
            "Buying 0.012675988791136975 amount of bitcoin with 54.64302100109966 for 4310.750183 dolallars on day 362\n",
            "1038.2173990208935 0.03740413008966002\n",
            "1038.2173990208935 0.03740413008966002\n",
            "Buying 0.012363881672429237 amount of bitcoin with 51.910869951044674 for 4198.59 dolallars on day 366\n",
            "986.3065290698488 0.04976801176208926\n",
            "986.3065290698488 0.04976801176208926\n",
            "Buying 0.01244936868230197 amount of bitcoin with 49.315326453492446 for 3961.271267 dolallars on day 367\n",
            "936.9912026163563 0.062217380444391235\n",
            "936.9912026163563 0.062217380444391235\n",
            "Buying 0.014112886114060247 amount of bitcoin with 46.84956013081782 for 3319.63 dolallars on day 368\n",
            "890.1416424855385 0.07633026655845149\n",
            "890.1416424855385 0.07633026655845149\n",
            "Buying 0.013837416677582577 amount of bitcoin with 44.50708212427693 for 3216.43 dolallars on day 369\n",
            "845.6345603612615 0.09016768323603407\n",
            "Selling 0.01123430637600304 bitcoin for 42.28172801806308 dollars on day 370\n",
            "Selling 0.011258219889719643 bitcoin for 44.39581441896623 dollars on day 373\n",
            "932.3121027982908 0.06767515697031137\n",
            "Buying 0.01281527765249664 amount of bitcoin with 46.61560513991454 for 3637.50255 dolallars on day 376\n",
            "885.6964976583762 0.08049043462280801\n",
            "Selling 0.011726771132194853 bitcoin for 44.28482488291881 dollars on day 377\n",
            "Selling 0.01179414519951269 bitcoin for 46.49906612706475 dollars on day 379\n",
            "Selling 0.011619263842064983 bitcoin for 48.82401943341799 dollars on day 382\n",
            "Selling 0.011756128695411549 bitcoin for 51.26522040508888 dollars on day 385\n",
            "1076.5696285068666 0.033594125753623943\n",
            "Buying 0.01273994128653685 amount of bitcoin with 53.828481425343334 for 4225.175 dolallars on day 388\n",
            "1022.7411470815233 0.04633406704016079\n",
            "Selling 0.01178584965656265 bitcoin for 51.13705735407617 dollars on day 389\n",
            "Selling 0.01166680426223346 bitcoin for 53.69391022177997 dollars on day 392\n",
            "Selling 0.011789064156190975 bitcoin for 56.37860573286898 dollars on day 394\n",
            "Selling 0.01063975451098971 bitcoin for 59.19753601951242 dollars on day 397\n",
            "1243.1482564097607 0.0004525944541839931\n",
            "Buying 0.011092183608621471 amount of bitcoin with 62.15741282048804 for 5603.71294 dolallars on day 401\n",
            "1180.9908435892728 0.011544778062805464\n",
            "Selling 0.010309588101170167 bitcoin for 59.04954217946364 dollars on day 403\n",
            "1240.0403857687365 0.0012351899616352968\n",
            "Buying 0.010551595739302231 amount of bitcoin with 62.00201928843683 for 5876.079867 dolallars on day 407\n",
            "1178.0383664802998 0.011786785700937528\n",
            "1178.0383664802998 0.011786785700937528\n",
            "Buying 0.010389037009285323 amount of bitcoin with 58.90191832401499 for 5669.622533 dolallars on day 409\n",
            "1119.1364481562848 0.02217582271022285\n",
            "Selling 0.009495249975190637 bitcoin for 55.95682240781424 dollars on day 410\n",
            "1175.0932705640992 0.012680572735032213\n",
            "Buying 0.01017097902776516 amount of bitcoin with 58.75466352820496 for 5776.69695 dolallars on day 412\n",
            "1116.3386070358943 0.02285155176279737\n",
            "Selling 0.009067911404855692 bitcoin for 55.81693035179472 dollars on day 413\n",
            "Selling 0.009173741119966704 bitcoin for 58.60777686938445 dollars on day 415\n",
            "1230.7633142570735 0.004609899237974975\n",
            "Buying 0.008804912675801536 amount of bitcoin with 61.538165712853676 for 6989.071667 dolallars on day 421\n",
            "1169.2251485442198 0.013414811913776511\n",
            "Selling 0.008243120224237944 bitcoin for 58.461257427210995 dollars on day 422\n",
            "1227.6864059714308 0.005171691689538568\n",
            "Buying 0.009135390055609155 amount of bitcoin with 61.38432029857154 for 6719.39785 dolallars on day 425\n",
            "1166.3020856728592 0.014307081745147722\n",
            "1166.3020856728592 0.014307081745147722\n",
            "Buying 0.008877021059371274 amount of bitcoin with 58.31510428364297 for 6569.22 dolallars on day 426\n",
            "1107.9869813892162 0.023184102804518998\n",
            "1107.9869813892162 0.023184102804518998\n",
            "Buying 0.00969146716020298 amount of bitcoin with 55.39934906946081 for 5716.301583 dolallars on day 427\n",
            "1052.5876323197554 0.032875569964721976\n",
            "Selling 0.008034740984315631 bitcoin for 52.62938161598777 dollars on day 428\n",
            "Selling 0.007568497034453104 bitcoin for 55.26085069678716 dollars on day 430\n",
            "Selling 0.0074246532686847305 bitcoin for 58.023893231626516 dollars on day 431\n",
            "Selling 0.007608356627727415 bitcoin for 60.925087893207845 dollars on day 434\n",
            "1279.4268457573646 0.0022393220495410954\n",
            "Buying 0.004224529018424746 amount of bitcoin with 63.97134228786823 for 15142.83415 dolallars on day 454\n",
            "1215.4555034694963 0.006463851067965842\n",
            "1215.4555034694963 0.006463851067965842\n",
            "Buying 0.0040869920737679356 amount of bitcoin with 60.77277517347482 for 14869.805 dolallars on day 455\n",
            "1154.6827282960214 0.010550843141733778\n",
            "Selling 0.0038313310758082727 bitcoin for 57.73413641480107 dollars on day 456\n",
            "Selling 0.00350888302191376 bitcoin for 60.62084323554112 dollars on day 457\n",
            "1273.0377079463635 0.003210629044011745\n",
            "Buying 0.0037869167568152645 amount of bitcoin with 63.65188539731818 for 16808.36667 dolallars on day 458\n",
            "1209.3858225490453 0.00699754580082701\n",
            "1209.3858225490453 0.00699754580082701\n",
            "Buying 0.0037206582404256546 amount of bitcoin with 60.469291127452266 for 16252.31 dolallars on day 459\n",
            "1148.916531421593 0.010718204041252665\n",
            "Selling 0.003232396455701396 bitcoin for 57.44582657107965 dollars on day 460\n",
            "Selling 0.0030934456895779345 bitcoin for 60.31811789963363 dollars on day 461\n",
            "1266.680475892306 0.004392361895973335\n",
            "Buying 0.003284976778645911 amount of bitcoin with 63.334023794615305 for 19279.9 dolallars on day 462\n",
            "1203.3464520976906 0.007677338674619246\n",
            "1203.3464520976906 0.007677338674619246\n",
            "Buying 0.0031730712689162274 amount of bitcoin with 60.16732260488453 for 18961.85667 dolallars on day 463\n",
            "1143.1791294928062 0.010850409943535474\n",
            "1143.1791294928062 0.010850409943535474\n",
            "Buying 0.0032225628128235333 amount of bitcoin with 57.15895647464031 for 17737.11167 dolallars on day 464\n",
            "1086.020173018166 0.014072972756359006\n",
            "1086.020173018166 0.014072972756359006\n",
            "Buying 0.0031238103940362393 amount of bitcoin with 54.3010086509083 for 17382.94 dolallars on day 465\n",
            "1031.7191643672577 0.017196783150395245\n",
            "1031.7191643672577 0.017196783150395245\n",
            "Buying 0.003214577103760203 amount of bitcoin with 51.58595821836289 for 16047.51 dolallars on day 466\n",
            "980.1332061488948 0.020411360254155447\n",
            "980.1332061488948 0.020411360254155447\n",
            "Buying 0.0032260442195956042 amount of bitcoin with 49.00666030744475 for 15190.945 dolallars on day 467\n",
            "931.12654584145 0.02363740447375105\n",
            "931.12654584145 0.02363740447375105\n",
            "Buying 0.0033793747004576963 amount of bitcoin with 46.5563272920725 for 13776.61 dolallars on day 468\n",
            "884.5702185493775 0.027016779174208746\n",
            "Selling 0.0031706900893758144 bitcoin for 44.228510927468875 dollars on day 469\n",
            "Selling 0.0032891736873398797 bitcoin for 46.43993647384232 dollars on day 470\n",
            "975.2386659506886 0.020556915397493054\n",
            "Buying 0.0035486606664125184 amount of bitcoin with 48.76193329753443 for 13740.94 dolallars on day 471\n",
            "926.4767326531542 0.024105576063905572\n",
            "Selling 0.0029715107310796618 bitcoin for 46.32383663265771 dollars on day 472\n",
            "972.8005692858119 0.02113406533282591\n",
            "Buying 0.0033823408246246964 amount of bitcoin with 48.640028464290594 for 14380.58167 dolallars on day 473\n",
            "924.1605408215213 0.024516406157450606\n",
            "924.1605408215213 0.024516406157450606\n",
            "Buying 0.003250879029990556 amount of bitcoin with 46.208027041076065 for 14214.01 dolallars on day 474\n",
            "877.9525137804452 0.027767285187441162\n",
            "877.9525137804452 0.027767285187441162\n",
            "Buying 0.0033216586497886707 amount of bitcoin with 43.89762568902226 for 13215.574 dolallars on day 475\n",
            "834.054888091423 0.031088943837229835\n",
            "Selling 0.002943949991763211 bitcoin for 41.70274440457115 dollars on day 476\n",
            "875.7576324959941 0.028144993845466624\n",
            "Buying 0.003173954887271652 amount of bitcoin with 43.78788162479971 for 13796.0 dolallars on day 477\n",
            "831.9697508711944 0.03131894873273828\n",
            "Selling 0.0027721501316698718 bitcoin for 41.59848754355972 dollars on day 478\n",
            "Selling 0.002543269286620666 bitcoin for 43.67841192073771 dollars on day 481\n",
            "Selling 0.0026480632946614844 bitcoin for 45.86233251677459 dollars on day 482\n",
            "963.1089828522664 0.023355466019786258\n",
            "Buying 0.002806924078387575 amount of bitcoin with 48.15544914261332 for 17155.95 dolallars on day 483\n",
            "914.9535337096531 0.026162390098173834\n",
            "914.9535337096531 0.026162390098173834\n",
            "Buying 0.0029967218897901634 amount of bitcoin with 45.74767668548266 for 15265.90667 dolallars on day 484\n",
            "869.2058570241704 0.029159111987963997\n",
            "869.2058570241704 0.029159111987963997\n",
            "Buying 0.002953618636060857 amount of bitcoin with 43.46029285120852 for 14714.25333 dolallars on day 485\n",
            "825.7455641729618 0.03211273062402485\n",
            "825.7455641729618 0.03211273062402485\n",
            "Buying 0.0028597407437511756 amount of bitcoin with 41.2872782086481 for 14437.42 dolallars on day 486\n",
            "784.4582859643137 0.03497247136777603\n",
            "784.4582859643137 0.03497247136777603\n",
            "Buying 0.0029498023582388124 amount of bitcoin with 39.22291429821569 for 13296.794 dolallars on day 487\n",
            "745.2353716660981 0.037922273726014845\n",
            "Selling 0.0026782207010240516 bitcoin for 37.26176858330491 dollars on day 488\n",
            "782.4971402494031 0.03524405302499079\n",
            "Buying 0.0028369456886947505 amount of bitcoin with 39.12485701247016 for 13791.19 dolallars on day 489\n",
            "743.3722832369328 0.03808099871368554\n",
            "Selling 0.0026525902265317042 bitcoin for 37.16861416184664 dollars on day 491\n",
            "780.5408973987795 0.03542840848715383\n",
            "Buying 0.0028793449728229882 amount of bitcoin with 39.027044869938976 for 13554.14 dolallars on day 492\n",
            "741.5138525288405 0.03830775345997682\n",
            "741.5138525288405 0.03830775345997682\n",
            "Buying 0.003335060761467342 amount of bitcoin with 37.07569262644203 for 11116.94667 dolallars on day 493\n",
            "704.4381599023985 0.04164281422144416\n",
            "Selling 0.0031045036373376056 bitcoin for 35.22190799511993 dollars on day 494\n",
            "739.6600678975185 0.038538310584106554\n",
            "Buying 0.003309494326966871 amount of bitcoin with 36.98300339487593 for 11174.82 dolallars on day 495\n",
            "702.6770645026426 0.04184780491107343\n",
            "Selling 0.002712872665780709 bitcoin for 35.13385322513213 dollars on day 496\n",
            "737.8109177277747 0.03913493224529272\n",
            "Buying 0.00320641589079232 amount of bitcoin with 36.890545886388736 for 11505.228 dolallars on day 497\n",
            "700.920371841386 0.04234134813608504\n",
            "700.920371841386 0.04234134813608504\n",
            "Buying 0.003122678316016847 amount of bitcoin with 35.0460185920693 for 11223.064 dolallars on day 499\n",
            "665.8743532493166 0.04546402645210189\n",
            "Selling 0.0029225447670402755 bitcoin for 33.29371766246583 dollars on day 501\n",
            "699.1680709117825 0.04254148168506161\n",
            "Buying 0.0031867815041173556 amount of bitcoin with 34.95840354558913 for 10969.815 dolallars on day 502\n",
            "664.2096673661933 0.04572826318917897\n",
            "Selling 0.0028816596034142213 bitcoin for 33.21048336830967 dollars on day 503\n",
            "697.4201507345031 0.042846603585764746\n",
            "Buying 0.003109968828678413 amount of bitcoin with 34.87100753672515 for 11212.655 dolallars on day 505\n",
            "662.5491431977779 0.04595657241444316\n",
            "662.5491431977779 0.04595657241444316\n",
            "Buying 0.0032528727960745843 amount of bitcoin with 33.1274571598889 for 10184.06167 dolallars on day 506\n",
            "629.421686037889 0.04920944521051775\n",
            "629.421686037889 0.04920944521051775\n",
            "Buying 0.0031213510413958465 amount of bitcoin with 31.47108430189445 for 10082.52 dolallars on day 507\n",
            "597.9506017359945 0.05233079625191359\n",
            "597.9506017359945 0.05233079625191359\n",
            "Buying 0.003291498379846831 amount of bitcoin with 29.897530086799726 for 9083.258333 dolallars on day 508\n",
            "568.0530716491947 0.055622294631760424\n",
            "568.0530716491947 0.055622294631760424\n",
            "Buying 0.003190627648443978 amount of bitcoin with 28.402653582459738 for 8901.901667 dolallars on day 509\n",
            "539.650418066735 0.058812922280204405\n",
            "539.650418066735 0.058812922280204405\n",
            "Buying 0.0032119569625765876 amount of bitcoin with 26.982520903336752 for 8400.648333 dolallars on day 511\n",
            "512.6678971633983 0.062024879242780995\n",
            "512.6678971633983 0.062024879242780995\n",
            "Buying 0.003748220797007354 amount of bitcoin with 25.633394858169915 for 6838.816667 dolallars on day 512\n",
            "487.03450230522833 0.06577310003978835\n",
            "Selling 0.0030064012818498305 bitcoin for 24.351725115261416 dollars on day 514\n",
            "Selling 0.003102869680007515 bitcoin for 25.569311371024487 dollars on day 515\n",
            "Selling 0.003217824862670885 bitcoin for 26.84777693957571 dollars on day 518\n",
            "563.8033157310899 0.05644600421526012\n",
            "Buying 0.00349406245727632 amount of bitcoin with 28.1901657865545 for 8068.02 dolallars on day 519\n",
            "535.6131499445354 0.05994006667253644\n",
            "Selling 0.0031148385318894435 bitcoin for 26.78065749722677 dollars on day 520\n",
            "Selling 0.003012404383649304 bitcoin for 28.11969037208811 dollars on day 521\n",
            "Selling 0.003117271654190037 bitcoin for 29.525674890692517 dollars on day 522\n",
            "Selling 0.0030612682650327573 bitcoin for 31.00195863522714 dollars on day 523\n",
            "Selling 0.0030024056057025637 bitcoin for 32.5520565669885 dollars on day 524\n",
            "Selling 0.0030831845153080936 bitcoin for 34.17965939533792 dollars on day 525\n",
            "Selling 0.0031507821157395564 bitcoin for 35.88864236510482 dollars on day 527\n",
            "753.6614896672012 0.03839791160102469\n",
            "Buying 0.0033508039746967416 amount of bitcoin with 37.68307448336006 for 11245.98 dolallars on day 528\n",
            "715.9784151838411 0.04174871557572143\n",
            "715.9784151838411 0.04174871557572143\n",
            "Buying 0.003604738940526272 amount of bitcoin with 35.79892075919206 for 9931.071667 dolallars on day 529\n",
            "680.1794944246491 0.0453534545162477\n",
            "Selling 0.003346642813266624 bitcoin for 34.008974721232455 dollars on day 530\n",
            "714.1884691458815 0.042006811702981074\n",
            "Buying 0.003682677228069958 amount of bitcoin with 35.70942345729408 for 9696.593333 dolallars on day 532\n",
            "678.4790456885875 0.045689488931051035\n",
            "Selling 0.003278118911571942 bitcoin for 33.923952284429376 dollars on day 533\n",
            "Selling 0.0032354360096093246 bitcoin for 35.620149898650844 dollars on day 536\n",
            "Selling 0.0033019623912757252 bitcoin for 37.40115739358338 dollars on day 538\n",
            "Selling 0.0034357472520611786 bitcoin for 39.271215263262555 dollars on day 539\n",
            "824.6955205285136 0.03243822436653287\n",
            "Buying 0.003831089492376351 amount of bitcoin with 41.234776026425685 for 10763.19833 dolallars on day 541\n",
            "783.4607445020879 0.03626931385890922\n",
            "783.4607445020879 0.03626931385890922\n",
            "Buying 0.0038715964293844127 amount of bitcoin with 39.173037225104395 for 10118.058 dolallars on day 542\n",
            "744.2877072769835 0.040140910288293634\n",
            "744.2877072769835 0.040140910288293634\n",
            "Buying 0.0037512761871777295 amount of bitcoin with 37.214385363849175 for 9920.46 dolallars on day 543\n",
            "707.0733219131344 0.04389218647547136\n",
            "707.0733219131344 0.04389218647547136\n",
            "Buying 0.0038896010002576207 amount of bitcoin with 35.35366609565672 for 9089.278333 dolallars on day 544\n",
            "671.7196558174776 0.04778178747572898\n",
            "671.7196558174776 0.04778178747572898\n",
            "Buying 0.0038401526538495963 amount of bitcoin with 33.58598279087388 for 8746.002 dolallars on day 545\n",
            "638.1336730266037 0.05162194012957858\n",
            "Selling 0.0034745974089167423 bitcoin for 31.906683651330184 dollars on day 547\n",
            "670.0403566779339 0.04814734272066184\n",
            "Buying 0.004008318994227043 amount of bitcoin with 33.502017833896694 for 8358.121667 dolallars on day 550\n",
            "636.5383388440372 0.05215566171488888\n",
            "Selling 0.0037309984854408808 bitcoin for 31.826916942201862 dollars on day 551\n",
            "668.365255786239 0.048424663229448\n",
            "Buying 0.004027320534414406 amount of bitcoin with 33.41826278931195 for 8297.89 dolallars on day 552\n",
            "634.9469929969271 0.05245198376386241\n",
            "634.9469929969271 0.05245198376386241\n",
            "Buying 0.0038851716195844118 amount of bitcoin with 31.747349649846356 for 8171.415 dolallars on day 553\n",
            "603.1996433470807 0.05633715538344682\n",
            "Selling 0.0035853379288260647 bitcoin for 30.159982167354038 dollars on day 554\n",
            "Selling 0.0036799100677836565 bitcoin for 31.667981275721743 dollars on day 555\n",
            "Selling 0.0037161708757520386 bitcoin for 33.25138033950783 dollars on day 556\n",
            "698.2789871296644 0.045355736511085064\n",
            "Buying 0.004017526912274632 amount of bitcoin with 34.91394935648322 for 8690.408333 dolallars on day 557\n",
            "663.3650377731811 0.049373263423359695\n",
            "663.3650377731811 0.049373263423359695\n",
            "Buying 0.00392154277020216 amount of bitcoin with 33.16825188865906 for 8457.96 dolallars on day 561\n",
            "630.1967858845221 0.053294806193561854\n",
            "630.1967858845221 0.053294806193561854\n",
            "Buying 0.0040006423525860025 amount of bitcoin with 31.509839294226108 for 7876.195 dolallars on day 562\n",
            "598.686946590296 0.057295448546147855\n",
            "598.686946590296 0.057295448546147855\n",
            "Buying 0.0043493221357836145 amount of bitcoin with 29.9343473295148 for 6882.531667 dolallars on day 565\n",
            "568.7525992607812 0.06164477068193147\n",
            "Selling 0.004041819638103768 bitcoin for 28.437629963039058 dollars on day 568\n",
            "Selling 0.004029387136003623 bitcoin for 29.859511461191012 dollars on day 569\n",
            "627.0497406850112 0.053573563907824084\n",
            "Buying 0.0045927548680439285 amount of bitcoin with 31.35248703425056 for 6826.51 dolallars on day 571\n",
            "595.6972536507607 0.05816631877586801\n",
            "595.6972536507607 0.05816631877586801\n",
            "Buying 0.0045102088037735355 amount of bitcoin with 29.784862682538034 for 6603.876667 dolallars on day 572\n",
            "565.9123909682227 0.06267652757964154\n",
            "Selling 0.00403206097007697 bitcoin for 28.295619548411135 dollars on day 574\n",
            "594.2080105166337 0.05864446660956457\n",
            "Buying 0.004434869134161313 amount of bitcoin with 29.710400525831687 for 6699.273333 dolallars on day 575\n",
            "564.4976099908021 0.06307933574372589\n",
            "Selling 0.004075049641680235 bitcoin for 28.224880499540106 dollars on day 577\n",
            "Selling 0.003776339176489483 bitcoin for 29.636124524517108 dollars on day 578\n",
            "Selling 0.0038720696771605743 bitcoin for 31.11793075074296 dollars on day 580\n",
            "Selling 0.003917373595724832 bitcoin for 32.67382728828011 dollars on day 581\n",
            "686.1503730538823 0.047438503652670765\n",
            "Buying 0.004345244712754529 amount of bitcoin with 34.307518652694114 for 7895.416926 dolallars on day 583\n",
            "651.8428544011881 0.05178374836542529\n",
            "Selling 0.003991719840531134 bitcoin for 32.59214272005941 dollars on day 584\n",
            "Selling 0.0038656770235753505 bitcoin for 34.22174985606238 dollars on day 586\n",
            "Selling 0.003760418545475024 bitcoin for 35.9328373488655 dollars on day 590\n",
            "Selling 0.003906507506780185 bitcoin for 37.729479216308775 dollars on day 591\n",
            "792.3190635424842 0.036259425449063604\n",
            "Buying 0.004278920797339193 amount of bitcoin with 39.61595317712421 for 9258.398333 dolallars on day 592\n",
            "752.70311036536 0.0405383462464028\n",
            "752.70311036536 0.0405383462464028\n",
            "Buying 0.004176894440848716 amount of bitcoin with 37.635155518268 for 9010.32 dolallars on day 593\n",
            "715.067954847092 0.04471524068725151\n",
            "Selling 0.0038303319974535973 bitcoin for 35.7533977423546 dollars on day 595\n",
            "750.8213525894466 0.04088490868979791\n",
            "Buying 0.004071069662053606 amount of bitcoin with 37.54106762947233 for 9221.426 dolallars on day 598\n",
            "713.2802849599743 0.044955978351851514\n",
            "Selling 0.003699867356726972 bitcoin for 35.664014247998715 dollars on day 599\n",
            "Selling 0.0038394208103265702 bitcoin for 37.447214960398654 dollars on day 600\n",
            "786.3915141683717 0.03741669018479797\n",
            "Buying 0.004082971889227253 amount of bitcoin with 39.31957570841859 for 9630.136277 dolallars on day 602\n",
            "747.0719384599531 0.04149966207402522\n",
            "747.0719384599531 0.04149966207402522\n",
            "Buying 0.0040475871957234635 amount of bitcoin with 37.35359692299766 for 9228.608333 dolallars on day 604\n",
            "709.7183415369554 0.04554724926974869\n",
            "709.7183415369554 0.04554724926974869\n",
            "Buying 0.0041902001888402185 amount of bitcoin with 35.48591707684778 for 8468.788 dolallars on day 607\n",
            "674.2324244601077 0.049737449458588906\n",
            "Selling 0.0038963790872752926 bitcoin for 33.71162122300539 dollars on day 610\n",
            "707.9440456831131 0.045841070371313616\n",
            "Buying 0.004158770716386741 amount of bitcoin with 35.39720228415566 for 8511.458 dolallars on day 611\n",
            "672.5468433989574 0.04999984108770036\n",
            "672.5468433989574 0.04999984108770036\n",
            "Buying 0.00414839023914208 amount of bitcoin with 33.62734216994787 for 8106.118333 dolallars on day 613\n",
            "638.9195012290096 0.05414823132684244\n",
            "Selling 0.003876912843597583 bitcoin for 31.945975061450483 dollars on day 614\n",
            "Selling 0.003942831832012505 bitcoin for 33.543273814523005 dollars on day 616\n",
            "704.408750104983 0.04632848665123235\n",
            "Buying 0.0042001313591877015 amount of bitcoin with 35.22043750524915 for 8385.556187 dolallars on day 617\n",
            "669.1883125997339 0.05052861801042005\n",
            "669.1883125997339 0.05052861801042005\n",
            "Buying 0.0044283439649838 amount of bitcoin with 33.459415629986694 for 7555.74 dolallars on day 619\n",
            "635.7288969697472 0.054956961975403854\n",
            "635.7288969697472 0.054956961975403854\n",
            "Buying 0.00432886308898 amount of bitcoin with 31.786444848487363 for 7342.908333 dolallars on day 622\n",
            "603.9424521212599 0.05928582506438385\n",
            "Selling 0.004047938585854236 bitcoin for 30.197122606062994 dollars on day 625\n",
            "Selling 0.004147334265440344 bitcoin for 31.706978736366143 dollars on day 629\n",
            "665.846553463689 0.05109055221308927\n",
            "Buying 0.004438815253132649 amount of bitcoin with 33.29232767318445 for 7500.273333 dolallars on day 631\n",
            "632.5542257905046 0.05552936746622192\n",
            "Selling 0.004152949739999224 bitcoin for 31.62771128952523 dollars on day 632\n",
            "664.1819370800298 0.051376417726222694\n",
            "Buying 0.00490034588474626 amount of bitcoin with 33.209096854001494 for 6776.888333 dolallars on day 637\n",
            "630.9728402260283 0.056276763610968956\n",
            "Selling 0.004586018366856765 bitcoin for 31.548642011301418 dollars on day 639\n",
            "662.5214822373298 0.05169074524411219\n",
            "Buying 0.005245036039056082 amount of bitcoin with 33.126074111866494 for 6315.7 dolallars on day 640\n",
            "629.3954081254633 0.05693578128316827\n",
            "Selling 0.004734422639117815 bitcoin for 31.469770406273167 dollars on day 641\n",
            "660.8651785317364 0.052201358644050455\n",
            "Buying 0.005076216480646747 amount of bitcoin with 33.04325892658682 for 6509.426667 dolallars on day 643\n",
            "627.8219196051496 0.0572775751246972\n",
            "Selling 0.004659506602383476 bitcoin for 31.39109598025748 dollars on day 646\n",
            "659.213015585407 0.052618068522313724\n",
            "Buying 0.005204937873756219 amount of bitcoin with 32.96065077927035 for 6332.573333 dolallars on day 649\n",
            "626.2523648061366 0.057823006396069944\n",
            "626.2523648061366 0.057823006396069944\n",
            "Buying 0.0050984415300731716 amount of bitcoin with 31.312618240306833 for 6141.605833 dolallars on day 650\n",
            "594.9397465658298 0.06292144792614311\n",
            "594.9397465658298 0.06292144792614311\n",
            "Buying 0.005034436465246217 amount of bitcoin with 29.74698732829149 for 5908.7025 dolallars on day 656\n",
            "565.1927592375382 0.06795588439138933\n",
            "Selling 0.004540955567140947 bitcoin for 28.259637961876912 dollars on day 657\n",
            "Selling 0.004654708100521919 bitcoin for 29.67261985997076 dollars on day 658\n",
            "Selling 0.004701808339465731 bitcoin for 31.156250852969293 dollars on day 660\n",
            "Selling 0.004843973760601504 bitcoin for 32.71406339561776 dollars on day 665\n",
            "686.995331307973 0.049214438623659235\n",
            "Buying 0.005275820256928311 amount of bitcoin with 34.349766565398646 for 6510.791667 dolallars on day 667\n",
            "652.6455647425743 0.05449025888058755\n",
            "652.6455647425743 0.05449025888058755\n",
            "Buying 0.0051168918145640665 amount of bitcoin with 32.632278237128716 for 6377.363333 dolallars on day 668\n",
            "620.0132865054455 0.059607150695151614\n",
            "620.0132865054455 0.059607150695151614\n",
            "Buying 0.00496458832238101 amount of bitcoin with 31.00066432527228 for 6244.3575 dolallars on day 670\n",
            "589.0126221801733 0.06457173901753263\n",
            "Selling 0.004520857262634654 bitcoin for 29.450631109008665 dollars on day 673\n",
            "Selling 0.0045012465832770295 bitcoin for 30.9231626644591 dollars on day 674\n",
            "Selling 0.004434718108517248 bitcoin for 32.46932079768205 dollars on day 675\n",
            "Selling 0.00458973886635934 bitcoin for 34.09278683756616 dollars on day 677\n",
            "Selling 0.004655132041268426 bitcoin for 35.797426179444464 dollars on day 680\n",
            "Selling 0.004555392782524247 bitcoin for 37.58729748841668 dollars on day 682\n",
            "789.3332472567503 0.03731465337295169\n",
            "Buying 0.004970731335238188 amount of bitcoin with 39.46666236283752 for 7939.81 dolallars on day 684\n",
            "749.8665848939128 0.04228538470818988\n",
            "Selling 0.004582275242878524 bitcoin for 37.49332924469564 dollars on day 685\n",
            "787.3599141386084 0.03770310946531135\n",
            "Buying 0.004972715183490637 amount of bitcoin with 39.36799570693042 for 7916.800833 dolallars on day 688\n",
            "747.991918431678 0.042675824648801985\n",
            "747.991918431678 0.042675824648801985\n",
            "Buying 0.004939934252807027 amount of bitcoin with 37.3995959215839 for 7570.869167 dolallars on day 689\n",
            "710.592322510094 0.04761575890160901\n",
            "710.592322510094 0.04761575890160901\n",
            "Buying 0.0048048712053502494 amount of bitcoin with 35.5296161255047 for 7394.499167 dolallars on day 691\n",
            "675.0627063845893 0.05242063010695926\n",
            "675.0627063845893 0.05242063010695926\n",
            "Buying 0.004657037847302272 amount of bitcoin with 33.753135319229465 for 7247.769167 dolallars on day 692\n",
            "641.3095710653598 0.05707766795426153\n",
            "641.3095710653598 0.05707766795426153\n",
            "Buying 0.0045734129222150584 amount of bitcoin with 32.065478553267994 for 7011.28 dolallars on day 693\n",
            "609.2440925120918 0.06165108087647659\n",
            "609.2440925120918 0.06165108087647659\n",
            "Buying 0.0045342604563411186 amount of bitcoin with 30.462204625604592 for 6718.23 dolallars on day 696\n",
            "578.7818878864872 0.06618534133281771\n",
            "578.7818878864872 0.06618534133281771\n",
            "Buying 0.004524014945712758 amount of bitcoin with 28.93909439432436 for 6396.7725 dolallars on day 697\n",
            "549.8427934921629 0.07070935627853048\n",
            "549.8427934921629 0.07070935627853048\n",
            "Buying 0.004478305718657255 amount of bitcoin with 27.492139674608143 for 6138.96 dolallars on day 699\n",
            "522.3506538175548 0.07518766199718774\n",
            "Selling 0.004138327968570607 bitcoin for 26.117532690877738 dollars on day 700\n",
            "Selling 0.004260462747557165 bitcoin for 27.423409325421627 dollars on day 706\n",
            "Selling 0.004379251134881807 bitcoin for 28.79457979169271 dollars on day 710\n",
            "604.6861756255469 0.06240962014617816\n",
            "Buying 0.0047578842913489575 amount of bitcoin with 30.234308781277345 for 6354.57 dolallars on day 711\n",
            "574.4518668442695 0.06716750443752711\n",
            "Selling 0.004389386986639887 bitcoin for 28.722593342213475 dollars on day 712\n",
            "Selling 0.004488286426202284 bitcoin for 30.15872300932415 dollars on day 713\n",
            "Selling 0.0045237826012123296 bitcoin for 31.666659159790356 dollars on day 716\n",
            "Selling 0.0045875122157673425 bitcoin for 33.249992117779875 dollars on day 721\n",
            "698.2498344733774 0.04917853620770527\n",
            "Buying 0.004908217618846464 amount of bitcoin with 34.91249172366887 for 7113.069231 dolallars on day 724\n",
            "663.3373427497086 0.05408675382655174\n",
            "663.3373427497086 0.05408675382655174\n",
            "Buying 0.005155520993714228 amount of bitcoin with 33.16686713748543 for 6433.271667 dolallars on day 725\n",
            "630.1704756122232 0.059242274820265964\n",
            "630.1704756122232 0.059242274820265964\n",
            "Buying 0.005012152313196814 amount of bitcoin with 31.50852378061116 for 6286.425833 dolallars on day 728\n",
            "598.661951831612 0.06425442713346277\n",
            "Selling 0.0046057562289300346 bitcoin for 29.9330975915806 dollars on day 733\n",
            "628.5950494231926 0.059648670904532734\n",
            "Buying 0.00499151834398695 amount of bitcoin with 31.42975247115963 for 6296.631667 dolallars on day 737\n",
            "597.1652969520329 0.06464018924851969\n",
            "Selling 0.004651861545438059 bitcoin for 29.858264847601646 dollars on day 739\n",
            "Selling 0.0047003330101850725 bitcoin for 31.35117808998173 dollars on day 740\n",
            "658.3747398896163 0.05528799469289657\n",
            "Buying 0.005088979971207382 amount of bitcoin with 32.91873699448082 for 6468.631667 dolallars on day 745\n",
            "625.4560028951355 0.06037697466410395\n",
            "Selling 0.004677299724914565 bitcoin for 31.272800144756776 dollars on day 747\n",
            "656.7288030398922 0.055699674939189384\n",
            "Buying 0.005012834081144559 amount of bitcoin with 32.836440151994616 for 6550.474167 dolallars on day 748\n",
            "623.8923628878977 0.06071250902033394\n",
            "623.8923628878977 0.06071250902033394\n",
            "Buying 0.0049922285404521965 amount of bitcoin with 31.194618144394884 for 6248.635833 dolallars on day 760\n",
            "592.6977447435028 0.06570473756078614\n",
            "Selling 0.004592725004316506 bitcoin for 29.63488723717514 dollars on day 764\n",
            "Selling 0.004715234127781811 bitcoin for 31.116631599033894 dollars on day 765\n",
            "653.4492635797118 0.05639677842868782\n",
            "Buying 0.005035188803007218 amount of bitcoin with 32.67246317898559 for 6488.825833 dolallars on day 769\n",
            "620.7768004007262 0.06143196723169504\n",
            "620.7768004007262 0.06143196723169504\n",
            "Buying 0.004862988079696654 amount of bitcoin with 31.03884002003631 for 6382.668333 dolallars on day 778\n",
            "589.7379603806899 0.06629495531139169\n",
            "Selling 0.004509534335715705 bitcoin for 29.486898019034495 dollars on day 787\n",
            "619.2248583997243 0.061785420975675985\n",
            "Buying 0.004838425010277442 amount of bitcoin with 30.96124291998622 for 6399.033333 dolallars on day 790\n",
            "588.2636154797381 0.06662384598595343\n",
            "588.2636154797381 0.06662384598595343\n",
            "Buying 0.004762377364879429 amount of bitcoin with 29.413180773986905 for 6176.155 dolallars on day 794\n",
            "558.8504347057511 0.07138622335083286\n",
            "558.8504347057511 0.07138622335083286\n",
            "Buying 0.004853854111817487 amount of bitcoin with 27.942521735287556 for 5756.77 dolallars on day 795\n",
            "530.9079129704636 0.07624007746265035\n",
            "530.9079129704636 0.07624007746265035\n",
            "Buying 0.004743474361992226 amount of bitcoin with 26.545395648523183 for 5596.1925 dolallars on day 796\n",
            "504.3625173219404 0.08098355182464258\n",
            "504.3625173219404 0.08098355182464258\n",
            "Buying 0.004754600161313405 amount of bitcoin with 25.218125866097022 for 5303.9425 dolallars on day 799\n",
            "479.1443914558434 0.08573815198595598\n",
            "479.1443914558434 0.08573815198595598\n",
            "Buying 0.005127862459046648 amount of bitcoin with 23.95721957279217 for 4671.97 dolallars on day 800\n",
            "455.18717188305123 0.09086601444500263\n",
            "455.18717188305123 0.09086601444500263\n",
            "Buying 0.005066665166397127 amount of bitcoin with 22.759358594152562 for 4491.98 dolallars on day 801\n",
            "432.4278132888987 0.09593267961139976\n",
            "432.4278132888987 0.09593267961139976\n",
            "Buying 0.00501733518538405 amount of bitcoin with 21.621390664444935 for 4309.3375 dolallars on day 803\n",
            "410.8064226244537 0.10095001479678381\n",
            "410.8064226244537 0.10095001479678381\n",
            "Buying 0.005372108919792839 amount of bitcoin with 20.540321131222687 for 3823.511667 dolallars on day 805\n",
            "390.266101493231 0.10632212371657665\n",
            "Selling 0.0047553367984589125 bitcoin for 19.513305074661552 dollars on day 808\n",
            "Selling 0.004805349786378235 bitcoin for 20.48897032839463 dollars on day 809\n",
            "430.26837689628724 0.0967614371317395\n",
            "Buying 0.005225791008820458 amount of bitcoin with 21.513418844814364 for 4116.7775 dolallars on day 811\n",
            "408.7549580514729 0.10198722814055995\n",
            "408.7549580514729 0.10198722814055995\n",
            "Buying 0.00515910193065915 amount of bitcoin with 20.437747902573648 for 3961.493333 dolallars on day 814\n",
            "388.3172101488992 0.1071463300712191\n",
            "388.3172101488992 0.1071463300712191\n",
            "Buying 0.005032167817652819 amount of bitcoin with 19.41586050744496 for 3858.349167 dolallars on day 815\n",
            "368.90134964145426 0.11217849788887192\n",
            "368.90134964145426 0.11217849788887192\n",
            "Buying 0.004925277967324982 amount of bitcoin with 18.445067482072712 for 3744.98 dolallars on day 816\n",
            "350.45628215938154 0.1171037758561969\n",
            "350.45628215938154 0.1171037758561969\n",
            "Buying 0.005145228784874954 amount of bitcoin with 17.52281410796908 for 3405.643333 dolallars on day 817\n",
            "332.9334680514125 0.12224900464107186\n",
            "Selling 0.004723854244250963 bitcoin for 16.646673402570624 dollars on day 820\n",
            "349.5801414539831 0.11752515039682089\n",
            "Buying 0.0051299004110901235 amount of bitcoin with 17.479007072699158 for 3407.28 dolallars on day 822\n",
            "332.10113438128394 0.12265505080791102\n",
            "332.10113438128394 0.12265505080791102\n",
            "Buying 0.0050650279294566555 amount of bitcoin with 16.605056719064198 for 3278.374167 dolallars on day 824\n",
            "315.49607766221976 0.12772007873736768\n",
            "Selling 0.004650035559760992 bitcoin for 15.77480388311099 dollars on day 827\n",
            "Selling 0.004657207307457962 bitcoin for 16.56354407726654 dollars on day 828\n",
            "Selling 0.004567102725647065 bitcoin for 17.391721281129865 dollars on day 829\n",
            "Selling 0.004584984000375281 bitcoin for 18.26130734518636 dollars on day 830\n",
            "Selling 0.004636878284297863 bitcoin for 19.174372712445678 dollars on day 831\n",
            "402.6618269613592 0.10462387085982852\n",
            "Buying 0.005147846746534685 amount of bitcoin with 20.133091348067964 for 3910.973333 dolallars on day 832\n",
            "382.52873561329125 0.1097717176063632\n",
            "Selling 0.004748985642938917 bitcoin for 19.126436780664562 dollars on day 833\n",
            "401.6551723939558 0.10502273196342428\n",
            "Buying 0.005265702806511424 amount of bitcoin with 20.082758619697792 for 3813.88 dolallars on day 835\n",
            "381.572413774258 0.1102884347699357\n",
            "Selling 0.004866428150910334 bitcoin for 19.0786206887129 dollars on day 847\n",
            "Selling 0.004963351952086422 bitcoin for 20.032551723148543 dollars on day 848\n",
            "420.68358618611944 0.10045865466693894\n",
            "Buying 0.005517037577814966 amount of bitcoin with 21.034179309305973 for 3812.585833 dolallars on day 851\n",
            "399.6494068768135 0.10597569224475391\n",
            "399.6494068768135 0.10597569224475391\n",
            "Buying 0.005480279833758156 amount of bitcoin with 19.982470343840674 for 3646.25 dolallars on day 852\n",
            "379.66693653297284 0.11145597207851206\n",
            "379.66693653297284 0.11145597207851206\n",
            "Buying 0.005374734375997645 amount of bitcoin with 18.983346826648642 for 3531.96 dolallars on day 855\n",
            "360.6835897063242 0.1168307064545097\n",
            "Selling 0.0049317035279121445 bitcoin for 18.034179485316212 dollars on day 856\n",
            "378.7177691916404 0.11189900292659756\n",
            "Buying 0.005336416894408022 amount of bitcoin with 18.93588845958202 for 3548.4275 dolallars on day 862\n",
            "359.78188073205837 0.11723541982100558\n",
            "359.78188073205837 0.11723541982100558\n",
            "Buying 0.005257509363047381 amount of bitcoin with 17.989094036602918 for 3421.6 dolallars on day 871\n",
            "341.79278669545545 0.12249292918405295\n",
            "Selling 0.004667922944806661 bitcoin for 17.089639334772773 dollars on day 881\n",
            "Selling 0.004588350542475046 bitcoin for 17.944121301511412 dollars on day 891\n",
            "Selling 0.0045514850146359505 bitcoin for 18.84132736658698 dollars on day 896\n",
            "395.6678746983266 0.1086851706821353\n",
            "Buying 0.005251917028348672 amount of bitcoin with 19.78339373491633 for 3766.89 dolallars on day 897\n",
            "375.8844809634103 0.11393708771048397\n",
            "Selling 0.0048535668017742955 bitcoin for 18.794224048170516 dollars on day 906\n",
            "Selling 0.004913889677555508 bitcoin for 19.73393525057904 dollars on day 917\n",
            "Selling 0.004989881352599017 bitcoin for 20.720632013107995 dollars on day 933\n",
            "Selling 0.004455703112458916 bitcoin for 21.756663613763394 dollars on day 934\n",
            "Selling 0.0045427603160318655 bitcoin for 22.844496794451565 dollars on day 937\n",
            "Selling 0.004622262276331928 bitcoin for 23.986721634174145 dollars on day 939\n",
            "Selling 0.004742976267449098 bitcoin for 25.186057715882853 dollars on day 942\n",
            "528.9072120335398 0.08081604790628334\n",
            "Buying 0.0052522851153872264 amount of bitcoin with 26.445360601676995 for 5035.02 dolallars on day 943\n",
            "502.46185143186284 0.08606833302167056\n",
            "Selling 0.0048758939956629185 bitcoin for 25.123092571593144 dollars on day 946\n",
            "527.584944003456 0.08119243902600765\n",
            "Buying 0.005237947650833132 amount of bitcoin with 26.379247200172802 for 5036.18 dolallars on day 947\n",
            "501.2056968032832 0.08643038667684078\n",
            "Selling 0.004822392279673283 bitcoin for 25.06028484016416 dollars on day 948\n",
            "Selling 0.0049560955689231636 bitcoin for 26.31329908217237 dollars on day 952\n",
            "Selling 0.005006916080048602 bitcoin for 27.62896403628099 dollars on day 955\n",
            "580.2082447619008 0.07164498274819572\n",
            "Buying 0.005583639310513115 amount of bitcoin with 29.010412238095043 for 5195.61 dolallars on day 957\n",
            "551.1978325238058 0.07722862205870884\n",
            "Selling 0.005198714204691744 bitcoin for 27.559891626190293 dollars on day 960\n",
            "Selling 0.005115285498944661 bitcoin for 28.937886207499805 dollars on day 965\n",
            "Selling 0.005265007679303491 bitcoin for 30.384780517874795 dollars on day 966\n",
            "Selling 0.005374014530543555 bitcoin for 31.904019543768534 dollars on day 970\n",
            "Selling 0.005449765902047852 bitcoin for 33.49922052095696 dollars on day 971\n",
            "Selling 0.005540968923696651 bitcoin for 35.174181547004814 dollars on day 972\n",
            "Selling 0.005095379560801766 bitcoin for 36.932890624355046 dollars on day 973\n",
            "775.590703111456 0.04018948575867912\n",
            "Buying 0.005556898009433485 amount of bitcoin with 38.7795351555728 for 6978.63 dolallars on day 974\n",
            "736.8111679558832 0.0457463837681126\n",
            "Selling 0.004709196010000392 bitcoin for 36.84055839779416 dollars on day 975\n",
            "Selling 0.0048397456072591175 bitcoin for 38.68258631768387 dollars on day 976\n",
            "Selling 0.004949093522958495 bitcoin for 40.61671563356807 dollars on day 977\n",
            "852.9510283049294 0.031248348627894603\n",
            "Buying 0.0054080354725672556 amount of bitcoin with 42.647551415246475 for 7885.96 dolallars on day 978\n",
            "810.303476889683 0.03665638410046186\n",
            "810.303476889683 0.03665638410046186\n",
            "Buying 0.005503119146736195 amount of bitcoin with 40.51517384448415 for 7362.22 dolallars on day 979\n",
            "769.7883030451989 0.042159503247198055\n",
            "Selling 0.00469744012500579 bitcoin for 38.489415152259944 dollars on day 981\n",
            "808.2777181974589 0.03746206312219227\n",
            "Buying 0.005047300423486979 amount of bitcoin with 40.413885909872945 for 8007.03 dolallars on day 982\n",
            "767.8638322875859 0.042509363545679246\n",
            "767.8638322875859 0.042509363545679246\n",
            "Buying 0.005034558619654166 amount of bitcoin with 38.3931916143793 for 7625.93 dolallars on day 984\n",
            "729.4706406732066 0.047543922165333416\n",
            "Selling 0.004628450480078821 bitcoin for 36.473532033660334 dollars on day 985\n",
            "Selling 0.004785201109965608 bitcoin for 38.29720863534335 dollars on day 986\n",
            "Selling 0.004598597627642601 bitcoin for 40.21206906711052 dollars on day 988\n",
            "844.4534504093209 0.03353167294764638\n",
            "Buying 0.0051040044340457435 amount of bitcoin with 42.22267252046605 for 8272.46 dolallars on day 992\n",
            "802.2307778888548 0.03863567738169213\n",
            "Selling 0.004689318431721391 bitcoin for 40.111538894442745 dollars on day 993\n",
            "Selling 0.004819426851625968 bitcoin for 42.11711583916488 dollars on day 995\n",
            "884.4594326224625 0.02912693209834477\n",
            "Buying 0.0054361901077236326 amount of bitcoin with 44.22297163112313 for 8134.92 dolallars on day 996\n",
            "840.2364609913393 0.0345631222060684\n",
            "840.2364609913393 0.0345631222060684\n",
            "Buying 0.005473282661034286 amount of bitcoin with 42.01182304956697 for 7675.8 dolallars on day 997\n",
            "798.2246379417724 0.040036404867102686\n",
            "Selling 0.005125998825730232 bitcoin for 39.91123189708862 dollars on day 998\n",
            "Selling 0.005239469123017927 bitcoin for 41.906793491943056 dollars on day 1000\n",
            "880.0426633308041 0.029670936918354526\n",
            "Buying 0.005758981699961156 amount of bitcoin with 44.002133166540204 for 7640.61 dolallars on day 1002\n",
            "836.0405301642638 0.03542991861831568\n",
            "Selling 0.005211975805162997 bitcoin for 41.802026508213196 dollars on day 1003\n",
            "877.842556672477 0.030217942813152685\n",
            "Buying 0.005543629219234142 amount of bitcoin with 43.89212783362385 for 7917.58 dolallars on day 1004\n",
            "833.9504288388532 0.03576157203238683\n",
            "Selling 0.005101824951969355 bitcoin for 41.69752144194266 dollars on day 1005\n",
            "Selling 0.005036164248877311 bitcoin for 43.78239751403979 dollars on day 1007\n",
            "Selling 0.0051889692486956655 bitcoin for 45.971517389741784 dollars on day 1008\n",
            "Selling 0.005375893419998115 bitcoin for 48.27009325922887 dollars on day 1009\n",
            "Selling 0.005433793256291122 bitcoin for 50.683597922190316 dollars on day 1010\n",
            "1064.3555563659966 0.009624926906555257\n",
            "Buying 0.005863455530663762 amount of bitcoin with 53.21777781829983 for 9076.18 dolallars on day 1011\n",
            "1011.1377785476967 0.01548838243721902\n",
            "Selling 0.005446942793602986 bitcoin for 50.55688892738484 dollars on day 1012\n",
            "Selling 0.0055694873626248205 bitcoin for 53.084733373754084 dollars on day 1013\n",
            "Selling 0.004309975220852948 bitcoin for 55.738970042441785 dollars on day 1019\n",
            "1170.5183708912773 0.00016197706013826654\n",
            "Buying 0.0052570472560542775 amount of bitcoin with 58.525918544563865 for 11132.85 dolallars on day 1020\n",
            "1111.9924523467134 0.005419024316192544\n",
            "Selling 0.00449313480040048 bitcoin for 55.599622617335676 dollars on day 1021\n",
            "1167.592074964049 0.0009258895157920645\n",
            "Buying 0.004909818167981381 amount of bitcoin with 58.379603748202456 for 11890.38 dolallars on day 1022\n",
            "1109.2124712158466 0.005835707683773446\n",
            "1109.2124712158466 0.005835707683773446\n",
            "Buying 0.005165023106447297 amount of bitcoin with 55.46062356079233 for 10737.73 dolallars on day 1023\n",
            "1053.7518476550542 0.011000730790220743\n",
            "1053.7518476550542 0.011000730790220743\n",
            "Buying 0.004980526224604934 amount of bitcoin with 52.68759238275271 for 10578.72 dolallars on day 1024\n",
            "1001.0642552723016 0.015981257014825678\n",
            "Selling 0.004632240617063235 bitcoin for 50.05321276361508 dollars on day 1025\n",
            "Selling 0.004385225353014649 bitcoin for 52.55587340179584 dollars on day 1026\n",
            "1103.6733414377127 0.006963791044747794\n",
            "Buying 0.0049445559354370314 amount of bitcoin with 55.18366707188564 for 11160.49 dolallars on day 1027\n",
            "1048.489674365827 0.011908346980184826\n",
            "1048.489674365827 0.011908346980184826\n",
            "Buying 0.004785369060633652 amount of bitcoin with 52.424483718291356 for 10955.16 dolallars on day 1028\n",
            "996.0651906475357 0.016693716040818478\n",
            "Selling 0.004434035093569537 bitcoin for 49.803259532376785 dollars on day 1029\n",
            "Selling 0.004556362894952225 bitcoin for 52.29342250899563 dollars on day 1030\n",
            "Selling 0.004459330535856619 bitcoin for 54.908093634445414 dollars on day 1031\n",
            "1153.0699663233536 0.0032439875164400975\n",
            "Buying 0.0047647913053965475 amount of bitcoin with 57.653498316167685 for 12099.9 dolallars on day 1033\n",
            "1095.4164680071858 0.008008778821836645\n",
            "1095.4164680071858 0.008008778821836645\n",
            "Buying 0.0048244032918864825 amount of bitcoin with 54.77082340035929 for 11352.87 dolallars on day 1034\n",
            "1040.6456446068264 0.012833182113723127\n",
            "Selling 0.004408032401839493 bitcoin for 52.03228223034132 dollars on day 1035\n",
            "1092.6779268371677 0.008425149711883633\n",
            "Buying 0.004797033685002185 amount of bitcoin with 54.63389634185839 for 11389.1 dolallars on day 1036\n",
            "1038.0440304953092 0.013222183396885818\n",
            "1038.0440304953092 0.013222183396885818\n",
            "Buying 0.005094964692583997 amount of bitcoin with 51.902201524765466 for 10186.96 dolallars on day 1037\n",
            "986.1418289705438 0.018317148089469815\n",
            "Selling 0.004534610884124449 bitcoin for 49.30709144852719 dollars on day 1038\n",
            "1035.4489204190709 0.013782537205345367\n",
            "Buying 0.0055090432595303485 amount of bitcoin with 51.772446020953545 for 9397.72 dolallars on day 1039\n",
            "983.6764743981173 0.019291580464875717\n",
            "Selling 0.0050839776934206845 bitcoin for 49.18382371990587 dollars on day 1040\n",
            "Selling 0.004854323790616509 bitcoin for 51.64301490590116 dollars on day 1041\n",
            "1084.5033130239242 0.009353278980838523\n",
            "Buying 0.005146777937137481 amount of bitcoin with 54.225165651196214 for 10535.75 dolallars on day 1042\n",
            "1030.278147372728 0.014500056917976005\n",
            "Selling 0.004785358653460451 bitcoin for 51.5139073686364 dollars on day 1043\n",
            "1081.7920547413644 0.009714698264515554\n",
            "Buying 0.005108860688031182 amount of bitcoin with 54.08960273706822 for 10587.41 dolallars on day 1044\n",
            "1027.7024520042962 0.014823558952546737\n",
            "1027.7024520042962 0.014823558952546737\n",
            "Buying 0.004977432586652241 amount of bitcoin with 51.385122600214814 for 10323.62 dolallars on day 1045\n",
            "976.3173294040814 0.019800991539198978\n",
            "976.3173294040814 0.019800991539198978\n",
            "Buying 0.004958795086517484 amount of bitcoin with 48.81586647020407 for 9844.3 dolallars on day 1046\n",
            "927.5014629338773 0.024759786625716464\n",
            "927.5014629338773 0.024759786625716464\n",
            "Buying 0.0048949886105742 amount of bitcoin with 46.375073146693865 for 9473.99 dolallars on day 1050\n",
            "881.1263897871834 0.029654775236290662\n",
            "Selling 0.004594402150076094 bitcoin for 44.05631948935917 dollars on day 1053\n",
            "Selling 0.004587061138539285 bitcoin for 46.25913546382713 dollars on day 1054\n",
            "Selling 0.004667175825610467 bitcoin for 48.57209223701848 dollars on day 1055\n",
            "Selling 0.004843578011298623 bitcoin for 51.00069684886941 dollars on day 1056\n",
            "Selling 0.004951720844315851 bitcoin for 53.550731691312876 dollars on day 1057\n",
            "Selling 0.005120864342174847 bitcoin for 56.22826827587852 dollars on day 1058\n",
            "1180.7936337934489 0.000889972924275494\n",
            "Buying 0.00514925701591555 amount of bitcoin with 59.03968168967245 for 11465.67 dolallars on day 1060\n",
            "1121.7539521037763 0.006039229940191044\n",
            "Selling 0.0046892853169923825 bitcoin for 56.08769760518882 dollars on day 1061\n",
            "1177.8416497089652 0.0013499446231986616\n",
            "Buying 0.004967012786543934 amount of bitcoin with 58.89208248544826 for 11856.64 dolallars on day 1063\n",
            "1118.9495672235169 0.0063169574097425955\n",
            "1118.9495672235169 0.0063169574097425955\n",
            "Buying 0.004958906878360451 amount of bitcoin with 55.94747836117585 for 11282.22 dolallars on day 1064\n",
            "1063.002088862341 0.011275864288103047\n",
            "Selling 0.004595041034813056 bitcoin for 53.15010444311705 dollars on day 1065\n",
            "1116.152193305458 0.00668082325328999\n",
            "Buying 0.004901311727052861 amount of bitcoin with 55.807609665272906 for 11386.26 dolallars on day 1066\n",
            "1060.3445836401852 0.011582134980342851\n",
            "1060.3445836401852 0.011582134980342851\n",
            "Buying 0.004882726400335349 amount of bitcoin with 53.01722918200926 for 10858.12 dolallars on day 1067\n",
            "1007.3273544581759 0.0164648613806782\n",
            "1007.3273544581759 0.0164648613806782\n",
            "Buying 0.005028109099258537 amount of bitcoin with 50.3663677229088 for 10016.96 dolallars on day 1068\n",
            "956.9609867352672 0.02149297047993674\n",
            "Selling 0.004644463189479824 bitcoin for 47.84804933676336 dollars on day 1069\n",
            "Selling 0.004601928671076949 bitcoin for 50.24045180360153 dollars on day 1073\n",
            "1055.049487875632 0.012246578619379965\n",
            "Buying 0.004902391176089497 amount of bitcoin with 52.7524743937816 for 10760.56 dolallars on day 1074\n",
            "1002.2970134818504 0.01714896979546946\n",
            "1002.2970134818504 0.01714896979546946\n",
            "Buying 0.004947464871966012 amount of bitcoin with 50.11485067409252 for 10129.4 dolallars on day 1075\n",
            "952.1821628077579 0.022096434667435473\n",
            "Selling 0.004575242882619219 bitcoin for 47.609108140387896 dollars on day 1077\n",
            "999.7912709481458 0.017521191784816252\n",
            "Buying 0.004928090414579082 amount of bitcoin with 49.98956354740729 for 10143.8 dolallars on day 1078\n",
            "949.8017074007385 0.022449282199395335\n",
            "Selling 0.0045838611861877205 bitcoin for 47.49008537003692 dollars on day 1080\n",
            "997.2917927707754 0.017865421013207615\n",
            "Buying 0.004902166215773649 amount of bitcoin with 49.864589638538774 for 10171.95 dolallars on day 1081\n",
            "947.4272031322366 0.022767587228981263\n",
            "947.4272031322366 0.022767587228981263\n",
            "Buying 0.004874690018606214 amount of bitcoin with 47.371360156611836 for 9717.82 dolallars on day 1082\n",
            "900.0558429756248 0.027642277247587478\n",
            "900.0558429756248 0.027642277247587478\n",
            "Buying 0.004744852644435556 amount of bitcoin with 45.002792148781246 for 9484.55 dolallars on day 1083\n",
            "855.0530508268436 0.03238712989202303\n",
            "Selling 0.0044529838391548894 bitcoin for 42.75265254134218 dollars on day 1085\n",
            "Selling 0.004594805535063628 bitcoin for 44.89028516840929 dollars on day 1086\n",
            "Selling 0.004538021865283648 bitcoin for 47.13479942682975 dollars on day 1087\n",
            "Selling 0.004659654279110281 bitcoin for 49.49153939817124 dollars on day 1088\n",
            "1039.322327361596 0.014141664373410587\n",
            "Buying 0.005036711167377255 amount of bitcoin with 51.96611636807981 for 10317.47 dolallars on day 1091\n",
            "987.3562109935162 0.019178375540787843\n",
            "Selling 0.004707430341308682 bitcoin for 49.367810549675816 dollars on day 1092\n",
            "1036.724021543192 0.014470945199479161\n",
            "Buying 0.00502597536443509 amount of bitcoin with 51.83620107715961 for 10313.66 dolallars on day 1094\n",
            "984.8878204660325 0.01949692056391425\n",
            "984.8878204660325 0.01949692056391425\n",
            "Buying 0.0048751851071922 amount of bitcoin with 49.24439102330163 for 10101.03 dolallars on day 1095\n",
            "935.6434294427308 0.02437210567110645\n",
            "Selling 0.004489582834825622 bitcoin for 46.78217147213655 dollars on day 1097\n",
            "982.4256009148673 0.01988252283628083\n",
            "Buying 0.004764231952085739 amount of bitcoin with 49.12128004574337 for 10310.43 dolallars on day 1100\n",
            "933.304320869124 0.024646754788366566\n",
            "933.304320869124 0.024646754788366566\n",
            "Buying 0.004579349114600093 amount of bitcoin with 46.665216043456205 for 10190.36 dolallars on day 1102\n",
            "886.6391048256678 0.02922610390296666\n",
            "886.6391048256678 0.02922610390296666\n",
            "Buying 0.004442306695160113 amount of bitcoin with 44.331955241283396 for 9979.49 dolallars on day 1106\n",
            "842.3071495843844 0.03366841059812677\n",
            "842.3071495843844 0.03366841059812677\n",
            "Buying 0.004349241430081152 amount of bitcoin with 42.11535747921923 for 9683.38 dolallars on day 1108\n",
            "800.1917921051652 0.03801765202820793\n",
            "800.1917921051652 0.03801765202820793\n",
            "Buying 0.004677509216022038 amount of bitcoin with 40.00958960525826 for 8553.61 dolallars on day 1109\n",
            "760.1822024999069 0.042695161244229965\n",
            "760.1822024999069 0.042695161244229965\n",
            "Buying 0.004507598835064431 amount of bitcoin with 38.009110124995345 for 8432.23 dolallars on day 1110\n",
            "722.1730923749116 0.047202760079294394\n",
            "722.1730923749116 0.047202760079294394\n",
            "Buying 0.004482406688822438 amount of bitcoin with 36.10865461874558 for 8055.64 dolallars on day 1111\n",
            "686.064437756166 0.05168516676811683\n",
            "Selling 0.004186434040909494 bitcoin for 34.3032218878083 dollars on day 1112\n",
            "720.3676596439742 0.04749873272720734\n",
            "Buying 0.004470590211698368 amount of bitcoin with 36.01838298219871 for 8056.74 dolallars on day 1114\n",
            "684.3492766617755 0.05196932293890571\n",
            "Selling 0.004118745150075566 bitcoin for 34.21746383308878 dollars on day 1115\n",
            "718.5667404948643 0.047850577788830145\n",
            "Buying 0.004405422737195509 amount of bitcoin with 35.92833702474322 for 8155.48 dolallars on day 1119\n",
            "682.6384034701211 0.05225600052602565\n",
            "682.6384034701211 0.05225600052602565\n",
            "Buying 0.004337108998963887 amount of bitcoin with 34.13192017350606 for 7869.74 dolallars on day 1121\n",
            "648.5064832966151 0.05659310952498954\n",
            "Selling 0.00394851987267819 bitcoin for 32.425324164830755 dollars on day 1122\n",
            "Selling 0.003964474561136141 bitcoin for 34.04659037307229 dollars on day 1124\n",
            "714.9783978345181 0.04868011509117521\n",
            "Buying 0.004322864215848148 amount of bitcoin with 35.748919891725905 for 8269.73 dolallars on day 1126\n",
            "679.2294779427922 0.05300297930702336\n",
            "679.2294779427922 0.05300297930702336\n",
            "Buying 0.004160843930667815 amount of bitcoin with 33.96147389713961 for 8162.16 dolallars on day 1130\n",
            "645.2680040456526 0.057163823237691175\n",
            "645.2680040456526 0.057163823237691175\n",
            "Buying 0.004031660091931485 amount of bitcoin with 32.26340020228263 for 8002.51 dolallars on day 1131\n",
            "613.00460384337 0.06119548332962266\n",
            "Selling 0.0037237281944450047 bitcoin for 30.6502301921685 dollars on day 1135\n",
            "643.6548340355384 0.05747175513517765\n",
            "Buying 0.004009431165473606 amount of bitcoin with 32.182741701776926 for 8026.76 dolallars on day 1137\n",
            "611.4720923337616 0.06148118630065126\n",
            "611.4720923337616 0.06148118630065126\n",
            "Buying 0.004093093116152502 amount of bitcoin with 30.57360461668808 for 7469.56 dolallars on day 1138\n",
            "580.8984877170735 0.06557427941680376\n",
            "Selling 0.0033512897377060797 bitcoin for 29.044924385853676 dollars on day 1140\n",
            "Selling 0.0032935020848340528 bitcoin for 30.49717060514636 dollars on day 1141\n",
            "Selling 0.003352551435203504 bitcoin for 32.02202913540368 dollars on day 1142\n",
            "672.4626118434772 0.05557693615906012\n",
            "Buying 0.0036472508875568803 amount of bitcoin with 33.623130592173865 for 9218.76 dolallars on day 1143\n",
            "638.8394812513034 0.059224187046617\n",
            "Selling 0.0033860690065104304 bitcoin for 31.94197406256517 dollars on day 1144\n",
            "670.7814553138685 0.05583811804010657\n",
            "Buying 0.0036596250325374564 amount of bitcoin with 33.539072765693426 for 9164.62 dolallars on day 1145\n",
            "637.2423825481751 0.05949774307264403\n",
            "Selling 0.0034255996687956535 bitcoin for 31.862119127408757 dollars on day 1148\n",
            "Selling 0.003552245431249483 bitcoin for 33.45522508377919 dollars on day 1150\n",
            "702.559726759363 0.05251989797259889\n",
            "Buying 0.003773068684738781 amount of bitcoin with 35.12798633796815 for 9310.19 dolallars on day 1151\n",
            "667.4317404213948 0.05629296665733768\n",
            "667.4317404213948 0.05629296665733768\n",
            "Buying 0.0036256754518645476 amount of bitcoin with 33.37158702106974 for 9204.24 dolallars on day 1153\n",
            "634.060153400325 0.059918642109202225\n",
            "634.060153400325 0.059918642109202225\n",
            "Buying 0.0036165711849382672 amount of bitcoin with 31.70300767001625 for 8766.04 dolallars on day 1154\n",
            "602.3571457303087 0.0635352132941405\n",
            "Selling 0.0033326831210070723 bitcoin for 30.117857286515438 dollars on day 1156\n",
            "632.4750030168242 0.06020253017313342\n",
            "Buying 0.0036274878840948832 amount of bitcoin with 31.62375015084121 for 8717.81 dolallars on day 1157\n",
            "600.8512528659829 0.06383001805722831\n",
            "600.8512528659829 0.06383001805722831\n",
            "Buying 0.0035521002357971443 amount of bitcoin with 30.04256264329915 for 8457.69 dolallars on day 1161\n",
            "570.8086902226838 0.06738211829302546\n",
            "570.8086902226838 0.06738211829302546\n",
            "Buying 0.003490761915209558 amount of bitcoin with 28.540434511134194 for 8175.99 dolallars on day 1164\n",
            "542.2682557115496 0.07087288020823501\n",
            "542.2682557115496 0.07087288020823501\n",
            "Buying 0.0035595593562324467 amount of bitcoin with 27.11341278557748 for 7617.07 dolallars on day 1167\n",
            "515.1548429259722 0.07443243956446746\n",
            "515.1548429259722 0.07443243956446746\n",
            "Buying 0.0035350679210165046 amount of bitcoin with 25.75774214629861 for 7286.35 dolallars on day 1168\n",
            "489.39710077967356 0.07796750748548396\n",
            "489.39710077967356 0.07796750748548396\n",
            "Buying 0.0035425565392164463 amount of bitcoin with 24.46985503898368 for 6907.4 dolallars on day 1170\n",
            "464.9272457406899 0.08151006402470042\n",
            "Selling 0.00326024505270285 bitcoin for 23.246362287034497 dollars on day 1171\n",
            "Selling 0.0032441828698131434 bitcoin for 24.40868040138622 dollars on day 1173\n",
            "Selling 0.0033037980709503907 bitcoin for 25.62911442145553 dollars on day 1175\n",
            "538.2114028505661 0.07170183803123402\n",
            "Buying 0.003560673078987883 amount of bitcoin with 26.910570142528304 for 7557.72 dolallars on day 1176\n",
            "511.3008327080378 0.0752625111102219\n",
            "511.3008327080378 0.0752625111102219\n",
            "Buying 0.00345347996949783 amount of bitcoin with 25.56504163540189 for 7402.69 dolallars on day 1177\n",
            "485.7357910726359 0.07871599107971973\n",
            "485.7357910726359 0.07871599107971973\n",
            "Buying 0.003328430189471752 amount of bitcoin with 24.286789553631795 for 7296.77 dolallars on day 1179\n",
            "461.44900151900407 0.08204442126919148\n",
            "461.44900151900407 0.08204442126919148\n",
            "Buying 0.003207692371723337 amount of bitcoin with 23.072450075950204 for 7192.85 dolallars on day 1180\n",
            "438.37655144305387 0.08525211364091481\n",
            "Selling 0.0029636176961443453 bitcoin for 21.918827572152694 dollars on day 1181\n",
            "Selling 0.0030494487287003945 bitcoin for 23.014768950760327 dollars on day 1182\n",
            "483.31014796596685 0.07923904721607009\n",
            "Buying 0.0032934611073508597 amount of bitcoin with 24.165507398298345 for 7337.42 dolallars on day 1185\n",
            "459.1446405676685 0.08253250832342095\n",
            "459.1446405676685 0.08253250832342095\n",
            "Buying 0.003179337359001466 amount of bitcoin with 22.957232028383427 for 7220.76 dolallars on day 1186\n",
            "436.18740853928506 0.08571184568242242\n",
            "436.18740853928506 0.08571184568242242\n",
            "Buying 0.003085762977552125 amount of bitcoin with 21.809370426964254 for 7067.74 dolallars on day 1190\n",
            "414.3780381123208 0.08879760865997455\n",
            "414.3780381123208 0.08879760865997455\n",
            "Buying 0.003011669661869259 amount of bitcoin with 20.71890190561604 for 6879.54 dolallars on day 1192\n",
            "393.6591362067047 0.09180927832184381\n",
            "393.6591362067047 0.09180927832184381\n",
            "Buying 0.002976799696668427 amount of bitcoin with 19.682956810335238 for 6612.12 dolallars on day 1193\n",
            "373.9761793963695 0.09478607801851224\n",
            "Selling 0.0025670050162498303 bitcoin for 18.698808969818476 dollars on day 1194\n",
            "392.67498836618796 0.09221907300226241\n",
            "Buying 0.00274564869376682 amount of bitcoin with 19.6337494183094 for 7150.86 dolallars on day 1195\n",
            "373.0412389478786 0.09496472169602924\n",
            "Selling 0.002482172512199085 bitcoin for 18.65206194739393 dollars on day 1198\n",
            "391.6933008952725 0.09248254918383016\n",
            "Buying 0.002674740653579806 amount of bitcoin with 19.584665044763625 for 7322.08 dolallars on day 1199\n",
            "372.1086358505089 0.09515728983740997\n",
            "372.1086358505089 0.09515728983740997\n",
            "Buying 0.0025867031933017614 amount of bitcoin with 18.605431792525447 for 7192.72 dolallars on day 1201\n",
            "353.50320405798345 0.09774399303071173\n",
            "Selling 0.0024208999780715944 bitcoin for 17.675160202899175 dollars on day 1204\n",
            "371.17836426088263 0.09532309305264013\n",
            "Buying 0.002589022825888408 amount of bitcoin with 18.558918213044134 for 7168.31 dolallars on day 1207\n",
            "352.6194460478385 0.09791211587852854\n",
            "352.6194460478385 0.09791211587852854\n",
            "Buying 0.0025389018526469687 amount of bitcoin with 17.630972302391925 for 6944.33 dolallars on day 1209\n",
            "334.9884737454466 0.1004510177311755\n",
            "Selling 0.0022861893968036374 bitcoin for 16.74942368727233 dollars on day 1210\n",
            "Selling 0.0022665744160041378 bitcoin for 17.586894871635945 dollars on day 1213\n",
            "Selling 0.0022615035772855377 bitcoin for 18.46623961521774 dollars on day 1214\n",
            "387.79103191957256 0.09363675034108218\n",
            "Buying 0.002410841152602517 amount of bitcoin with 19.38955159597863 for 8042.65 dolallars on day 1215\n",
            "368.40148032359394 0.09604759149368469\n",
            "368.40148032359394 0.09604759149368469\n",
            "Buying 0.0023561348819353097 amount of bitcoin with 18.420074016179697 for 7817.92 dolallars on day 1216\n",
            "349.98140630741426 0.09840372637562\n",
            "Selling 0.0021380326507601673 bitcoin for 17.499070315370712 dollars on day 1217\n",
            "367.480476622785 0.09626569372485984\n",
            "Buying 0.0022905998550318274 amount of bitcoin with 18.37402383113925 for 8021.49 dolallars on day 1218\n",
            "349.1064527916457 0.09855629357989167\n",
            "Selling 0.002135476719339842 bitcoin for 17.45532263958229 dollars on day 1219\n",
            "Selling 0.0020727457835707196 bitcoin for 18.3280887715614 dollars on day 1221\n",
            "384.8898642027894 0.0943480710769811\n",
            "Buying 0.0022064236433650733 amount of bitcoin with 19.24449321013947 for 8722.03 dolallars on day 1223\n",
            "365.64537099264993 0.09655449472034616\n",
            "Selling 0.0020541090059067965 bitcoin for 18.282268549632498 dollars on day 1224\n",
            "383.9276395422824 0.09450038571443936\n",
            "Buying 0.0022056288579484383 amount of bitcoin with 19.19638197711412 for 8703.36 dolallars on day 1226\n",
            "364.7312575651683 0.0967060145723878\n",
            "364.7312575651683 0.0967060145723878\n",
            "Buying 0.002174096772486104 amount of bitcoin with 18.236562878258415 for 8388.11 dolallars on day 1230\n",
            "346.49469468690984 0.0988801113448739\n",
            "Selling 0.0020172202493992483 bitcoin for 17.324734734345494 dollars on day 1233\n",
            "Selling 0.0020448989825583327 bitcoin for 18.190971471062767 dollars on day 1234\n",
            "Selling 0.0020350682842301317 bitcoin for 19.100520044615905 dollars on day 1235\n",
            "401.110920936934 0.09278292382868619\n",
            "Buying 0.0021612022279385787 amount of bitcoin with 20.0555460468467 for 9279.81 dolallars on day 1236\n",
            "381.0553748900873 0.09494412605662478\n",
            "Selling 0.0020050543963773633 bitcoin for 19.052768744504366 dollars on day 1237\n",
            "400.10814363459167 0.09293907166024741\n",
            "Buying 0.0021433362062413777 amount of bitcoin with 20.005407181729584 for 9333.77 dolallars on day 1238\n",
            "380.1027364528621 0.09508240786648879\n",
            "380.1027364528621 0.09508240786648879\n",
            "Buying 0.0020743119863528723 amount of bitcoin with 19.005136822643106 for 9162.14 dolallars on day 1241\n",
            "361.09759963021895 0.09715671985284166\n",
            "Selling 0.0018778021593059675 bitcoin for 18.054879981510947 dollars on day 1243\n",
            "Selling 0.001943243612486136 bitcoin for 18.957623980586497 dollars on day 1244\n",
            "Selling 0.002009212079758378 bitcoin for 19.905505179615822 dollars on day 1246\n",
            "Selling 0.002056675575832565 bitcoin for 20.900780438596612 dollars on day 1247\n",
            "438.91638921052885 0.0892697864254586\n",
            "Buying 0.002226919037394652 amount of bitcoin with 21.945819460526444 for 9854.79 dolallars on day 1248\n",
            "416.9705697500024 0.09149670546285325\n",
            "Selling 0.0020289788297367223 bitcoin for 20.848528487500122 dollars on day 1249\n",
            "437.81909823750254 0.08946772663311653\n",
            "Buying 0.002210276571572896 amount of bitcoin with 21.890954911875127 for 9904.17 dolallars on day 1253\n",
            "415.9281433256274 0.09167800320468943\n",
            "415.9281433256274 0.09167800320468943\n",
            "Buying 0.00214309121832921 amount of bitcoin with 20.79640716628137 for 9703.93 dolallars on day 1255\n",
            "395.131736159346 0.09382109442301864\n",
            "Selling 0.0019406017108895112 bitcoin for 19.756586807967302 dollars on day 1256\n",
            "414.88832296731334 0.09188049271212913\n",
            "Buying 0.0021598147731912718 amount of bitcoin with 20.74441614836567 for 9604.72 dolallars on day 1257\n",
            "394.1439068189477 0.0940403074853204\n",
            "Selling 0.0019728126883570857 bitcoin for 19.707195340947386 dollars on day 1261\n",
            "413.8511021598951 0.09206749479696331\n",
            "Buying 0.0021412552174874928 amount of bitcoin with 20.692555107994757 for 9663.75 dolallars on day 1262\n",
            "393.15854705190037 0.09420875001445081\n",
            "393.15854705190037 0.09420875001445081\n",
            "Buying 0.002111678010623421 amount of bitcoin with 19.65792735259502 for 9309.15 dolallars on day 1263\n",
            "373.50061969930533 0.09632042802507423\n",
            "373.50061969930533 0.09632042802507423\n",
            "Buying 0.002125660289313014 amount of bitcoin with 18.675030984965268 for 8785.52 dolallars on day 1264\n",
            "354.8255887143401 0.09844608831438725\n",
            "354.8255887143401 0.09844608831438725\n",
            "Buying 0.0020788523588957104 amount of bitcoin with 17.741279435717004 for 8534.17 dolallars on day 1267\n",
            "337.0843092786231 0.10052494067328296\n",
            "Selling 0.0018910081729386608 bitcoin for 16.854215463931155 dollars on day 1268\n",
            "353.93852474255425 0.0986339325003443\n",
            "Buying 0.002021503190089454 amount of bitcoin with 17.696926237127713 for 8754.34 dolallars on day 1270\n",
            "336.2415985054265 0.10065543569043375\n",
            "Selling 0.0018541256001199163 bitcoin for 16.812079925271327 dollars on day 1272\n",
            "353.05367843069786 0.09880131009031384\n",
            "Buying 0.0019837529958583395 amount of bitcoin with 17.652683921534894 for 8898.63 dolallars on day 1274\n",
            "335.40099450916296 0.10078506308617219\n",
            "335.40099450916296 0.10078506308617219\n",
            "Buying 0.002085987940047385 amount of bitcoin with 16.770049725458147 for 8039.38 dolallars on day 1275\n",
            "318.6309447837048 0.10287105102621957\n",
            "318.6309447837048 0.10287105102621957\n",
            "Buying 0.002008530982229473 amount of bitcoin with 15.931547239185242 for 7931.94 dolallars on day 1276\n",
            "302.69939754451957 0.10487958200844905\n",
            "302.69939754451957 0.10487958200844905\n",
            "Buying 0.003133397901380267 amount of bitcoin with 15.134969877225979 for 4830.21 dolallars on day 1279\n",
            "287.5644276672936 0.10801297990982932\n",
            "Selling 0.0025634060405033817 bitcoin for 14.378221383364682 dollars on day 1280\n",
            "301.9426490506583 0.10544957386932594\n",
            "Buying 0.0029222556457733284 amount of bitcoin with 15.097132452532916 for 5166.26 dolallars on day 1281\n",
            "286.8455165981254 0.10837182951509926\n",
            "Selling 0.0026815811395297083 bitcoin for 14.34227582990627 dollars on day 1282\n",
            "301.1877924280317 0.10569024837556955\n",
            "Buying 0.002996088537686708 amount of bitcoin with 15.059389621401586 for 5026.35 dolallars on day 1283\n",
            "286.1284028066301 0.10868633691325626\n",
            "Selling 0.0026702989094636426 bitcoin for 14.306420140331506 dollars on day 1284\n",
            "Selling 0.0024247386924309273 bitcoin for 15.02174114734808 dollars on day 1286\n",
            "315.45656409430967 0.10359129931136168\n",
            "Buying 0.0027088884736966323 amount of bitcoin with 15.772828204715484 for 5822.62 dolallars on day 1289\n",
            "299.6837358895942 0.10630018778505831\n",
            "Selling 0.002304493705857701 bitcoin for 14.98418679447971 dollars on day 1290\n",
            "Selling 0.0023245060765700615 bitcoin for 15.733396134203694 dollars on day 1291\n",
            "330.40131881827756 0.10167118800263056\n",
            "Buying 0.0025937874862678777 amount of bitcoin with 16.520065940913877 for 6369.09 dolallars on day 1294\n",
            "313.8812528773637 0.10426497548889843\n",
            "313.8812528773637 0.10426497548889843\n",
            "Buying 0.002506658357576436 amount of bitcoin with 15.694062643868186 for 6260.95 dolallars on day 1295\n",
            "298.1871902334955 0.10677163384647487\n",
            "298.1871902334955 0.10677163384647487\n",
            "Buying 0.0025332745741885065 amount of bitcoin with 14.909359511674777 for 5885.41 dolallars on day 1296\n",
            "283.27783072182075 0.10930490842066337\n",
            "Selling 0.002211280291148572 bitcoin for 14.163891536091038 dollars on day 1297\n",
            "Selling 0.0022354391582723836 bitcoin for 14.872086112895591 dollars on day 1299\n",
            "Selling 0.002293352643523217 bitcoin for 15.615690418540371 dollars on day 1300\n",
            "Selling 0.002232878709481887 bitcoin for 16.396474939467392 dollars on day 1304\n",
            "344.3259737288152 0.10033195761823731\n",
            "Buying 0.0023893108349037563 amount of bitcoin with 17.216298686440762 for 7205.55 dolallars on day 1305\n",
            "327.1096750423744 0.10272126845314107\n",
            "Selling 0.00222069479039715 bitcoin for 16.355483752118722 dollars on day 1306\n",
            "343.46515879449316 0.10050057366274393\n",
            "Buying 0.0024985680610199353 amount of bitcoin with 17.17325793972466 for 6873.24 dolallars on day 1308\n",
            "326.2919008547685 0.10299914172376386\n",
            "326.2919008547685 0.10299914172376386\n",
            "Buying 0.0024630265169145247 amount of bitcoin with 16.31459504273843 for 6623.8 dolallars on day 1313\n",
            "309.9773058120301 0.10546216824067838\n",
            "Selling 0.0021791727944244954 bitcoin for 15.498865290601508 dollars on day 1314\n",
            "Selling 0.0022417690478405234 bitcoin for 16.27380855513158 dollars on day 1316\n",
            "341.7499796577632 0.10104122639841336\n",
            "Buying 0.0023965502273322675 amount of bitcoin with 17.08749898288816 for 7130.04 dolallars on day 1317\n",
            "324.66248067487504 0.10343777662574562\n",
            "324.66248067487504 0.10343777662574562\n",
            "Buying 0.0023731804781328947 amount of bitcoin with 16.233124033743753 for 6840.24 dolallars on day 1318\n",
            "308.4293566411313 0.10581095710387851\n",
            "Selling 0.002162598437532035 bitcoin for 15.421467832056564 dollars on day 1320\n",
            "Selling 0.002165473042641943 bitcoin for 16.192541223659394 dollars on day 1321\n",
            "Selling 0.0022082831599414443 bitcoin for 17.002168284842366 dollars on day 1324\n",
            "Selling 0.0020338379151416137 bitcoin for 17.85227669908448 dollars on day 1327\n",
            "374.8978106807741 0.09724076454862148\n",
            "Buying 0.0021723710950736553 amount of bitcoin with 18.744890534038706 for 8628.77 dolallars on day 1328\n",
            "356.1529201467354 0.09941313564369512\n",
            "Selling 0.0020179413152843022 bitcoin for 17.80764600733677 dollars on day 1329\n",
            "Selling 0.0020836197191055326 bitcoin for 18.69802830770361 dollars on day 1330\n",
            "Selling 0.0021408111758189634 bitcoin for 19.632929723088793 dollars on day 1334\n",
            "Selling 0.0020609465061907878 bitcoin for 20.614576209243232 dollars on day 1335\n",
            "432.90610039410785 0.09110981692729553\n",
            "Buying 0.002203802258211875 amount of bitcoin with 21.645305019705393 for 9821.8 dolallars on day 1336\n",
            "411.26079537440245 0.0933136191855074\n",
            "411.26079537440245 0.0933136191855074\n",
            "Buying 0.0021583077599132737 amount of bitcoin with 20.563039768720124 for 9527.39 dolallars on day 1337\n",
            "390.69775560568235 0.09547192694542067\n",
            "390.69775560568235 0.09547192694542067\n",
            "Buying 0.0022314212161897045 amount of bitcoin with 19.53488778028412 for 8754.46 dolallars on day 1338\n",
            "371.1628678253982 0.09770334816161037\n",
            "371.1628678253982 0.09770334816161037\n",
            "Buying 0.0021536039213519292 amount of bitcoin with 18.558143391269912 for 8617.25 dolallars on day 1339\n",
            "352.6047244341283 0.0998569520829623\n",
            "Selling 0.002000133441227883 bitcoin for 17.630236221706415 dollars on day 1340\n",
            "Selling 0.001989242131930042 bitcoin for 18.511748032791736 dollars on day 1341\n",
            "Selling 0.001985364654891553 bitcoin for 19.437335434431322 dollars on day 1342\n",
            "408.18404412305773 0.09388221185491283\n",
            "Buying 0.002193691059704145 amount of bitcoin with 20.409202206152887 for 9303.59 dolallars on day 1343\n",
            "387.77484191690485 0.09607590291461697\n",
            "Selling 0.0020051649632702386 bitcoin for 19.388742095845245 dollars on day 1345\n",
            "Selling 0.0020803924077931264 bitcoin for 20.358179200637508 dollars on day 1347\n",
            "427.5217632133876 0.09199034554355359\n",
            "Buying 0.0022475901446133008 amount of bitcoin with 21.376088160669383 for 9510.67 dolallars on day 1348\n",
            "406.1456750527182 0.0942379356881669\n",
            "406.1456750527182 0.0942379356881669\n",
            "Buying 0.002242023385150312 amount of bitcoin with 20.30728375263591 for 9057.57 dolallars on day 1349\n",
            "385.8383913000823 0.0964799590733172\n",
            "Selling 0.0021044368289984267 bitcoin for 19.291919565004118 dollars on day 1350\n",
            "405.1303108650864 0.09437552224431878\n",
            "Buying 0.002320139958887095 amount of bitcoin with 20.256515543254324 for 8730.73 dolallars on day 1352\n",
            "384.87379532183206 0.09669566220320587\n",
            "Selling 0.002162294937794433 bitcoin for 19.243689766091606 dollars on day 1353\n",
            "Selling 0.0021968781059279094 bitcoin for 20.205874254396186 dollars on day 1355\n",
            "Selling 0.002217128474253987 bitcoin for 21.216167967115993 dollars on day 1356\n",
            "445.53952730943587 0.09011936068522955\n",
            "Buying 0.0023633591802095693 amount of bitcoin with 22.276976365471796 for 9425.98 dolallars on day 1357\n",
            "423.2625509439641 0.09248271986543911\n",
            "Selling 0.0021821931664138547 bitcoin for 21.163127547198204 dollars on day 1358\n",
            "444.4256784911623 0.09030052669902526\n",
            "Buying 0.002351249616389455 amount of bitcoin with 22.221283924558115 for 9450.84 dolallars on day 1359\n",
            "422.20439456660415 0.09265177631541471\n",
            "Selling 0.0020687714534394276 bitcoin for 21.110219728330208 dollars on day 1360\n",
            "443.31461429493436 0.09058300486197529\n",
            "Buying 0.002326971584351038 amount of bitcoin with 22.16573071474672 for 9525.57 dolallars on day 1361\n",
            "421.14888358018766 0.09290997644632633\n",
            "Selling 0.002180302025981398 bitcoin for 21.057444179009384 dollars on day 1362\n",
            "Selling 0.0022574078251559903 bitcoin for 22.110316387959855 dollars on day 1363\n",
            "464.3166441471569 0.08847226659518895\n",
            "Buying 0.0024123478069731498 amount of bitcoin with 23.21583220735785 for 9623.75 dolallars on day 1364\n",
            "441.1008119397991 0.0908846144021621\n",
            "Selling 0.002261162576520036 bitcoin for 22.055040596989954 dollars on day 1366\n",
            "Selling 0.00234103197459389 bitcoin for 23.157792626839452 dollars on day 1369\n",
            "486.3136451636285 0.08628241985104818\n",
            "Buying 0.0026184129361133167 amount of bitcoin with 24.315682258181425 for 9286.42 dolallars on day 1370\n",
            "461.9979629054471 0.08890083278716149\n",
            "Selling 0.00244185744196571 bitcoin for 23.099898145272356 dollars on day 1371\n",
            "485.0978610507194 0.08645897534519578\n",
            "Buying 0.0025996474895189398 amount of bitcoin with 24.254893052535973 for 9330.07 dolallars on day 1373\n",
            "460.84296799818344 0.08905862283471472\n",
            "Selling 0.0024188692420647885 bitcoin for 23.042148399909173 dollars on day 1375\n",
            "483.88511639809263 0.08663975359264993\n",
            "Buying 0.0025793367206613017 amount of bitcoin with 24.194255819904633 for 9380.03 dolallars on day 1377\n",
            "459.690860578188 0.08921909031331123\n",
            "Selling 0.0023715208606062176 bitcoin for 22.9845430289094 dollars on day 1381\n",
            "482.6754036070974 0.08684756945270501\n",
            "Buying 0.002601580558821772 amount of bitcoin with 24.133770180354873 for 9276.58 dolallars on day 1383\n",
            "458.5416334267425 0.08944915001152678\n",
            "458.5416334267425 0.08944915001152678\n",
            "Buying 0.0025044739630821215 amount of bitcoin with 22.927081671337127 for 9154.45 dolallars on day 1385\n",
            "435.6145517554054 0.0919536239746089\n",
            "435.6145517554054 0.0919536239746089\n",
            "Buying 0.002418943939434052 amount of bitcoin with 21.780727587770272 for 9004.23 dolallars on day 1386\n",
            "413.8338241676351 0.09437256791404296\n",
            "Selling 0.0022669689638401174 bitcoin for 20.691691208381755 dollars on day 1387\n",
            "Selling 0.002352250098934956 bitcoin for 21.726275768800846 dollars on day 1390\n",
            "456.25179114481773 0.08975334885126789\n",
            "Buying 0.0025101936356859158 amount of bitcoin with 22.812589557240887 for 9087.98 dolallars on day 1391\n",
            "433.43920158757686 0.0922635424869538\n",
            "Selling 0.0023181269345173765 bitcoin for 21.671960079378845 dollars on day 1395\n",
            "455.1111616669557 0.08994541555243643\n",
            "Buying 0.0024632452428597173 amount of bitcoin with 22.755558083347786 for 9238.04 dolallars on day 1398\n",
            "432.3556035836079 0.09240866079529615\n",
            "432.3556035836079 0.09240866079529615\n",
            "Buying 0.002367515078214916 amount of bitcoin with 21.617780179180397 for 9131.0 dolallars on day 1405\n",
            "410.7378234044275 0.09477617587351107\n",
            "Selling 0.002186482973962794 bitcoin for 20.536891170221377 dollars on day 1410\n",
            "Selling 0.0022609658532443274 bitcoin for 21.563735728732446 dollars on day 1411\n",
            "Selling 0.0023324154020261728 bitcoin for 22.64192251516907 dollars on day 1414\n",
            "Selling 0.0023920339356772904 bitcoin for 23.774018640927522 dollars on day 1415\n",
            "Selling 0.002260624463248379 bitcoin for 24.9627195729739 dollars on day 1416\n",
            "524.2171110324518 0.08334365324535213\n",
            "Buying 0.0023969821097895912 amount of bitcoin with 26.210855551622593 for 10934.94 dolallars on day 1417\n",
            "498.0062554808292 0.08574063535514172\n",
            "Selling 0.0022427319531285236 bitcoin for 24.900312774041463 dollars on day 1418\n",
            "Selling 0.0023047959263271063 bitcoin for 26.145328412743535 dollars on day 1420\n",
            "Selling 0.0023218297192653657 bitcoin for 27.45259483338071 dollars on day 1421\n",
            "576.5044915009948 0.07887127775642072\n",
            "Buying 0.0026020782680133045 amount of bitcoin with 28.825224575049745 for 11077.77 dolallars on day 1422\n",
            "547.6792669259451 0.08147335602443402\n",
            "Selling 0.0024357387453489065 bitcoin for 27.383963346297254 dollars on day 1423\n",
            "Selling 0.0024470192636781517 bitcoin for 28.753161513612117 dollars on day 1425\n",
            "603.8163917858544 0.07659059801540696\n",
            "Buying 0.0026014010123813266 amount of bitcoin with 30.190819589292722 for 11605.6 dolallars on day 1427\n",
            "573.6255721965617 0.07919199902778828\n",
            "Selling 0.0024373091037958538 bitcoin for 28.681278609828087 dollars on day 1428\n",
            "Selling 0.002532184190262657 bitcoin for 30.11534254031949 dollars on day 1430\n",
            "632.4221933467093 0.07422250573372977\n",
            "Buying 0.0027756246619321306 amount of bitcoin with 31.621109667335464 for 11392.43 dolallars on day 1431\n",
            "600.8010836793738 0.07699813039566189\n",
            "Selling 0.0025956768909972073 bitcoin for 30.040054183968692 dollars on day 1432\n",
            "Selling 0.002678178252230506 bitcoin for 31.54205689316713 dollars on day 1433\n",
            "Selling 0.0027798499193659805 bitcoin for 33.119159737825484 dollars on day 1436\n",
            "Selling 0.0028286895849845904 bitcoin for 34.77511772471676 dollars on day 1437\n",
            "730.2774722190519 0.0661157357480836\n",
            "Buying 0.003050568703278457 amount of bitcoin with 36.513873610952594 for 11969.53 dolallars on day 1438\n",
            "693.7635986080993 0.06916630445136206\n",
            "693.7635986080993 0.06916630445136206\n",
            "Buying 0.002956211004806968 amount of bitcoin with 34.688179930404964 for 11734.0 dolallars on day 1439\n",
            "659.0754186776943 0.07212251545616903\n",
            "Selling 0.002777201317219098 bitcoin for 32.95377093388472 dollars on day 1440\n",
            "692.029189611579 0.06934531413894993\n",
            "Buying 0.0030028690492396775 amount of bitcoin with 34.60145948057895 for 11522.8 dolallars on day 1441\n",
            "657.4277301310001 0.0723481831881896\n",
            "Selling 0.002813502402250536 bitcoin for 32.87138650655 dollars on day 1442\n",
            "690.2991166375501 0.06953468078593907\n",
            "Buying 0.003044344896702728 amount of bitcoin with 34.51495583187751 for 11337.4 dolallars on day 1445\n",
            "655.7841608056726 0.0725790256826418\n",
            "Selling 0.002859348572539617 bitcoin for 32.78920804028363 dollars on day 1446\n",
            "688.5733688459562 0.06971967711010219\n",
            "Buying 0.0030462429640654904 amount of bitcoin with 34.42866844229781 for 11302.01 dolallars on day 1447\n",
            "654.1447004036584 0.07276592007416767\n",
            "Selling 0.0028355391335037968 bitcoin for 32.70723502018292 dollars on day 1448\n",
            "Selling 0.002933314152742199 bitcoin for 34.34259677119206 dollars on day 1450\n",
            "Selling 0.0030243202658462812 bitcoin for 36.05972660975167 dollars on day 1452\n",
            "757.2542588047851 0.06397274652207538\n",
            "Buying 0.0033220366099965654 amount of bitcoin with 37.862712940239255 for 11397.44 dolallars on day 1453\n",
            "719.3915458645458 0.06729478313207195\n",
            "719.3915458645458 0.06729478313207195\n",
            "Buying 0.0035307525875535135 amount of bitcoin with 35.969577293227296 for 10187.51 dolallars on day 1454\n",
            "683.4219685713185 0.07082553571962547\n",
            "Selling 0.003264373090333002 bitcoin for 34.17109842856593 dollars on day 1455\n",
            "717.5930669998845 0.06756116262929246\n",
            "Buying 0.003531594031075397 amount of bitcoin with 35.87965334999423 for 10159.62 dolallars on day 1456\n",
            "681.7134136498903 0.07109275666036785\n",
            "Selling 0.003287666423202599 bitcoin for 34.085670682494516 dollars on day 1458\n",
            "715.7990843323848 0.06780509023716526\n",
            "Buying 0.00353602563810764 amount of bitcoin with 35.78995421661924 for 10121.52 dolallars on day 1459\n",
            "680.0091301157656 0.07134111587527289\n",
            "Selling 0.0033243079427198426 bitcoin for 34.000456505788286 dollars on day 1460\n",
            "Selling 0.003448435409940797 bitcoin for 35.700479331077695 dollars on day 1461\n",
            "Selling 0.0035116409825185282 bitcoin for 37.48550329763158 dollars on day 1465\n",
            "Selling 0.0036492828842953078 bitcoin for 39.35977846251316 dollars on day 1466\n",
            "Selling 0.003774766554258357 bitcoin for 41.32776738563882 dollars on day 1467\n",
            "Selling 0.003915934654184591 bitcoin for 43.39415575492076 dollars on day 1470\n",
            "911.277270853336 0.049716747447355464\n",
            "Buying 0.0041726487151755596 amount of bitcoin with 45.5638635426668 for 10919.65 dolallars on day 1471\n",
            "865.7134073106691 0.05388939616253102\n",
            "865.7134073106691 0.05388939616253102\n",
            "Buying 0.004149929184861787 amount of bitcoin with 43.28567036553346 for 10430.46 dolallars on day 1472\n",
            "822.4277369451356 0.05803932534739281\n",
            "Selling 0.003904341805170875 bitcoin for 41.12138684725679 dollars on day 1473\n",
            "863.5491237923924 0.05413498354222194\n",
            "Buying 0.0042188226651104525 amount of bitcoin with 43.17745618961962 for 10234.48 dolallars on day 1474\n",
            "820.3716676027727 0.05835380620733239\n",
            "Selling 0.003821928806443521 bitcoin for 41.01858338013864 dollars on day 1475\n",
            "Selling 0.003972909061060584 bitcoin for 43.06951254914557 dollars on day 1480\n",
            "904.4597635320569 0.050558968339828286\n",
            "Buying 0.004258589896885544 amount of bitcoin with 45.22298817660285 for 10619.24 dolallars on day 1482\n",
            "859.236775355454 0.05481755823671383\n",
            "Selling 0.00398216618045158 bitcoin for 42.961838767772704 dollars on day 1486\n",
            "902.1986141232268 0.05083539205626225\n",
            "Buying 0.004254152846652346 amount of bitcoin with 45.10993070616134 for 10603.74 dolallars on day 1487\n",
            "857.0886834170655 0.055089544902914594\n",
            "Selling 0.003923213147203984 bitcoin for 42.854434170853274 dollars on day 1489\n",
            "Selling 0.0040672858261853895 bitcoin for 44.99715587939594 dollars on day 1490\n",
            "Selling 0.004180163950054786 bitcoin for 47.247013673365736 dollars on day 1491\n",
            "Selling 0.004298890156102927 bitcoin for 49.609364357034025 dollars on day 1493\n",
            "1041.7966514977145 0.038619991823367505\n",
            "Buying 0.004557992532085932 amount of bitcoin with 52.089832574885726 for 11428.24 dolallars on day 1494\n",
            "989.7068189228288 0.04317798435545344\n",
            "989.7068189228288 0.04317798435545344\n",
            "Buying 0.00436857516185214 amount of bitcoin with 49.48534094614144 for 11327.57 dolallars on day 1497\n",
            "940.2214779766873 0.04754655951730558\n",
            "Selling 0.004085006682090541 bitcoin for 47.01107389883437 dollars on day 1499\n",
            "Selling 0.0041980741539302145 bitcoin for 49.36162759377609 dollars on day 1500\n",
            "Selling 0.004346139182343062 bitcoin for 51.829708973464896 dollars on day 1501\n",
            "Selling 0.0042411986089094505 bitcoin for 54.42119442213814 dollars on day 1502\n",
            "Selling 0.004398857153884263 bitcoin for 57.14225414324505 dollars on day 1503\n",
            "Selling 0.00457017554613468 bitcoin for 59.9993668504073 dollars on day 1505\n",
            "Selling 0.004614838928915909 bitcoin for 62.99933519292767 dollars on day 1508\n",
            "1322.986039051481 0.017092269261097467\n",
            "Buying 0.0049777486607400145 amount of bitcoin with 66.14930195257405 for 13289.0 dolallars on day 1509\n",
            "1256.836737098907 0.022070017921837482\n",
            "Selling 0.004669249156672756 bitcoin for 62.84183685494535 dollars on day 1510\n",
            "Selling 0.00486437823248048 bitcoin for 65.98392869769262 dollars on day 1511\n",
            "Selling 0.005016764646480113 bitcoin for 69.28312513257724 dollars on day 1512\n",
            "1454.945627784122 0.007519625886204134\n",
            "Buying 0.0053588486471407805 amount of bitcoin with 72.74728138920611 for 13575.17 dolallars on day 1514\n",
            "1382.198346394916 0.012878474533344914\n",
            "Selling 0.004928217184084628 bitcoin for 69.1099173197458 dollars on day 1515\n",
            "Selling 0.005126272602253463 bitcoin for 72.56541318573309 dollars on day 1516\n",
            "1523.873676900395 0.0028239847470068235\n",
            "Buying 0.005134400629994646 amount of bitcoin with 76.19368384501975 for 14839.84 dolallars on day 1519\n",
            "1447.6799930553752 0.00795838537700147\n",
            "Selling 0.004672769269929425 bitcoin for 72.38399965276876 dollars on day 1520\n",
            "1520.063992708144 0.0032856161070720447\n",
            "Buying 0.004958283647251706 amount of bitcoin with 76.0031996354072 for 15328.53 dolallars on day 1521\n",
            "1444.060793072737 0.008243899754323752\n",
            "Selling 0.004596387318683455 bitcoin for 72.20303965363685 dollars on day 1523\n",
            "1516.2638327263737 0.0036475124356402967\n",
            "Buying 0.004711507167411409 amount of bitcoin with 75.81319163631869 for 16091.07 dolallars on day 1526\n",
            "1440.4506410900551 0.008359019603051705\n",
            "1440.4506410900551 0.008359019603051705\n",
            "Buying 0.0045103839174020525 amount of bitcoin with 72.02253205450276 for 15968.16 dolallars on day 1527\n",
            "1368.4281090355523 0.012869403520453758\n",
            "Selling 0.004090929256346138 bitcoin for 68.42140545177762 dollars on day 1528\n",
            "Selling 0.004063552800856942 bitcoin for 71.84247572436651 dollars on day 1529\n",
            "Selling 0.004238267911564481 bitcoin for 75.43459951058483 dollars on day 1530\n",
            "1584.1265897222813 0.00047665355168619635\n",
            "Buying 0.004299485703513033 amount of bitcoin with 79.20632948611407 for 18422.28 dolallars on day 1534\n",
            "1504.9202602361672 0.004776139255199229\n",
            "Selling 0.003924680376487199 bitcoin for 75.24601301180836 dollars on day 1536\n",
            "1580.1662732479756 0.0008514588787120305\n",
            "Buying 0.004216070270888632 amount of bitcoin with 79.00831366239879 for 18739.8 dolallars on day 1537\n",
            "1501.157959585577 0.005067529149600663\n",
            "1501.157959585577 0.005067529149600663\n",
            "Buying 0.004376186371481278 amount of bitcoin with 75.05789797927885 for 17151.44 dolallars on day 1538\n",
            "1426.1000616062981 0.00944371552108194\n",
            "Selling 0.004021165925480837 bitcoin for 71.3050030803149 dollars on day 1540\n",
            "Selling 0.004115649708345097 bitcoin for 74.87025323433066 dollars on day 1541\n",
            "1572.2753179209437 0.001306899887256006\n",
            "Buying 0.004183247690892291 amount of bitcoin with 78.6137658960472 for 18792.52 dolallars on day 1543\n",
            "1493.6615520248965 0.0054901475781482965\n",
            "Selling 0.0038842874150864554 bitcoin for 74.68307760124483 dollars on day 1544\n",
            "1568.3446296261413 0.0016058601630618411\n",
            "Buying 0.004200062852196544 amount of bitcoin with 78.41723148130707 for 18670.49 dolallars on day 1546\n",
            "1489.9273981448343 0.005805923015258386\n",
            "Selling 0.0038891222427516127 bitcoin for 74.49636990724171 dollars on day 1547\n",
            "1564.423768052076 0.0019168007725067731\n",
            "Buying 0.004077968637477839 amount of bitcoin with 78.2211884026038 for 19181.41 dolallars on day 1549\n",
            "1486.2025796494722 0.005994769409984613\n",
            "1486.2025796494722 0.005994769409984613\n",
            "Buying 0.0040564799565952275 amount of bitcoin with 74.31012898247361 for 18318.87 dolallars on day 1550\n",
            "1411.8924506669987 0.010051249366579841\n",
            "Selling 0.003804788822627279 bitcoin for 70.59462253334993 dollars on day 1551\n",
            "1482.4870732003487 0.006246460543952562\n",
            "Buying 0.0040621070016274565 amount of bitcoin with 74.12435366001743 for 18247.76 dolallars on day 1552\n",
            "1408.3627195403312 0.01030856754558002\n",
            "1408.3627195403312 0.01030856754558002\n",
            "Buying 0.0039057479565007608 amount of bitcoin with 70.41813597701656 for 18029.36 dolallars on day 1553\n",
            "1337.9445835633146 0.014214315502080781\n",
            "Selling 0.0035577122685086205 bitcoin for 66.89722917816573 dollars on day 1554\n",
            "Selling 0.0036652228830145153 bitcoin for 70.24209063707401 dollars on day 1555\n",
            "Selling 0.003826101772612672 bitcoin for 73.75419516892771 dollars on day 1556\n",
            "1548.8380985474819 0.0031652785779449737\n",
            "Buying 0.0032967217040777235 amount of bitcoin with 77.4419049273741 for 23490.58 dolallars on day 1562\n",
            "1471.3961936201079 0.006462000282022698\n",
            "1471.3961936201079 0.006462000282022698\n",
            "Buying 0.0032344804190109593 amount of bitcoin with 73.5698096810054 for 22745.48 dolallars on day 1563\n",
            "1397.8263839391025 0.009696480701033657\n",
            "Selling 0.002933529843956078 bitcoin for 69.89131919695512 dollars on day 1564\n",
            "1467.7177031360575 0.006762950857077579\n",
            "Buying 0.0031559247178711246 amount of bitcoin with 73.38588515680287 for 23253.37 dolallars on day 1565\n",
            "1394.3318179792545 0.009918875574948704\n",
            "Selling 0.002939701996917747 bitcoin for 69.71659089896274 dollars on day 1566\n",
            "Selling 0.0029644312588094097 bitcoin for 73.20242044391087 dollars on day 1567\n",
            "Selling 0.0029067023809176877 bitcoin for 76.8625414661064 dollars on day 1568\n",
            "1614.1133707882345 0.001108039938303859\n",
            "Buying 0.003074902274483446 amount of bitcoin with 80.70566853941173 for 26246.58 dolallars on day 1569\n",
            "1533.4077022488227 0.004182942212787305\n",
            "Selling 0.002835790369029683 bitcoin for 76.67038511244114 dollars on day 1570\n",
            "1610.078087361264 0.0013471518437576224\n",
            "Buying 0.0025129960661208436 amount of bitcoin with 80.5039043680632 for 32035.03 dolallars on day 1577\n",
            "1529.5741829932008 0.003860147909878466\n",
            "Selling 0.0022462904345611494 bitcoin for 76.47870914966005 dollars on day 1578\n",
            "1606.0528921428609 0.0016138574753173165\n",
            "Buying 0.0019955568540310177 amount of bitcoin with 80.30264460714305 for 40240.72 dolallars on day 1582\n",
            "1525.7502475357178 0.0036094143293483342\n",
            "1525.7502475357178 0.0036094143293483342\n",
            "Buying 0.0019949616325899313 amount of bitcoin with 76.2875123767859 for 38240.09 dolallars on day 1583\n",
            "1449.462735158932 0.005604375961938265\n",
            "1449.462735158932 0.005604375961938265\n",
            "Buying 0.002038915715090435 amount of bitcoin with 72.4731367579466 for 35544.94 dolallars on day 1584\n",
            "1376.9895984009854 0.0076432916770287004\n",
            "1376.9895984009854 0.0076432916770287004\n",
            "Buying 0.0020242809682060317 amount of bitcoin with 68.84947992004928 for 34011.82 dolallars on day 1585\n",
            "1308.140118480936 0.009667572645234733\n",
            "Selling 0.0017491717308512767 bitcoin for 65.4070059240468 dollars on day 1586\n",
            "Selling 0.0017538314500093889 bitcoin for 68.67735622024915 dollars on day 1587\n",
            "1442.224480625232 0.006164569464374068\n",
            "Buying 0.0019580266606223007 amount of bitcoin with 72.1112240312616 for 36828.52 dolallars on day 1588\n",
            "1370.1132565939704 0.008122596124996368\n",
            "1370.1132565939704 0.008122596124996368\n",
            "Buying 0.001899494882315876 amount of bitcoin with 68.50566282969852 for 36065.2 dolallars on day 1589\n",
            "1301.6075937642718 0.010022091007312245\n",
            "1301.6075937642718 0.010022091007312245\n",
            "Buying 0.0018182427152176805 amount of bitcoin with 65.0803796882136 for 35793.01 dolallars on day 1590\n",
            "1236.5272140760583 0.011840333722529926\n",
            "Selling 0.0016877530571694941 bitcoin for 61.82636070380292 dollars on day 1591\n",
            "1298.3535747798612 0.010152580665360431\n",
            "Buying 0.0018022610895350201 amount of bitcoin with 64.91767873899306 for 36020.13 dolallars on day 1592\n",
            "1233.435896040868 0.01195484175489545\n",
            "1233.435896040868 0.01195484175489545\n",
            "Buying 0.0017353282171306942 amount of bitcoin with 61.67179480204341 for 35538.98 dolallars on day 1593\n",
            "1171.7641012388247 0.013690169972026146\n",
            "1171.7641012388247 0.013690169972026146\n",
            "Buying 0.0019023453907197909 amount of bitcoin with 58.58820506194124 for 30797.88 dolallars on day 1594\n",
            "1113.1758961768835 0.015592515362745937\n",
            "Selling 0.0016865085126843635 bitcoin for 55.65879480884418 dollars on day 1595\n",
            "1168.8346909857278 0.013906006850061573\n",
            "Buying 0.0018206295299988844 amount of bitcoin with 58.441734549286394 for 32099.74 dolallars on day 1596\n",
            "1110.3929564364414 0.015726636380060457\n",
            "Selling 0.0017201079108680427 bitcoin for 55.519647821822076 dollars on day 1597\n",
            "Selling 0.0017914076729902211 bitcoin for 58.295630212913174 dollars on day 1599\n",
            "1224.2082344711766 0.012215120796202194\n",
            "Buying 0.0020122314883528657 amount of bitcoin with 61.210411723558835 for 30419.17 dolallars on day 1600\n",
            "1162.9978227476179 0.014227352284555059\n",
            "Selling 0.0017408494803750933 bitcoin for 58.1498911373809 dollars on day 1601\n",
            "Selling 0.001779358951475274 bitcoin for 61.05738569424994 dollars on day 1602\n",
            "1282.2050995792486 0.010707143852704692\n",
            "Buying 0.0019347345787378142 amount of bitcoin with 64.11025497896243 for 33136.46 dolallars on day 1604\n",
            "1218.094844600286 0.012641878431442505\n",
            "Selling 0.001816810068043466 bitcoin for 60.90474223001431 dollars on day 1605\n",
            "Selling 0.0017999040616069788 bitcoin for 63.94997934151502 dollars on day 1606\n",
            "Selling 0.0017822229735865638 bitcoin for 67.14747830859078 dollars on day 1607\n",
            "1410.097044480406 0.0072429413282054985\n",
            "Buying 0.0019054289156104512 amount of bitcoin with 70.50485222402031 for 37002.09 dolallars on day 1608\n",
            "1339.5921922563857 0.00914837024381595\n",
            "Selling 0.0017497921061610986 bitcoin for 66.97960961281929 dollars on day 1609\n",
            "Selling 0.0017884730333512595 bitcoin for 70.32859009346025 dollars on day 1610\n",
            "1476.9003919626653 0.005610105104303591\n",
            "Buying 0.0018969592556054178 amount of bitcoin with 73.84501959813326 for 38928.1 dolallars on day 1611\n",
            "1403.055372364532 0.007507064359909009\n",
            "Selling 0.0015130772732086237 bitcoin for 70.1527686182266 dollars on day 1612\n",
            "Selling 0.0015810489609294168 bitcoin for 73.66040704913794 dollars on day 1613\n",
            "1546.8685480318968 0.004412938125770968\n",
            "Buying 0.001723408672893633 amount of bitcoin with 77.34342740159485 for 44878.17 dolallars on day 1614\n",
            "1469.525120630302 0.0061363467986646015\n",
            "Selling 0.0015303287548494836 bitcoin for 73.47625603151509 dollars on day 1615\n",
            "1543.001376661817 0.004606018043815118\n",
            "Buying 0.0016251905111939159 amount of bitcoin with 77.15006883309086 for 47471.4 dolallars on day 1616\n",
            "1465.851307828726 0.006231208555009034\n",
            "1465.851307828726 0.006231208555009034\n",
            "Buying 0.0015532959683204899 amount of bitcoin with 73.2925653914363 for 47185.19 dolallars on day 1617\n",
            "1392.5587424372898 0.007784504523329523\n",
            "Selling 0.0014291339971733484 bitcoin for 69.62793712186449 dollars on day 1618\n",
            "1462.1866795591543 0.006355370526156175\n",
            "Buying 0.0015246405295720128 amount of bitcoin with 73.10933397795772 for 47951.85 dolallars on day 1619\n",
            "1389.0773455811966 0.007880011055728189\n",
            "Selling 0.0014128097233134156 bitcoin for 69.45386727905984 dollars on day 1620\n",
            "Selling 0.001399252442821117 bitcoin for 72.92656064301282 dollars on day 1621\n",
            "1531.4577735032692 0.005067948889593657\n",
            "Buying 0.0014837363609267813 amount of bitcoin with 76.57288867516347 for 51608.15 dolallars on day 1622\n",
            "1454.8848848281057 0.0065516852505204375\n",
            "Selling 0.0013009441621239755 bitcoin for 72.74424424140528 dollars on day 1623\n",
            "Selling 0.001328653674940684 bitcoin for 76.38145645347555 dollars on day 1625\n",
            "1604.0105855229865 0.003922087413455778\n",
            "Buying 0.0014818087791260218 amount of bitcoin with 80.20052927614933 for 54123.4 dolallars on day 1626\n",
            "1523.8100562468371 0.0054038961925817995\n",
            "1523.8100562468371 0.0054038961925817995\n",
            "Buying 0.0015587117955456174 amount of bitcoin with 76.19050281234186 for 48880.43 dolallars on day 1627\n",
            "1447.6195534344952 0.006962607988127417\n",
            "Selling 0.0014297522258188819 bitcoin for 72.38097767172476 dollars on day 1628\n",
            "1520.00053110622 0.005532855762308535\n",
            "Buying 0.0016239176177331532 amount of bitcoin with 76.00002655531101 for 46800.42 dolallars on day 1629\n",
            "1444.000504550909 0.007156773380041687\n",
            "1444.000504550909 0.007156773380041687\n",
            "Buying 0.0015580393231626084 amount of bitcoin with 72.20002522754545 for 46340.31 dolallars on day 1630\n",
            "1371.8004793233636 0.008714812703204296\n",
            "1371.8004793233636 0.008714812703204296\n",
            "Buying 0.0014860520225524548 amount of bitcoin with 68.59002396616818 for 46155.87 dolallars on day 1631\n",
            "1303.2104553571953 0.01020086472575675\n",
            "1303.2104553571953 0.01020086472575675\n",
            "Buying 0.00144435515175493 amount of bitcoin with 65.16052276785977 for 45113.92 dolallars on day 1632\n",
            "1238.0499325893356 0.01164521987751168\n",
            "Selling 0.0012475706431958203 bitcoin for 61.90249662946678 dollars on day 1633\n",
            "1299.9524292188025 0.01039764923431586\n",
            "Buying 0.001344146904108362 amount of bitcoin with 64.99762146094012 for 48356.04 dolallars on day 1634\n",
            "1234.9548077578625 0.011741796138424222\n",
            "Selling 0.0012232677080749148 bitcoin for 61.747740387893124 dollars on day 1635\n",
            "1296.7025481457556 0.010518528430349307\n",
            "Buying 0.0013382164306129443 amount of bitcoin with 64.83512740728779 for 48448.91 dolallars on day 1636\n",
            "1231.867420738468 0.011856744860962252\n",
            "Selling 0.0012605737094802357 bitcoin for 61.5933710369234 dollars on day 1637\n",
            "Selling 0.0012638932725571885 bitcoin for 64.67303958876957 dollars on day 1639\n",
            "Selling 0.0012984237382813134 bitcoin for 67.90669156820805 dollars on day 1640\n",
            "Selling 0.0012991991866591606 bitcoin for 71.30202614661845 dollars on day 1641\n",
            "Selling 0.0013369791229664281 bitcoin for 74.86712745394937 dollars on day 1642\n",
            "Selling 0.0013608905862933112 bitcoin for 78.61048382664683 dollars on day 1643\n",
            "1650.8201603595835 0.004036785244724615\n",
            "Buying 0.0014416817345308283 amount of bitcoin with 82.54100801797918 for 57253.28 dolallars on day 1644\n",
            "1568.2791523416042 0.005478466979255444\n",
            "Selling 0.0012800454338031527 bitcoin for 78.41395761708021 dollars on day 1645\n",
            "1646.6931099586845 0.004198421545452291\n",
            "Buying 0.001392352850220598 amount of bitcoin with 82.33465549793424 for 59133.47 dolallars on day 1646\n",
            "1564.3584544607504 0.005590774395672889\n",
            "1564.3584544607504 0.005590774395672889\n",
            "Buying 0.0014028932926761631 amount of bitcoin with 78.21792272303753 for 55754.72 dolallars on day 1647\n",
            "1486.140531737713 0.0069936676883490525\n",
            "Selling 0.0013065573585435612 bitcoin for 74.30702658688566 dollars on day 1648\n",
            "Selling 0.00132436606379288 bitcoin for 78.02237791622994 dollars on day 1649\n",
            "1638.4699362408287 0.004362744266012611\n",
            "Buying 0.00142065756039603 amount of bitcoin with 81.92349681204144 for 57665.9 dolallars on day 1650\n",
            "1556.5464394287874 0.005783401826408642\n",
            "Selling 0.0013401151607391013 bitcoin for 77.82732197143937 dollars on day 1651\n",
            "1634.3737614002268 0.004443286665669541\n",
            "Buying 0.0014233935324782852 amount of bitcoin with 81.71868807001135 for 57411.17 dolallars on day 1653\n",
            "1552.6550733302156 0.005866680198147825\n",
            "1552.6550733302156 0.005866680198147825\n",
            "Buying 0.0014322075630442451 amount of bitcoin with 77.63275366651078 for 54204.96 dolallars on day 1654\n",
            "1475.0223196637048 0.007298887761192071\n",
            "Selling 0.0013537913842382747 bitcoin for 73.75111598318524 dollars on day 1655\n",
            "1548.77343564689 0.005945096376953796\n",
            "Buying 0.0014747911285972598 amount of bitcoin with 77.4386717823445 for 52508.23 dolallars on day 1656\n",
            "1471.3347638645455 0.007419887505551056\n",
            "1471.3347638645455 0.007419887505551056\n",
            "Buying 0.0014308163345755028 amount of bitcoin with 73.56673819322728 for 51415.92 dolallars on day 1657\n",
            "1397.7680256713181 0.008850703840126558\n",
            "Selling 0.0012689800062273121 bitcoin for 69.88840128356591 dollars on day 1658\n",
            "Selling 0.0013135993358817434 bitcoin for 73.3828213477442 dollars on day 1659\n",
            "Selling 0.0013370653787517597 bitcoin for 77.05196241513141 dollars on day 1661\n",
            "Selling 0.0013775648127441228 bitcoin for 80.90456053588798 dollars on day 1662\n",
            "Selling 0.0014390630018553266 bitcoin for 84.94978856268239 dollars on day 1665\n",
            "1783.94555981633 0.0021144313046662934\n",
            "Buying 0.0015627674019691208 amount of bitcoin with 89.1972779908165 for 57076.49 dolallars on day 1666\n",
            "1694.7482818255135 0.0036771987066354143\n",
            "Selling 0.0014558054736327041 bitcoin for 84.73741409127568 dollars on day 1667\n",
            "Selling 0.0015066571973129633 bitcoin for 88.97428479583947 dollars on day 1668\n",
            "1868.4599807126285 0.000714736035689747\n",
            "Buying 0.001610173360149703 amount of bitcoin with 93.42299903563143 for 58020.46 dolallars on day 1669\n",
            "1775.0369816769971 0.0023249093958394498\n",
            "1775.0369816769971 0.0023249093958394498\n",
            "Buying 0.0015863481646888199 amount of bitcoin with 88.75184908384986 for 55947.27 dolallars on day 1670\n",
            "1686.2851325931472 0.003911257560528269\n",
            "Selling 0.0014524772544803823 bitcoin for 84.31425662965736 dollars on day 1671\n",
            "Selling 0.0014810782189771513 bitcoin for 88.52996946114024 dollars on day 1673\n",
            "1859.1293586839447 0.0009777020870707356\n",
            "Buying 0.0015535534696766 amount of bitcoin with 92.95646793419724 for 59834.74 dolallars on day 1675\n",
            "1766.1728907497475 0.0025312555567473354\n",
            "Selling 0.0013894960688425133 bitcoin for 88.30864453748738 dollars on day 1676\n",
            "1854.481535287235 0.001141759487904822\n",
            "Buying 0.0014725325169600869 amount of bitcoin with 92.72407676436175 for 62969.12 dolallars on day 1677\n",
            "1761.757458522873 0.0026142920048649087\n",
            "Selling 0.0013926357358760206 bitcoin for 88.08787292614366 dollars on day 1678\n",
            "1849.8453314490168 0.001221656268988888\n",
            "Buying 0.0015050165431004573 amount of bitcoin with 92.49226657245084 for 61455.98 dolallars on day 1679\n",
            "1757.353064876566 0.0027266728120893453\n",
            "1757.353064876566 0.0027266728120893453\n",
            "Buying 0.0014623383033498263 amount of bitcoin with 87.8676532438283 for 60087.09 dolallars on day 1680\n",
            "1669.4854116327376 0.0041890111154391715\n",
            "1669.4854116327376 0.0041890111154391715\n",
            "Buying 0.0014839479882420316 amount of bitcoin with 83.47427058163689 for 56251.48 dolallars on day 1681\n",
            "1586.0111410511008 0.005672959103681203\n",
            "1586.0111410511008 0.005672959103681203\n",
            "Buying 0.0014236281303451664 amount of bitcoin with 79.30055705255505 for 55703.14 dolallars on day 1682\n",
            "1506.7105839985459 0.00709658723402637\n",
            "Selling 0.0013331855522514863 bitcoin for 75.3355291999273 dollars on day 1683\n",
            "1582.0461131984732 0.0057634016817748835\n",
            "Buying 0.0014700626228409419 amount of bitcoin with 79.10230565992367 for 53808.8 dolallars on day 1684\n",
            "1502.9438075385497 0.007233464304615826\n",
            "1502.9438075385497 0.007233464304615826\n",
            "Buying 0.0014526330248299832 amount of bitcoin with 75.14719037692748 for 51731.71 dolallars on day 1685\n",
            "1427.7966171616222 0.00868609732944581\n",
            "1427.7966171616222 0.00868609732944581\n",
            "Buying 0.001395610216971691 amount of bitcoin with 71.38983085808111 for 51153.13 dolallars on day 1686\n",
            "1356.4067863035411 0.0100817075464175\n",
            "1356.4067863035411 0.0100817075464175\n",
            "Buying 0.0013534149272653286 amount of bitcoin with 67.82033931517707 for 50110.53 dolallars on day 1687\n",
            "1288.586446988364 0.011435122473682828\n",
            "1288.586446988364 0.011435122473682828\n",
            "Buying 0.0013128591113832622 amount of bitcoin with 64.4293223494182 for 49075.58 dolallars on day 1688\n",
            "1224.1571246389458 0.01274798158506609\n",
            "Selling 0.0011322911714813813 bitcoin for 61.20785623194729 dollars on day 1689\n",
            "Selling 0.0011669973711164487 bitcoin for 64.26824904354466 dollars on day 1690\n",
            "1349.6332299144378 0.01044869304246826\n",
            "Buying 0.0012295302554969818 amount of bitcoin with 67.4816614957219 for 54884.1 dolallars on day 1691\n",
            "1282.151568418716 0.011678223297965242\n",
            "1282.151568418716 0.011678223297965242\n",
            "Buying 0.0011963906942805997 amount of bitcoin with 64.1075784209358 for 53584.15 dolallars on day 1692\n",
            "1218.04398999778 0.012874613992245843\n",
            "Selling 0.0010537328912986435 bitcoin for 60.90219949988901 dollars on day 1693\n",
            "1278.9461894976691 0.011820881100947199\n",
            "Buying 0.001129602364560957 amount of bitcoin with 63.94730947488346 for 56610.46 dolallars on day 1695\n",
            "1214.9988800227857 0.012950483465508155\n",
            "Selling 0.001061814510729218 bitcoin for 60.74994400113928 dollars on day 1696\n",
            "1275.748824023925 0.011888668954778938\n",
            "Buying 0.001198072511579195 amount of bitcoin with 63.78744120119625 for 53241.72 dolallars on day 1697\n",
            "1211.9613828227286 0.013086741466358133\n",
            "Selling 0.0010543703414813545 bitcoin for 60.598069141136435 dollars on day 1698\n",
            "1272.559451963865 0.012032371124876779\n",
            "Buying 0.0011275925459592027 amount of bitcoin with 63.62797259819325 for 56428.16 dolallars on day 1699\n",
            "1208.9314793656717 0.013159963670835982\n",
            "Selling 0.0010534382980122538 bitcoin for 60.44657396828359 dollars on day 1700\n",
            "Selling 0.0010770436848580138 bitcoin for 63.46890266669777 dollars on day 1701\n",
            "1332.8469560006533 0.011029481687965715\n",
            "Buying 0.0011434713978365175 amount of bitcoin with 66.64234780003267 for 58280.73 dolallars on day 1702\n",
            "1266.2046082006207 0.012172953085802232\n",
            "1266.2046082006207 0.012172953085802232\n",
            "Buying 0.0011328966584059881 amount of bitcoin with 63.31023041003104 for 55883.5 dolallars on day 1703\n",
            "1202.8943777905897 0.013305849744208221\n",
            "Selling 0.0010598188350577882 bitcoin for 60.144718889529486 dollars on day 1704\n",
            "1263.0390966801192 0.012246030909150434\n",
            "Buying 0.0012886289480564133 amount of bitcoin with 63.15195483400596 for 49007.09 dolallars on day 1705\n",
            "1199.8871418461133 0.013534659857206846\n",
            "Selling 0.0012070747893869972 bitcoin for 59.99435709230567 dollars on day 1706\n",
            "Selling 0.0012618368413077094 bitcoin for 62.994074946920946 dollars on day 1707\n",
            "1322.87557388534 0.01106574822651214\n",
            "Buying 0.001415246444953118 amount of bitcoin with 66.143778694267 for 46736.58 dolallars on day 1708\n",
            "1256.731795191073 0.012480994671465257\n",
            "1256.731795191073 0.012480994671465257\n",
            "Buying 0.0013530226270983034 amount of bitcoin with 62.83658975955365 for 46441.64 dolallars on day 1709\n",
            "1193.8952054315193 0.01383401729856356\n",
            "1193.8952054315193 0.01383401729856356\n",
            "Buying 0.0013692639611025165 amount of bitcoin with 59.69476027157597 for 43596.24 dolallars on day 1710\n",
            "1134.2004451599435 0.015203281259666076\n",
            "1134.2004451599435 0.015203281259666076\n",
            "Buying 0.0013215364272482289 amount of bitcoin with 56.710022257997174 for 42912.19 dolallars on day 1711\n",
            "1077.4904229019462 0.016524817686914304\n",
            "1077.4904229019462 0.016524817686914304\n",
            "Buying 0.0014574755877796942 amount of bitcoin with 53.87452114509731 for 36964.27 dolallars on day 1712\n",
            "1023.6159017568489 0.017982293274693997\n",
            "Selling 0.0012549135326479011 bitcoin for 51.18079508784245 dollars on day 1713\n",
            "1074.7966968446913 0.016727379742046095\n",
            "Buying 0.0014415056415037564 amount of bitcoin with 53.739834842234565 for 37280.35 dolallars on day 1714\n",
            "1021.0568620024567 0.01816888538354985\n",
            "Selling 0.0013603825139993774 bitcoin for 51.052843100122836 dollars on day 1715\n",
            "1072.1097051025795 0.016808502869550473\n",
            "Buying 0.0015424023812465645 amount of bitcoin with 53.605485255128976 for 34754.54 dolallars on day 1716\n",
            "1018.5042198474505 0.018350905250797037\n",
            "Selling 0.0013149255109048 bitcoin for 50.92521099237253 dollars on day 1717\n",
            "1069.429430839823 0.017035979739892237\n",
            "Buying 0.001392105584202006 amount of bitcoin with 53.47147154199115 for 38410.5 dolallars on day 1718\n",
            "1015.9579592978318 0.018428085324094245\n",
            "Selling 0.0012936852803310849 bitcoin for 50.7978979648916 dollars on day 1719\n",
            "1066.7558572627233 0.01713440004376316\n",
            "Buying 0.0013873687222319346 amount of bitcoin with 53.33779286313617 for 38445.29 dolallars on day 1720\n",
            "1013.4180643995871 0.018521768765995095\n",
            "1013.4180643995871 0.018521768765995095\n",
            "Buying 0.0014197658372372514 amount of bitcoin with 50.67090321997936 for 35689.62 dolallars on day 1721\n",
            "962.7471611796078 0.019941534603232346\n",
            "962.7471611796078 0.019941534603232346\n",
            "Buying 0.0013893389673527946 amount of bitcoin with 48.137358058980396 for 34647.67 dolallars on day 1722\n",
            "914.6098031206275 0.021330873570585142\n",
            "Selling 0.001281519282021494 bitcoin for 45.73049015603138 dollars on day 1723\n",
            "Selling 0.001286955768097512 bitcoin for 48.017014663832946 dollars on day 1724\n",
            "1008.3573079404918 0.018762398520466135\n",
            "Buying 0.0013751837128211332 amount of bitcoin with 50.41786539702459 for 36662.64 dolallars on day 1725\n",
            "957.9394425434672 0.02013758223328727\n",
            "Selling 0.0012743558941534859 bitcoin for 47.89697212717336 dollars on day 1726\n",
            "Selling 0.001283328150707439 bitcoin for 50.291820733532035 dollars on day 1727\n",
            "1056.1282354041725 0.017579898188426344\n",
            "Buying 0.0014316302464086473 amount of bitcoin with 52.806411770208626 for 36885.51 dolallars on day 1728\n",
            "1003.321823633964 0.01901152843483499\n",
            "1003.321823633964 0.01901152843483499\n",
            "Buying 0.00141192104282865 amount of bitcoin with 50.1660911816982 for 35530.38 dolallars on day 1729\n",
            "953.1557324522657 0.02042344947766364\n",
            "Selling 0.0013306220799882648 bitcoin for 47.65778662261329 dollars on day 1730\n",
            "1000.813519074879 0.019092827397675375\n",
            "Buying 0.001493088767873602 amount of bitcoin with 50.04067595374395 for 33514.87 dolallars on day 1731\n",
            "950.772843121135 0.020585916165548977\n",
            "Selling 0.0012731850610486575 bitcoin for 47.53864215605675 dollars on day 1733\n",
            "998.3114852771918 0.019312731104500318\n",
            "Buying 0.0013599280488467674 amount of bitcoin with 49.91557426385959 for 36704.57 dolallars on day 1734\n",
            "948.3959110133321 0.020672659153347085\n",
            "Selling 0.001270859132099344 bitcoin for 47.419795550666606 dollars on day 1735\n",
            "995.8157065639987 0.01940180002124774\n",
            "Buying 0.0014027588562920289 amount of bitcoin with 49.79078532819994 for 35494.9 dolallars on day 1736\n",
            "946.0249212357987 0.020804558877539767\n",
            "Selling 0.001210777996821598 bitcoin for 47.30124606178994 dollars on day 1737\n",
            "Selling 0.001225547882210331 bitcoin for 49.666308364879434 dollars on day 1738\n",
            "1042.992475662468 0.01836823299850784\n",
            "Buying 0.0012976235969421 amount of bitcoin with 52.1496237831234 for 40188.56 dolallars on day 1739\n",
            "990.8428518793446 0.019665856595449942\n",
            "990.8428518793446 0.019665856595449942\n",
            "Buying 0.0012926891231194583 amount of bitcoin with 49.542142593967235 for 38324.87 dolallars on day 1740\n",
            "941.3007092853774 0.0209585457185694\n",
            "941.3007092853774 0.0209585457185694\n",
            "Buying 0.0012363398657842346 amount of bitcoin with 47.065035464268874 for 38068.04 dolallars on day 1741\n",
            "894.2356738211085 0.022194885584353636\n",
            "894.2356738211085 0.022194885584353636\n",
            "Buying 0.0012513856406512944 amount of bitcoin with 44.71178369105543 for 35729.82 dolallars on day 1742\n",
            "849.5238901300531 0.02344627122500493\n",
            "849.5238901300531 0.02344627122500493\n",
            "Buying 0.0011956984359241232 amount of bitcoin with 42.47619450650266 for 35524.17 dolallars on day 1743\n",
            "807.0476956235505 0.02464196966092905\n",
            "807.0476956235505 0.02464196966092905\n",
            "Buying 0.0012734862199001635 amount of bitcoin with 40.35238478117753 for 31686.55 dolallars on day 1745\n",
            "766.6953108423729 0.025915455880829216\n",
            "Selling 0.0011814364500450927 bitcoin for 38.334765542118646 dollars on day 1746\n",
            "Selling 0.0011953054260748165 bitcoin for 40.25150381922458 dollars on day 1747\n",
            "Selling 0.0012201164977602317 bitcoin for 42.26407901018581 dollars on day 1748\n",
            "887.545659213902 0.022318597506949076\n",
            "Buying 0.0014025432833625396 amount of bitcoin with 44.377282960695105 for 31640.58 dolallars on day 1749\n",
            "843.1683762532069 0.023721140790311616\n",
            "Selling 0.001310859015266059 bitcoin for 42.15841881266035 dollars on day 1750\n",
            "Selling 0.0012777325012604723 bitcoin for 44.266339753293366 dollars on day 1751\n",
            "929.5931348191606 0.021132549273785085\n",
            "Buying 0.0013489306059163012 amount of bitcoin with 46.479656740958035 for 34456.67 dolallars on day 1752\n",
            "883.1134780782025 0.022481479879701387\n",
            "Selling 0.0012317575159329645 bitcoin for 44.15567390391013 dollars on day 1753\n",
            "927.2691519821127 0.021249722363768424\n",
            "Buying 0.001322880171262704 amount of bitcoin with 46.36345759910564 for 35047.36 dolallars on day 1754\n",
            "880.905694383007 0.022572602535031128\n",
            "880.905694383007 0.022572602535031128\n",
            "Buying 0.0013133387697111464 amount of bitcoin with 44.04528471915035 for 33536.88 dolallars on day 1755\n",
            "836.8604096638567 0.023885941304742274\n",
            "Selling 0.0012358801283755445 bitcoin for 41.84302048319284 dollars on day 1756\n",
            "Selling 0.0012665454996760492 bitcoin for 43.93517150735248 dollars on day 1757\n",
            "Selling 0.0013065093355778816 bitcoin for 46.1319300827201 dollars on day 1758\n",
            "968.7705317371222 0.0200770063411128\n",
            "Buying 0.0014353019333268374 amount of bitcoin with 48.43852658685611 for 33747.97 dolallars on day 1759\n",
            "920.332005150266 0.021512308274439635\n",
            "Selling 0.0013450816055273814 bitcoin for 46.0166002575133 dollars on day 1760\n",
            "966.3486054077794 0.020167226668912255\n",
            "Buying 0.0014278605501334839 amount of bitcoin with 48.31743027038897 for 33839.04 dolallars on day 1761\n",
            "918.0311751373904 0.02159508721904574\n",
            "918.0311751373904 0.02159508721904574\n",
            "Buying 0.0013961427848747671 amount of bitcoin with 45.90155875686952 for 32877.41 dolallars on day 1762\n",
            "872.1296163805209 0.022991230003920504\n",
            "Selling 0.0012894260546891482 bitcoin for 43.606480819026046 dollars on day 1763\n",
            "915.736097199547 0.021701803949231355\n",
            "Buying 0.0013661353472424116 amount of bitcoin with 45.78680485997735 for 33515.57 dolallars on day 1764\n",
            "869.9492923395696 0.023067939296473768\n",
            "Selling 0.0012708286232114888 bitcoin for 43.49746461697848 dollars on day 1765\n",
            "913.4467569565481 0.02179711067326228\n",
            "Buying 0.0013774049549607535 amount of bitcoin with 45.67233784782741 for 33158.25 dolallars on day 1766\n",
            "867.7744191087207 0.023174515628223036\n",
            "867.7744191087207 0.023174515628223036\n",
            "Buying 0.0013274177813583333 amount of bitcoin with 43.38872095543604 for 32686.56 dolallars on day 1767\n",
            "824.3856981532847 0.02450193340958137\n",
            "Selling 0.001256126003254777 bitcoin for 41.21928490766424 dollars on day 1768\n",
            "865.6049830609489 0.02324580740632659\n",
            "Buying 0.0013636475077515242 amount of bitcoin with 43.28024915304745 for 31738.59 dolallars on day 1769\n",
            "822.3247339079015 0.024609454914078115\n",
            "822.3247339079015 0.024609454914078115\n",
            "Buying 0.0013085487272274361 amount of bitcoin with 41.11623669539508 for 31421.25 dolallars on day 1770\n",
            "781.2084972125065 0.025918003641305552\n",
            "Selling 0.0012289532980999042 bitcoin for 39.060424860625325 dollars on day 1772\n",
            "820.2689220731318 0.02468905034320565\n",
            "Buying 0.0013309166004235662 amount of bitcoin with 41.01344610365659 for 30815.94 dolallars on day 1773\n",
            "779.2554759694751 0.026019966943629215\n",
            "779.2554759694751 0.026019966943629215\n",
            "Buying 0.0013079039913231232 amount of bitcoin with 38.96277379847376 for 29790.24 dolallars on day 1774\n",
            "740.2927021710013 0.02732787093495234\n",
            "Selling 0.0011524555066074996 bitcoin for 37.01463510855007 dollars on day 1775\n",
            "Selling 0.0012033407403386899 bitcoin for 38.86536686397757 dollars on day 1776\n",
            "Selling 0.001215207100047748 bitcoin for 40.80863520717645 dollars on day 1777\n",
            "Selling 0.0012499968484671897 bitcoin for 42.84906696753527 dollars on day 1778\n",
            "Selling 0.0012721975364457727 bitcoin for 44.991520315912034 dollars on day 1779\n",
            "Selling 0.0012659016856603152 bitcoin for 47.24109633170764 dollars on day 1780\n",
            "Selling 0.0012587731332017885 bitcoin for 49.603151148293016 dollars on day 1781\n",
            "Selling 0.0013020003661195344 bitcoin for 52.08330870570766 dollars on day 1782\n",
            "Selling 0.0012954773255174636 bitcoin for 54.68747414099304 dollars on day 1784\n",
            "1148.4369569608539 0.016112520692546338\n",
            "Buying 0.0013783759846727867 amount of bitcoin with 57.421847848042695 for 41659.06 dolallars on day 1785\n",
            "1091.0151091128112 0.017490896677219123\n",
            "1091.0151091128112 0.017490896677219123\n",
            "Buying 0.0013637532032291769 amount of bitcoin with 54.55075545564056 for 40000.46 dolallars on day 1786\n",
            "1036.4643536571707 0.0188546498804483\n",
            "1036.4643536571707 0.0188546498804483\n",
            "Buying 0.0013222252644888098 amount of bitcoin with 51.82321768285854 for 39193.94 dolallars on day 1787\n",
            "984.6411359743122 0.020176875144937108\n",
            "984.6411359743122 0.020176875144937108\n",
            "Buying 0.001290892464175248 amount of bitcoin with 49.23205679871561 for 38138.0 dolallars on day 1788\n",
            "935.4090791755966 0.021467767609112356\n",
            "Selling 0.0011766110498926503 bitcoin for 46.77045395877983 dollars on day 1789\n",
            "Selling 0.0012012371375353169 bitcoin for 49.10897665671882 dollars on day 1790\n",
            "Selling 0.0012040462731020508 bitcoin for 51.564425489554765 dollars on day 1791\n",
            "Selling 0.0012130324207962047 bitcoin for 54.1426467640325 dollars on day 1792\n",
            "1136.9955820446824 0.01667284072778613\n",
            "Buying 0.0012974620562704548 amount of bitcoin with 56.84977910223412 for 43816.14 dolallars on day 1793\n",
            "1080.1458029424482 0.017970302784056588\n",
            "Selling 0.0011656217806121625 bitcoin for 54.00729014712241 dollars on day 1794\n",
            "1134.1530930895706 0.016804681003444424\n",
            "Buying 0.0012433606957336675 amount of bitcoin with 56.70765465447853 for 45608.37 dolallars on day 1795\n",
            "1077.445438435092 0.01804804169917809\n",
            "1077.445438435092 0.01804804169917809\n",
            "Buying 0.0012128537698587053 amount of bitcoin with 53.8722719217546 for 44417.78 dolallars on day 1797\n",
            "1023.5731665133374 0.019260895469036796\n",
            "Selling 0.001069922643394233 bitcoin for 51.17865832566687 dollars on day 1798\n",
            "1074.7518248390043 0.01819097282564256\n",
            "Buying 0.0011406302963617317 amount of bitcoin with 53.73759124195021 for 47112.19 dolallars on day 1799\n",
            "1021.014233597054 0.01933160312200429\n",
            "1021.014233597054 0.01933160312200429\n",
            "Buying 0.0011102192392516879 amount of bitcoin with 51.050711679852704 for 45982.55 dolallars on day 1801\n",
            "969.9635219172013 0.02044182236125598\n",
            "969.9635219172013 0.02044182236125598\n",
            "Buying 0.0010862201431279896 amount of bitcoin with 48.49817609586007 for 44648.57 dolallars on day 1802\n",
            "921.4653458213412 0.02152804250438397\n",
            "Selling 0.0010289296382423605 bitcoin for 46.07326729106706 dollars on day 1803\n",
            "Selling 0.001035140536103735 bitcoin for 48.37693065562041 dollars on day 1804\n",
            "Selling 0.0010297606760576235 bitcoin for 50.79577718840144 dollars on day 1805\n",
            "1066.7113209564302 0.01843421165398025\n",
            "Buying 0.0010899931383952985 amount of bitcoin with 53.335566047821516 for 48932.02 dolallars on day 1806\n",
            "1013.3757549086087 0.01952420479237555\n",
            "Selling 0.001027021168967985 bitcoin for 50.668787745430436 dollars on day 1807\n",
            "Selling 0.001074282454444899 bitcoin for 53.20222713270196 dollars on day 1808\n",
            "1117.246769786741 0.017422901168962668\n",
            "Buying 0.0011700247125294022 amount of bitcoin with 55.86233848933705 for 47744.58 dolallars on day 1809\n",
            "1061.3844312974038 0.01859292588149207\n",
            "Selling 0.0010836625834198662 bitcoin for 53.06922156487019 dollars on day 1810\n",
            "1114.4536528622739 0.017509263298072203\n",
            "Buying 0.0011865281167884728 amount of bitcoin with 55.7226826431137 for 46962.8 dolallars on day 1811\n",
            "1058.73097021916 0.018695791414860677\n",
            "Selling 0.001079085545038105 bitcoin for 52.936548510958005 dollars on day 1812\n",
            "1111.667518730118 0.01761670586982257\n",
            "Buying 0.0011367289826097143 amount of bitcoin with 55.5833759365059 for 48897.65 dolallars on day 1813\n",
            "1056.084142793612 0.018753434852432287\n",
            "1056.084142793612 0.018753434852432287\n",
            "Buying 0.001121709296501727 amount of bitcoin with 52.80420713968061 for 47074.77 dolallars on day 1815\n",
            "1003.2799356539315 0.019875144148934015\n",
            "Selling 0.0010266304396783271 bitcoin for 50.16399678269658 dollars on day 1817\n",
            "Selling 0.001067773235705144 bitcoin for 52.672196621831404 dollars on day 1818\n",
            "Selling 0.0011053350992773102 bitcoin for 55.30580645292298 dollars on day 1819\n",
            "Selling 0.0011217336527951083 bitcoin for 58.07109677556913 dollars on day 1821\n",
            "Selling 0.0011575106518990608 bitcoin for 60.97465161434759 dollars on day 1822\n",
            "1280.4676839012993 0.014396161069579066\n",
            "Buying 0.0013677530320461773 amount of bitcoin with 64.02338419506496 for 46809.17 dolallars on day 1823\n",
            "1216.4442997062345 0.015763914101625243\n",
            "1216.4442997062345 0.015763914101625243\n",
            "Buying 0.0013199729457787303 amount of bitcoin with 60.822214985311724 for 46078.38 dolallars on day 1824\n",
            "1155.6220847209227 0.017083887047403972\n",
            "Selling 0.0012461232835356386 bitcoin for 57.78110423604613 dollars on day 1825\n",
            "Cash: 1213.4031889569687\n",
            "Gold: 0.0\n",
            "Bitcoin: 0.015837763763868334\n",
            "Total Money: 1947.7795472170128\n",
            "Total Gains 947.7795472170128\n",
            "Returns: 94.77795472170129%\n"
          ]
        }
      ],
      "source": [
        "# Starting price of bitcoin\n",
        "file_path = \"/content/drive/My Drive/csv's/BCHAIN-MKPRU.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "starting_bc_price = df.iloc[0]['Value']\n",
        "# Starting price of gold\n",
        "#starting_gold_price = df.iloc[0]['USD (PM)']\n",
        "# Represents how much of each vessel we own at the beginning\n",
        "alloc = np.array([1000, 0, 0.0])\n",
        "# How much a price must change to trigger buy or sell\n",
        "threshold = 100\n",
        "# How much money we move in and out of a vessel when buying or selling\n",
        "unit = 0.05*alloc[0]\n",
        "\n",
        "prev_price = starting_bc_price\n",
        "# For every possible day to trade bitcoin\n",
        "for index, row in df.iterrows():\n",
        "    # Get the day's price\n",
        "    curr_price = df.iloc[index]['Value']\n",
        "    # Calculate our unit based on how much bitcoin we have\n",
        "    unit = 0.05*alloc[0]\n",
        "    enter_cash = unit\n",
        "    # If bitcoin is in an upswing, sell\n",
        "    if(curr_price - threshold >= prev_price):\n",
        "        # Ensure we have enough bitcoin to sell\n",
        "        num_bitcoin =  enter_cash / curr_price\n",
        "        if(alloc[2]-num_bitcoin >= 0):\n",
        "            # Cash moving out of bitcoin\n",
        "            exit_cash = unit\n",
        "            print(f\"Selling {num_bitcoin} bitcoin for {exit_cash} dollars on day {index}\")\n",
        "            alloc[2] -= num_bitcoin\n",
        "            alloc[0] +=  exit_cash #* 0.98\n",
        "    # If bitcoin is in a downswing\n",
        "    elif(curr_price + threshold <= prev_price):\n",
        "        # Money needed to buy bitcoin\n",
        "        # Ensure we have enough money to buy bitcoin\n",
        "        if(alloc[0] - enter_cash > 0):\n",
        "            num_bitcoin =  enter_cash / curr_price\n",
        "            print(alloc[0], alloc[2])\n",
        "            print(f\"Buying {num_bitcoin} amount of bitcoin with {enter_cash} for {curr_price} dolallars on day {index}\")\n",
        "            alloc[2] +=  num_bitcoin\n",
        "            alloc[0] -=  enter_cash #* 1.02\n",
        "            print(alloc[0], alloc[2])\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    prev_price = curr_price\n",
        "\n",
        "print(f\"Cash: {alloc[0]}\")\n",
        "print(f\"Gold: {alloc[1]}\")\n",
        "print(f\"Bitcoin: {alloc[2]}\")\n",
        "total_money = alloc[0] + alloc[2]*df.iloc[-1]['Value'] #+ alloc[1]*df.iloc[-1]['USD (PM)']\n",
        "print(f\"Total Money: {total_money}\")\n",
        "print(f\"Total Gains {total_money-1000}\")\n",
        "print(f\"Returns: {100*((total_money -1000)/ (1000))}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LPnkphYsFR8"
      },
      "source": [
        "## LSTM for Bitcoin Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/My Drive/modelingComp/BCHAIN-MKPRU.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Calculate price changes\n",
        "df['Price_Change'] = df['Value'].diff()\n",
        "\n",
        "# Invert the price changes\n",
        "df['Inverted_Change'] = -df['Price_Change']\n",
        "\n",
        "# Generate inverted prices with a lower bound of 0\n",
        "df['Inverted_Price'] = df['Value'].iloc[0]  # Start with the initial price\n",
        "for i in range(1, len(df)):\n",
        "    # Calculate the new price, ensuring it doesn't go below zero\n",
        "    new_price = df['Inverted_Price'].iloc[i-1] + df['Inverted_Change'].iloc[i]\n",
        "    df.at[i, 'Inverted_Price'] = max(0, new_price)\n",
        "\n",
        "# Drop unnecessary columns for clarity\n",
        "df = df[['Date', 'Value', 'Inverted_Price']]\n",
        "df.head()\n",
        "\n",
        "inverted_df = df[['Date', 'Inverted_Price']]\n",
        "inverted_df.head()\n",
        "inverted_df = inverted_df.rename(columns={\"Inverted_Price\": \"Value\"})\n",
        "\n",
        "inverted_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "q4IsLjzgfdvy",
        "outputId": "0c7a6ffd-4b78-4576-ca97-c098f00fd791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/modelingComp/BCHAIN-MKPRU.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4b5389e2c712>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/modelingComp/BCHAIN-MKPRU.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Calculate price changes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Price_Change'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/modelingComp/BCHAIN-MKPRU.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: rename \"Inverted_Price\" column to \"Value\" in invert_df\n",
        "\n",
        "invert_df = invert_df.rename(columns={\"Inverted_Price\": \"Value\"})\n",
        "invert_df.head()\n",
        "#min(invert_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "7EhE-WpagXvQ",
        "outputId": "436db2b9-4457-4784-a6ca-88316fa5b184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'invert_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-788b589da203>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# prompt: rename \"Inverted_Price\" column to \"Value\" in invert_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minvert_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minvert_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Inverted_Price\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Value\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0minvert_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvert_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'invert_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: graph invert_df\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.line(inverted_df, x=inverted_df.Date, y=inverted_df.Value, labels={'date': 'Date', 'close': 'Inverted Price'})\n",
        "fig.update_traces(marker_line_width=2, opacity=0.8, marker_line_color='orange')\n",
        "fig.update_layout(title_text='Inverted Bitcoin Price Over Time', plot_bgcolor='white',\n",
        "                  font_size=15, font_color='black')\n",
        "fig.update_xaxes(showgrid=False)\n",
        "fig.update_yaxes(showgrid=False)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "tsXmU-nejqAx",
        "outputId": "c7aa5298-20b0-44a4-cfe9-1a54dfa3732f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"a8c19ae1-3bb4-4856-966a-a4e857f26afb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a8c19ae1-3bb4-4856-966a-a4e857f26afb\")) {                    Plotly.newPlot(                        \"a8c19ae1-3bb4-4856-966a-a4e857f26afb\",                        [{\"hovertemplate\":\"Date=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"orange\",\"width\":2}},\"mode\":\"lines\",\"name\":\"\",\"showlegend\":false,\"x\":[\"2016-09-11T00:00:00\",\"2016-09-12T00:00:00\",\"2016-09-13T00:00:00\",\"2016-09-14T00:00:00\",\"2016-09-15T00:00:00\",\"2016-09-16T00:00:00\",\"2016-09-17T00:00:00\",\"2016-09-18T00:00:00\",\"2016-09-19T00:00:00\",\"2016-09-20T00:00:00\",\"2016-09-21T00:00:00\",\"2016-09-22T00:00:00\",\"2016-09-23T00:00:00\",\"2016-09-24T00:00:00\",\"2016-09-25T00:00:00\",\"2016-09-26T00:00:00\",\"2016-09-27T00:00:00\",\"2016-09-28T00:00:00\",\"2016-09-29T00:00:00\",\"2016-09-30T00:00:00\",\"2016-10-01T00:00:00\",\"2016-10-02T00:00:00\",\"2016-10-03T00:00:00\",\"2016-10-04T00:00:00\",\"2016-10-05T00:00:00\",\"2016-10-06T00:00:00\",\"2016-10-07T00:00:00\",\"2016-10-08T00:00:00\",\"2016-10-09T00:00:00\",\"2016-10-10T00:00:00\",\"2016-10-11T00:00:00\",\"2016-10-12T00:00:00\",\"2016-10-13T00:00:00\",\"2016-10-14T00:00:00\",\"2016-10-15T00:00:00\",\"2016-10-16T00:00:00\",\"2016-10-17T00:00:00\",\"2016-10-18T00:00:00\",\"2016-10-19T00:00:00\",\"2016-10-20T00:00:00\",\"2016-10-21T00:00:00\",\"2016-10-22T00:00:00\",\"2016-10-23T00:00:00\",\"2016-10-24T00:00:00\",\"2016-10-25T00:00:00\",\"2016-10-26T00:00:00\",\"2016-10-27T00:00:00\",\"2016-10-28T00:00:00\",\"2016-10-29T00:00:00\",\"2016-10-30T00:00:00\",\"2016-10-31T00:00:00\",\"2016-11-01T00:00:00\",\"2016-11-02T00:00:00\",\"2016-11-03T00:00:00\",\"2016-11-04T00:00:00\",\"2016-11-05T00:00:00\",\"2016-11-06T00:00:00\",\"2016-11-07T00:00:00\",\"2016-11-08T00:00:00\",\"2016-11-09T00:00:00\",\"2016-11-10T00:00:00\",\"2016-11-11T00:00:00\",\"2016-11-12T00:00:00\",\"2016-11-13T00:00:00\",\"2016-11-14T00:00:00\",\"2016-11-15T00:00:00\",\"2016-11-16T00:00:00\",\"2016-11-17T00:00:00\",\"2016-11-18T00:00:00\",\"2016-11-19T00:00:00\",\"2016-11-20T00:00:00\",\"2016-11-21T00:00:00\",\"2016-11-22T00:00:00\",\"2016-11-23T00:00:00\",\"2016-11-24T00:00:00\",\"2016-11-25T00:00:00\",\"2016-11-26T00:00:00\",\"2016-11-27T00:00:00\",\"2016-11-28T00:00:00\",\"2016-11-29T00:00:00\",\"2016-11-30T00:00:00\",\"2016-12-01T00:00:00\",\"2016-12-02T00:00:00\",\"2016-12-03T00:00:00\",\"2016-12-04T00:00:00\",\"2016-12-05T00:00:00\",\"2016-12-06T00:00:00\",\"2016-12-07T00:00:00\",\"2016-12-08T00:00:00\",\"2016-12-09T00:00:00\",\"2016-12-10T00:00:00\",\"2016-12-11T00:00:00\",\"2016-12-12T00:00:00\",\"2016-12-13T00:00:00\",\"2016-12-14T00:00:00\",\"2016-12-15T00:00:00\",\"2016-12-16T00:00:00\",\"2016-12-17T00:00:00\",\"2016-12-18T00:00:00\",\"2016-12-19T00:00:00\",\"2016-12-20T00:00:00\",\"2016-12-21T00:00:00\",\"2016-12-22T00:00:00\",\"2016-12-23T00:00:00\",\"2016-12-24T00:00:00\",\"2016-12-25T00:00:00\",\"2016-12-26T00:00:00\",\"2016-12-27T00:00:00\",\"2016-12-28T00:00:00\",\"2016-12-29T00:00:00\",\"2016-12-30T00:00:00\",\"2016-12-31T00:00:00\",\"2017-01-01T00:00:00\",\"2017-01-02T00:00:00\",\"2017-01-03T00:00:00\",\"2017-01-04T00:00:00\",\"2017-01-05T00:00:00\",\"2017-01-06T00:00:00\",\"2017-01-07T00:00:00\",\"2017-01-08T00:00:00\",\"2017-01-09T00:00:00\",\"2017-01-10T00:00:00\",\"2017-01-11T00:00:00\",\"2017-01-12T00:00:00\",\"2017-01-13T00:00:00\",\"2017-01-14T00:00:00\",\"2017-01-15T00:00:00\",\"2017-01-16T00:00:00\",\"2017-01-17T00:00:00\",\"2017-01-18T00:00:00\",\"2017-01-19T00:00:00\",\"2017-01-20T00:00:00\",\"2017-01-21T00:00:00\",\"2017-01-22T00:00:00\",\"2017-01-23T00:00:00\",\"2017-01-24T00:00:00\",\"2017-01-25T00:00:00\",\"2017-01-26T00:00:00\",\"2017-01-27T00:00:00\",\"2017-01-28T00:00:00\",\"2017-01-29T00:00:00\",\"2017-01-30T00:00:00\",\"2017-01-31T00:00:00\",\"2017-02-01T00:00:00\",\"2017-02-02T00:00:00\",\"2017-02-03T00:00:00\",\"2017-02-04T00:00:00\",\"2017-02-05T00:00:00\",\"2017-02-06T00:00:00\",\"2017-02-07T00:00:00\",\"2017-02-08T00:00:00\",\"2017-02-09T00:00:00\",\"2017-02-10T00:00:00\",\"2017-02-11T00:00:00\",\"2017-02-12T00:00:00\",\"2017-02-13T00:00:00\",\"2017-02-14T00:00:00\",\"2017-02-15T00:00:00\",\"2017-02-16T00:00:00\",\"2017-02-17T00:00:00\",\"2017-02-18T00:00:00\",\"2017-02-19T00:00:00\",\"2017-02-20T00:00:00\",\"2017-02-21T00:00:00\",\"2017-02-22T00:00:00\",\"2017-02-23T00:00:00\",\"2017-02-24T00:00:00\",\"2017-02-25T00:00:00\",\"2017-02-26T00:00:00\",\"2017-02-27T00:00:00\",\"2017-02-28T00:00:00\",\"2017-03-01T00:00:00\",\"2017-03-02T00:00:00\",\"2017-03-03T00:00:00\",\"2017-03-04T00:00:00\",\"2017-03-05T00:00:00\",\"2017-03-06T00:00:00\",\"2017-03-07T00:00:00\",\"2017-03-08T00:00:00\",\"2017-03-09T00:00:00\",\"2017-03-10T00:00:00\",\"2017-03-11T00:00:00\",\"2017-03-12T00:00:00\",\"2017-03-13T00:00:00\",\"2017-03-14T00:00:00\",\"2017-03-15T00:00:00\",\"2017-03-16T00:00:00\",\"2017-03-17T00:00:00\",\"2017-03-18T00:00:00\",\"2017-03-19T00:00:00\",\"2017-03-20T00:00:00\",\"2017-03-21T00:00:00\",\"2017-03-22T00:00:00\",\"2017-03-23T00:00:00\",\"2017-03-24T00:00:00\",\"2017-03-25T00:00:00\",\"2017-03-26T00:00:00\",\"2017-03-27T00:00:00\",\"2017-03-28T00:00:00\",\"2017-03-29T00:00:00\",\"2017-03-30T00:00:00\",\"2017-03-31T00:00:00\",\"2017-04-01T00:00:00\",\"2017-04-02T00:00:00\",\"2017-04-03T00:00:00\",\"2017-04-04T00:00:00\",\"2017-04-05T00:00:00\",\"2017-04-06T00:00:00\",\"2017-04-07T00:00:00\",\"2017-04-08T00:00:00\",\"2017-04-09T00:00:00\",\"2017-04-10T00:00:00\",\"2017-04-11T00:00:00\",\"2017-04-12T00:00:00\",\"2017-04-13T00:00:00\",\"2017-04-14T00:00:00\",\"2017-04-15T00:00:00\",\"2017-04-16T00:00:00\",\"2017-04-17T00:00:00\",\"2017-04-18T00:00:00\",\"2017-04-19T00:00:00\",\"2017-04-20T00:00:00\",\"2017-04-21T00:00:00\",\"2017-04-22T00:00:00\",\"2017-04-23T00:00:00\",\"2017-04-24T00:00:00\",\"2017-04-25T00:00:00\",\"2017-04-26T00:00:00\",\"2017-04-27T00:00:00\",\"2017-04-28T00:00:00\",\"2017-04-29T00:00:00\",\"2017-04-30T00:00:00\",\"2017-05-01T00:00:00\",\"2017-05-02T00:00:00\",\"2017-05-03T00:00:00\",\"2017-05-04T00:00:00\",\"2017-05-05T00:00:00\",\"2017-05-06T00:00:00\",\"2017-05-07T00:00:00\",\"2017-05-08T00:00:00\",\"2017-05-09T00:00:00\",\"2017-05-10T00:00:00\",\"2017-05-11T00:00:00\",\"2017-05-12T00:00:00\",\"2017-05-13T00:00:00\",\"2017-05-14T00:00:00\",\"2017-05-15T00:00:00\",\"2017-05-16T00:00:00\",\"2017-05-17T00:00:00\",\"2017-05-18T00:00:00\",\"2017-05-19T00:00:00\",\"2017-05-20T00:00:00\",\"2017-05-21T00:00:00\",\"2017-05-22T00:00:00\",\"2017-05-23T00:00:00\",\"2017-05-24T00:00:00\",\"2017-05-25T00:00:00\",\"2017-05-26T00:00:00\",\"2017-05-27T00:00:00\",\"2017-05-28T00:00:00\",\"2017-05-29T00:00:00\",\"2017-05-30T00:00:00\",\"2017-05-31T00:00:00\",\"2017-06-01T00:00:00\",\"2017-06-02T00:00:00\",\"2017-06-03T00:00:00\",\"2017-06-04T00:00:00\",\"2017-06-05T00:00:00\",\"2017-06-06T00:00:00\",\"2017-06-07T00:00:00\",\"2017-06-08T00:00:00\",\"2017-06-09T00:00:00\",\"2017-06-10T00:00:00\",\"2017-06-11T00:00:00\",\"2017-06-12T00:00:00\",\"2017-06-13T00:00:00\",\"2017-06-14T00:00:00\",\"2017-06-15T00:00:00\",\"2017-06-16T00:00:00\",\"2017-06-17T00:00:00\",\"2017-06-18T00:00:00\",\"2017-06-19T00:00:00\",\"2017-06-20T00:00:00\",\"2017-06-21T00:00:00\",\"2017-06-22T00:00:00\",\"2017-06-23T00:00:00\",\"2017-06-24T00:00:00\",\"2017-06-25T00:00:00\",\"2017-06-26T00:00:00\",\"2017-06-27T00:00:00\",\"2017-06-28T00:00:00\",\"2017-06-29T00:00:00\",\"2017-06-30T00:00:00\",\"2017-07-01T00:00:00\",\"2017-07-02T00:00:00\",\"2017-07-03T00:00:00\",\"2017-07-04T00:00:00\",\"2017-07-05T00:00:00\",\"2017-07-06T00:00:00\",\"2017-07-07T00:00:00\",\"2017-07-08T00:00:00\",\"2017-07-09T00:00:00\",\"2017-07-10T00:00:00\",\"2017-07-11T00:00:00\",\"2017-07-12T00:00:00\",\"2017-07-13T00:00:00\",\"2017-07-14T00:00:00\",\"2017-07-15T00:00:00\",\"2017-07-16T00:00:00\",\"2017-07-17T00:00:00\",\"2017-07-18T00:00:00\",\"2017-07-19T00:00:00\",\"2017-07-20T00:00:00\",\"2017-07-21T00:00:00\",\"2017-07-22T00:00:00\",\"2017-07-23T00:00:00\",\"2017-07-24T00:00:00\",\"2017-07-25T00:00:00\",\"2017-07-26T00:00:00\",\"2017-07-27T00:00:00\",\"2017-07-28T00:00:00\",\"2017-07-29T00:00:00\",\"2017-07-30T00:00:00\",\"2017-07-31T00:00:00\",\"2017-08-01T00:00:00\",\"2017-08-02T00:00:00\",\"2017-08-03T00:00:00\",\"2017-08-04T00:00:00\",\"2017-08-05T00:00:00\",\"2017-08-06T00:00:00\",\"2017-08-07T00:00:00\",\"2017-08-08T00:00:00\",\"2017-08-09T00:00:00\",\"2017-08-10T00:00:00\",\"2017-08-11T00:00:00\",\"2017-08-12T00:00:00\",\"2017-08-13T00:00:00\",\"2017-08-14T00:00:00\",\"2017-08-15T00:00:00\",\"2017-08-16T00:00:00\",\"2017-08-17T00:00:00\",\"2017-08-18T00:00:00\",\"2017-08-19T00:00:00\",\"2017-08-20T00:00:00\",\"2017-08-21T00:00:00\",\"2017-08-22T00:00:00\",\"2017-08-23T00:00:00\",\"2017-08-24T00:00:00\",\"2017-08-25T00:00:00\",\"2017-08-26T00:00:00\",\"2017-08-27T00:00:00\",\"2017-08-28T00:00:00\",\"2017-08-29T00:00:00\",\"2017-08-30T00:00:00\",\"2017-08-31T00:00:00\",\"2017-09-01T00:00:00\",\"2017-09-02T00:00:00\",\"2017-09-03T00:00:00\",\"2017-09-04T00:00:00\",\"2017-09-05T00:00:00\",\"2017-09-06T00:00:00\",\"2017-09-07T00:00:00\",\"2017-09-08T00:00:00\",\"2017-09-09T00:00:00\",\"2017-09-10T00:00:00\",\"2017-09-11T00:00:00\",\"2017-09-12T00:00:00\",\"2017-09-13T00:00:00\",\"2017-09-14T00:00:00\",\"2017-09-15T00:00:00\",\"2017-09-16T00:00:00\",\"2017-09-17T00:00:00\",\"2017-09-18T00:00:00\",\"2017-09-19T00:00:00\",\"2017-09-20T00:00:00\",\"2017-09-21T00:00:00\",\"2017-09-22T00:00:00\",\"2017-09-23T00:00:00\",\"2017-09-24T00:00:00\",\"2017-09-25T00:00:00\",\"2017-09-26T00:00:00\",\"2017-09-27T00:00:00\",\"2017-09-28T00:00:00\",\"2017-09-29T00:00:00\",\"2017-09-30T00:00:00\",\"2017-10-01T00:00:00\",\"2017-10-02T00:00:00\",\"2017-10-03T00:00:00\",\"2017-10-04T00:00:00\",\"2017-10-05T00:00:00\",\"2017-10-06T00:00:00\",\"2017-10-07T00:00:00\",\"2017-10-08T00:00:00\",\"2017-10-09T00:00:00\",\"2017-10-10T00:00:00\",\"2017-10-11T00:00:00\",\"2017-10-12T00:00:00\",\"2017-10-13T00:00:00\",\"2017-10-14T00:00:00\",\"2017-10-15T00:00:00\",\"2017-10-16T00:00:00\",\"2017-10-17T00:00:00\",\"2017-10-18T00:00:00\",\"2017-10-19T00:00:00\",\"2017-10-20T00:00:00\",\"2017-10-21T00:00:00\",\"2017-10-22T00:00:00\",\"2017-10-23T00:00:00\",\"2017-10-24T00:00:00\",\"2017-10-25T00:00:00\",\"2017-10-26T00:00:00\",\"2017-10-27T00:00:00\",\"2017-10-28T00:00:00\",\"2017-10-29T00:00:00\",\"2017-10-30T00:00:00\",\"2017-10-31T00:00:00\",\"2017-11-01T00:00:00\",\"2017-11-02T00:00:00\",\"2017-11-03T00:00:00\",\"2017-11-04T00:00:00\",\"2017-11-05T00:00:00\",\"2017-11-06T00:00:00\",\"2017-11-07T00:00:00\",\"2017-11-08T00:00:00\",\"2017-11-09T00:00:00\",\"2017-11-10T00:00:00\",\"2017-11-11T00:00:00\",\"2017-11-12T00:00:00\",\"2017-11-13T00:00:00\",\"2017-11-14T00:00:00\",\"2017-11-15T00:00:00\",\"2017-11-16T00:00:00\",\"2017-11-17T00:00:00\",\"2017-11-18T00:00:00\",\"2017-11-19T00:00:00\",\"2017-11-20T00:00:00\",\"2017-11-21T00:00:00\",\"2017-11-22T00:00:00\",\"2017-11-23T00:00:00\",\"2017-11-24T00:00:00\",\"2017-11-25T00:00:00\",\"2017-11-26T00:00:00\",\"2017-11-27T00:00:00\",\"2017-11-28T00:00:00\",\"2017-11-29T00:00:00\",\"2017-11-30T00:00:00\",\"2017-12-01T00:00:00\",\"2017-12-02T00:00:00\",\"2017-12-03T00:00:00\",\"2017-12-04T00:00:00\",\"2017-12-05T00:00:00\",\"2017-12-06T00:00:00\",\"2017-12-07T00:00:00\",\"2017-12-08T00:00:00\",\"2017-12-09T00:00:00\",\"2017-12-10T00:00:00\",\"2017-12-11T00:00:00\",\"2017-12-12T00:00:00\",\"2017-12-13T00:00:00\",\"2017-12-14T00:00:00\",\"2017-12-15T00:00:00\",\"2017-12-16T00:00:00\",\"2017-12-17T00:00:00\",\"2017-12-18T00:00:00\",\"2017-12-19T00:00:00\",\"2017-12-20T00:00:00\",\"2017-12-21T00:00:00\",\"2017-12-22T00:00:00\",\"2017-12-23T00:00:00\",\"2017-12-24T00:00:00\",\"2017-12-25T00:00:00\",\"2017-12-26T00:00:00\",\"2017-12-27T00:00:00\",\"2017-12-28T00:00:00\",\"2017-12-29T00:00:00\",\"2017-12-30T00:00:00\",\"2017-12-31T00:00:00\",\"2018-01-01T00:00:00\",\"2018-01-02T00:00:00\",\"2018-01-03T00:00:00\",\"2018-01-04T00:00:00\",\"2018-01-05T00:00:00\",\"2018-01-06T00:00:00\",\"2018-01-07T00:00:00\",\"2018-01-08T00:00:00\",\"2018-01-09T00:00:00\",\"2018-01-10T00:00:00\",\"2018-01-11T00:00:00\",\"2018-01-12T00:00:00\",\"2018-01-13T00:00:00\",\"2018-01-14T00:00:00\",\"2018-01-15T00:00:00\",\"2018-01-16T00:00:00\",\"2018-01-17T00:00:00\",\"2018-01-18T00:00:00\",\"2018-01-19T00:00:00\",\"2018-01-20T00:00:00\",\"2018-01-21T00:00:00\",\"2018-01-22T00:00:00\",\"2018-01-23T00:00:00\",\"2018-01-24T00:00:00\",\"2018-01-25T00:00:00\",\"2018-01-26T00:00:00\",\"2018-01-27T00:00:00\",\"2018-01-28T00:00:00\",\"2018-01-29T00:00:00\",\"2018-01-30T00:00:00\",\"2018-01-31T00:00:00\",\"2018-02-01T00:00:00\",\"2018-02-02T00:00:00\",\"2018-02-03T00:00:00\",\"2018-02-04T00:00:00\",\"2018-02-05T00:00:00\",\"2018-02-06T00:00:00\",\"2018-02-07T00:00:00\",\"2018-02-08T00:00:00\",\"2018-02-09T00:00:00\",\"2018-02-10T00:00:00\",\"2018-02-11T00:00:00\",\"2018-02-12T00:00:00\",\"2018-02-13T00:00:00\",\"2018-02-14T00:00:00\",\"2018-02-15T00:00:00\",\"2018-02-16T00:00:00\",\"2018-02-17T00:00:00\",\"2018-02-18T00:00:00\",\"2018-02-19T00:00:00\",\"2018-02-20T00:00:00\",\"2018-02-21T00:00:00\",\"2018-02-22T00:00:00\",\"2018-02-23T00:00:00\",\"2018-02-24T00:00:00\",\"2018-02-25T00:00:00\",\"2018-02-26T00:00:00\",\"2018-02-27T00:00:00\",\"2018-02-28T00:00:00\",\"2018-03-01T00:00:00\",\"2018-03-02T00:00:00\",\"2018-03-03T00:00:00\",\"2018-03-04T00:00:00\",\"2018-03-05T00:00:00\",\"2018-03-06T00:00:00\",\"2018-03-07T00:00:00\",\"2018-03-08T00:00:00\",\"2018-03-09T00:00:00\",\"2018-03-10T00:00:00\",\"2018-03-11T00:00:00\",\"2018-03-12T00:00:00\",\"2018-03-13T00:00:00\",\"2018-03-14T00:00:00\",\"2018-03-15T00:00:00\",\"2018-03-16T00:00:00\",\"2018-03-17T00:00:00\",\"2018-03-18T00:00:00\",\"2018-03-19T00:00:00\",\"2018-03-20T00:00:00\",\"2018-03-21T00:00:00\",\"2018-03-22T00:00:00\",\"2018-03-23T00:00:00\",\"2018-03-24T00:00:00\",\"2018-03-25T00:00:00\",\"2018-03-26T00:00:00\",\"2018-03-27T00:00:00\",\"2018-03-28T00:00:00\",\"2018-03-29T00:00:00\",\"2018-03-30T00:00:00\",\"2018-03-31T00:00:00\",\"2018-04-01T00:00:00\",\"2018-04-02T00:00:00\",\"2018-04-03T00:00:00\",\"2018-04-04T00:00:00\",\"2018-04-05T00:00:00\",\"2018-04-06T00:00:00\",\"2018-04-07T00:00:00\",\"2018-04-08T00:00:00\",\"2018-04-09T00:00:00\",\"2018-04-10T00:00:00\",\"2018-04-11T00:00:00\",\"2018-04-12T00:00:00\",\"2018-04-13T00:00:00\",\"2018-04-14T00:00:00\",\"2018-04-15T00:00:00\",\"2018-04-16T00:00:00\",\"2018-04-17T00:00:00\",\"2018-04-18T00:00:00\",\"2018-04-19T00:00:00\",\"2018-04-20T00:00:00\",\"2018-04-21T00:00:00\",\"2018-04-22T00:00:00\",\"2018-04-23T00:00:00\",\"2018-04-24T00:00:00\",\"2018-04-25T00:00:00\",\"2018-04-26T00:00:00\",\"2018-04-27T00:00:00\",\"2018-04-28T00:00:00\",\"2018-04-29T00:00:00\",\"2018-04-30T00:00:00\",\"2018-05-01T00:00:00\",\"2018-05-02T00:00:00\",\"2018-05-03T00:00:00\",\"2018-05-04T00:00:00\",\"2018-05-05T00:00:00\",\"2018-05-06T00:00:00\",\"2018-05-07T00:00:00\",\"2018-05-08T00:00:00\",\"2018-05-09T00:00:00\",\"2018-05-10T00:00:00\",\"2018-05-11T00:00:00\",\"2018-05-12T00:00:00\",\"2018-05-13T00:00:00\",\"2018-05-14T00:00:00\",\"2018-05-15T00:00:00\",\"2018-05-16T00:00:00\",\"2018-05-17T00:00:00\",\"2018-05-18T00:00:00\",\"2018-05-19T00:00:00\",\"2018-05-20T00:00:00\",\"2018-05-21T00:00:00\",\"2018-05-22T00:00:00\",\"2018-05-23T00:00:00\",\"2018-05-24T00:00:00\",\"2018-05-25T00:00:00\",\"2018-05-26T00:00:00\",\"2018-05-27T00:00:00\",\"2018-05-28T00:00:00\",\"2018-05-29T00:00:00\",\"2018-05-30T00:00:00\",\"2018-05-31T00:00:00\",\"2018-06-01T00:00:00\",\"2018-06-02T00:00:00\",\"2018-06-03T00:00:00\",\"2018-06-04T00:00:00\",\"2018-06-05T00:00:00\",\"2018-06-06T00:00:00\",\"2018-06-07T00:00:00\",\"2018-06-08T00:00:00\",\"2018-06-09T00:00:00\",\"2018-06-10T00:00:00\",\"2018-06-11T00:00:00\",\"2018-06-12T00:00:00\",\"2018-06-13T00:00:00\",\"2018-06-14T00:00:00\",\"2018-06-15T00:00:00\",\"2018-06-16T00:00:00\",\"2018-06-17T00:00:00\",\"2018-06-18T00:00:00\",\"2018-06-19T00:00:00\",\"2018-06-20T00:00:00\",\"2018-06-21T00:00:00\",\"2018-06-22T00:00:00\",\"2018-06-23T00:00:00\",\"2018-06-24T00:00:00\",\"2018-06-25T00:00:00\",\"2018-06-26T00:00:00\",\"2018-06-27T00:00:00\",\"2018-06-28T00:00:00\",\"2018-06-29T00:00:00\",\"2018-06-30T00:00:00\",\"2018-07-01T00:00:00\",\"2018-07-02T00:00:00\",\"2018-07-03T00:00:00\",\"2018-07-04T00:00:00\",\"2018-07-05T00:00:00\",\"2018-07-06T00:00:00\",\"2018-07-07T00:00:00\",\"2018-07-08T00:00:00\",\"2018-07-09T00:00:00\",\"2018-07-10T00:00:00\",\"2018-07-11T00:00:00\",\"2018-07-12T00:00:00\",\"2018-07-13T00:00:00\",\"2018-07-14T00:00:00\",\"2018-07-15T00:00:00\",\"2018-07-16T00:00:00\",\"2018-07-17T00:00:00\",\"2018-07-18T00:00:00\",\"2018-07-19T00:00:00\",\"2018-07-20T00:00:00\",\"2018-07-21T00:00:00\",\"2018-07-22T00:00:00\",\"2018-07-23T00:00:00\",\"2018-07-24T00:00:00\",\"2018-07-25T00:00:00\",\"2018-07-26T00:00:00\",\"2018-07-27T00:00:00\",\"2018-07-28T00:00:00\",\"2018-07-29T00:00:00\",\"2018-07-30T00:00:00\",\"2018-07-31T00:00:00\",\"2018-08-01T00:00:00\",\"2018-08-02T00:00:00\",\"2018-08-03T00:00:00\",\"2018-08-04T00:00:00\",\"2018-08-05T00:00:00\",\"2018-08-06T00:00:00\",\"2018-08-07T00:00:00\",\"2018-08-08T00:00:00\",\"2018-08-09T00:00:00\",\"2018-08-10T00:00:00\",\"2018-08-11T00:00:00\",\"2018-08-12T00:00:00\",\"2018-08-13T00:00:00\",\"2018-08-14T00:00:00\",\"2018-08-15T00:00:00\",\"2018-08-16T00:00:00\",\"2018-08-17T00:00:00\",\"2018-08-18T00:00:00\",\"2018-08-19T00:00:00\",\"2018-08-20T00:00:00\",\"2018-08-21T00:00:00\",\"2018-08-22T00:00:00\",\"2018-08-23T00:00:00\",\"2018-08-24T00:00:00\",\"2018-08-25T00:00:00\",\"2018-08-26T00:00:00\",\"2018-08-27T00:00:00\",\"2018-08-28T00:00:00\",\"2018-08-29T00:00:00\",\"2018-08-30T00:00:00\",\"2018-08-31T00:00:00\",\"2018-09-01T00:00:00\",\"2018-09-02T00:00:00\",\"2018-09-03T00:00:00\",\"2018-09-04T00:00:00\",\"2018-09-05T00:00:00\",\"2018-09-06T00:00:00\",\"2018-09-07T00:00:00\",\"2018-09-08T00:00:00\",\"2018-09-09T00:00:00\",\"2018-09-10T00:00:00\",\"2018-09-11T00:00:00\",\"2018-09-12T00:00:00\",\"2018-09-13T00:00:00\",\"2018-09-14T00:00:00\",\"2018-09-15T00:00:00\",\"2018-09-16T00:00:00\",\"2018-09-17T00:00:00\",\"2018-09-18T00:00:00\",\"2018-09-19T00:00:00\",\"2018-09-20T00:00:00\",\"2018-09-21T00:00:00\",\"2018-09-22T00:00:00\",\"2018-09-23T00:00:00\",\"2018-09-24T00:00:00\",\"2018-09-25T00:00:00\",\"2018-09-26T00:00:00\",\"2018-09-27T00:00:00\",\"2018-09-28T00:00:00\",\"2018-09-29T00:00:00\",\"2018-09-30T00:00:00\",\"2018-10-01T00:00:00\",\"2018-10-02T00:00:00\",\"2018-10-03T00:00:00\",\"2018-10-04T00:00:00\",\"2018-10-05T00:00:00\",\"2018-10-06T00:00:00\",\"2018-10-07T00:00:00\",\"2018-10-08T00:00:00\",\"2018-10-09T00:00:00\",\"2018-10-10T00:00:00\",\"2018-10-11T00:00:00\",\"2018-10-12T00:00:00\",\"2018-10-13T00:00:00\",\"2018-10-14T00:00:00\",\"2018-10-15T00:00:00\",\"2018-10-16T00:00:00\",\"2018-10-17T00:00:00\",\"2018-10-18T00:00:00\",\"2018-10-19T00:00:00\",\"2018-10-20T00:00:00\",\"2018-10-21T00:00:00\",\"2018-10-22T00:00:00\",\"2018-10-23T00:00:00\",\"2018-10-24T00:00:00\",\"2018-10-25T00:00:00\",\"2018-10-26T00:00:00\",\"2018-10-27T00:00:00\",\"2018-10-28T00:00:00\",\"2018-10-29T00:00:00\",\"2018-10-30T00:00:00\",\"2018-10-31T00:00:00\",\"2018-11-01T00:00:00\",\"2018-11-02T00:00:00\",\"2018-11-03T00:00:00\",\"2018-11-04T00:00:00\",\"2018-11-05T00:00:00\",\"2018-11-06T00:00:00\",\"2018-11-07T00:00:00\",\"2018-11-08T00:00:00\",\"2018-11-09T00:00:00\",\"2018-11-10T00:00:00\",\"2018-11-11T00:00:00\",\"2018-11-12T00:00:00\",\"2018-11-13T00:00:00\",\"2018-11-14T00:00:00\",\"2018-11-15T00:00:00\",\"2018-11-16T00:00:00\",\"2018-11-17T00:00:00\",\"2018-11-18T00:00:00\",\"2018-11-19T00:00:00\",\"2018-11-20T00:00:00\",\"2018-11-21T00:00:00\",\"2018-11-22T00:00:00\",\"2018-11-23T00:00:00\",\"2018-11-24T00:00:00\",\"2018-11-25T00:00:00\",\"2018-11-26T00:00:00\",\"2018-11-27T00:00:00\",\"2018-11-28T00:00:00\",\"2018-11-29T00:00:00\",\"2018-11-30T00:00:00\",\"2018-12-01T00:00:00\",\"2018-12-02T00:00:00\",\"2018-12-03T00:00:00\",\"2018-12-04T00:00:00\",\"2018-12-05T00:00:00\",\"2018-12-06T00:00:00\",\"2018-12-07T00:00:00\",\"2018-12-08T00:00:00\",\"2018-12-09T00:00:00\",\"2018-12-10T00:00:00\",\"2018-12-11T00:00:00\",\"2018-12-12T00:00:00\",\"2018-12-13T00:00:00\",\"2018-12-14T00:00:00\",\"2018-12-15T00:00:00\",\"2018-12-16T00:00:00\",\"2018-12-17T00:00:00\",\"2018-12-18T00:00:00\",\"2018-12-19T00:00:00\",\"2018-12-20T00:00:00\",\"2018-12-21T00:00:00\",\"2018-12-22T00:00:00\",\"2018-12-23T00:00:00\",\"2018-12-24T00:00:00\",\"2018-12-25T00:00:00\",\"2018-12-26T00:00:00\",\"2018-12-27T00:00:00\",\"2018-12-28T00:00:00\",\"2018-12-29T00:00:00\",\"2018-12-30T00:00:00\",\"2018-12-31T00:00:00\",\"2019-01-01T00:00:00\",\"2019-01-02T00:00:00\",\"2019-01-03T00:00:00\",\"2019-01-04T00:00:00\",\"2019-01-05T00:00:00\",\"2019-01-06T00:00:00\",\"2019-01-07T00:00:00\",\"2019-01-08T00:00:00\",\"2019-01-09T00:00:00\",\"2019-01-10T00:00:00\",\"2019-01-11T00:00:00\",\"2019-01-12T00:00:00\",\"2019-01-13T00:00:00\",\"2019-01-14T00:00:00\",\"2019-01-15T00:00:00\",\"2019-01-16T00:00:00\",\"2019-01-17T00:00:00\",\"2019-01-18T00:00:00\",\"2019-01-19T00:00:00\",\"2019-01-20T00:00:00\",\"2019-01-21T00:00:00\",\"2019-01-22T00:00:00\",\"2019-01-23T00:00:00\",\"2019-01-24T00:00:00\",\"2019-01-25T00:00:00\",\"2019-01-26T00:00:00\",\"2019-01-27T00:00:00\",\"2019-01-28T00:00:00\",\"2019-01-29T00:00:00\",\"2019-01-30T00:00:00\",\"2019-01-31T00:00:00\",\"2019-02-01T00:00:00\",\"2019-02-02T00:00:00\",\"2019-02-03T00:00:00\",\"2019-02-04T00:00:00\",\"2019-02-05T00:00:00\",\"2019-02-06T00:00:00\",\"2019-02-07T00:00:00\",\"2019-02-08T00:00:00\",\"2019-02-09T00:00:00\",\"2019-02-10T00:00:00\",\"2019-02-11T00:00:00\",\"2019-02-12T00:00:00\",\"2019-02-13T00:00:00\",\"2019-02-14T00:00:00\",\"2019-02-15T00:00:00\",\"2019-02-16T00:00:00\",\"2019-02-17T00:00:00\",\"2019-02-18T00:00:00\",\"2019-02-19T00:00:00\",\"2019-02-20T00:00:00\",\"2019-02-21T00:00:00\",\"2019-02-22T00:00:00\",\"2019-02-23T00:00:00\",\"2019-02-24T00:00:00\",\"2019-02-25T00:00:00\",\"2019-02-26T00:00:00\",\"2019-02-27T00:00:00\",\"2019-02-28T00:00:00\",\"2019-03-01T00:00:00\",\"2019-03-02T00:00:00\",\"2019-03-03T00:00:00\",\"2019-03-04T00:00:00\",\"2019-03-05T00:00:00\",\"2019-03-06T00:00:00\",\"2019-03-07T00:00:00\",\"2019-03-08T00:00:00\",\"2019-03-09T00:00:00\",\"2019-03-10T00:00:00\",\"2019-03-11T00:00:00\",\"2019-03-12T00:00:00\",\"2019-03-13T00:00:00\",\"2019-03-14T00:00:00\",\"2019-03-15T00:00:00\",\"2019-03-16T00:00:00\",\"2019-03-17T00:00:00\",\"2019-03-18T00:00:00\",\"2019-03-19T00:00:00\",\"2019-03-20T00:00:00\",\"2019-03-21T00:00:00\",\"2019-03-22T00:00:00\",\"2019-03-23T00:00:00\",\"2019-03-24T00:00:00\",\"2019-03-25T00:00:00\",\"2019-03-26T00:00:00\",\"2019-03-27T00:00:00\",\"2019-03-28T00:00:00\",\"2019-03-29T00:00:00\",\"2019-03-30T00:00:00\",\"2019-03-31T00:00:00\",\"2019-04-01T00:00:00\",\"2019-04-02T00:00:00\",\"2019-04-03T00:00:00\",\"2019-04-04T00:00:00\",\"2019-04-05T00:00:00\",\"2019-04-06T00:00:00\",\"2019-04-07T00:00:00\",\"2019-04-08T00:00:00\",\"2019-04-09T00:00:00\",\"2019-04-10T00:00:00\",\"2019-04-11T00:00:00\",\"2019-04-12T00:00:00\",\"2019-04-13T00:00:00\",\"2019-04-14T00:00:00\",\"2019-04-15T00:00:00\",\"2019-04-16T00:00:00\",\"2019-04-17T00:00:00\",\"2019-04-18T00:00:00\",\"2019-04-19T00:00:00\",\"2019-04-20T00:00:00\",\"2019-04-21T00:00:00\",\"2019-04-22T00:00:00\",\"2019-04-23T00:00:00\",\"2019-04-24T00:00:00\",\"2019-04-25T00:00:00\",\"2019-04-26T00:00:00\",\"2019-04-27T00:00:00\",\"2019-04-28T00:00:00\",\"2019-04-29T00:00:00\",\"2019-04-30T00:00:00\",\"2019-05-01T00:00:00\",\"2019-05-02T00:00:00\",\"2019-05-03T00:00:00\",\"2019-05-04T00:00:00\",\"2019-05-05T00:00:00\",\"2019-05-06T00:00:00\",\"2019-05-07T00:00:00\",\"2019-05-08T00:00:00\",\"2019-05-09T00:00:00\",\"2019-05-10T00:00:00\",\"2019-05-11T00:00:00\",\"2019-05-12T00:00:00\",\"2019-05-13T00:00:00\",\"2019-05-14T00:00:00\",\"2019-05-15T00:00:00\",\"2019-05-16T00:00:00\",\"2019-05-17T00:00:00\",\"2019-05-18T00:00:00\",\"2019-05-19T00:00:00\",\"2019-05-20T00:00:00\",\"2019-05-21T00:00:00\",\"2019-05-22T00:00:00\",\"2019-05-23T00:00:00\",\"2019-05-24T00:00:00\",\"2019-05-25T00:00:00\",\"2019-05-26T00:00:00\",\"2019-05-27T00:00:00\",\"2019-05-28T00:00:00\",\"2019-05-29T00:00:00\",\"2019-05-30T00:00:00\",\"2019-05-31T00:00:00\",\"2019-06-01T00:00:00\",\"2019-06-02T00:00:00\",\"2019-06-03T00:00:00\",\"2019-06-04T00:00:00\",\"2019-06-05T00:00:00\",\"2019-06-06T00:00:00\",\"2019-06-07T00:00:00\",\"2019-06-08T00:00:00\",\"2019-06-09T00:00:00\",\"2019-06-10T00:00:00\",\"2019-06-11T00:00:00\",\"2019-06-12T00:00:00\",\"2019-06-13T00:00:00\",\"2019-06-14T00:00:00\",\"2019-06-15T00:00:00\",\"2019-06-16T00:00:00\",\"2019-06-17T00:00:00\",\"2019-06-18T00:00:00\",\"2019-06-19T00:00:00\",\"2019-06-20T00:00:00\",\"2019-06-21T00:00:00\",\"2019-06-22T00:00:00\",\"2019-06-23T00:00:00\",\"2019-06-24T00:00:00\",\"2019-06-25T00:00:00\",\"2019-06-26T00:00:00\",\"2019-06-27T00:00:00\",\"2019-06-28T00:00:00\",\"2019-06-29T00:00:00\",\"2019-06-30T00:00:00\",\"2019-07-01T00:00:00\",\"2019-07-02T00:00:00\",\"2019-07-03T00:00:00\",\"2019-07-04T00:00:00\",\"2019-07-05T00:00:00\",\"2019-07-06T00:00:00\",\"2019-07-07T00:00:00\",\"2019-07-08T00:00:00\",\"2019-07-09T00:00:00\",\"2019-07-10T00:00:00\",\"2019-07-11T00:00:00\",\"2019-07-12T00:00:00\",\"2019-07-13T00:00:00\",\"2019-07-14T00:00:00\",\"2019-07-15T00:00:00\",\"2019-07-16T00:00:00\",\"2019-07-17T00:00:00\",\"2019-07-18T00:00:00\",\"2019-07-19T00:00:00\",\"2019-07-20T00:00:00\",\"2019-07-21T00:00:00\",\"2019-07-22T00:00:00\",\"2019-07-23T00:00:00\",\"2019-07-24T00:00:00\",\"2019-07-25T00:00:00\",\"2019-07-26T00:00:00\",\"2019-07-27T00:00:00\",\"2019-07-28T00:00:00\",\"2019-07-29T00:00:00\",\"2019-07-30T00:00:00\",\"2019-07-31T00:00:00\",\"2019-08-01T00:00:00\",\"2019-08-02T00:00:00\",\"2019-08-03T00:00:00\",\"2019-08-04T00:00:00\",\"2019-08-05T00:00:00\",\"2019-08-06T00:00:00\",\"2019-08-07T00:00:00\",\"2019-08-08T00:00:00\",\"2019-08-09T00:00:00\",\"2019-08-10T00:00:00\",\"2019-08-11T00:00:00\",\"2019-08-12T00:00:00\",\"2019-08-13T00:00:00\",\"2019-08-14T00:00:00\",\"2019-08-15T00:00:00\",\"2019-08-16T00:00:00\",\"2019-08-17T00:00:00\",\"2019-08-18T00:00:00\",\"2019-08-19T00:00:00\",\"2019-08-20T00:00:00\",\"2019-08-21T00:00:00\",\"2019-08-22T00:00:00\",\"2019-08-23T00:00:00\",\"2019-08-24T00:00:00\",\"2019-08-25T00:00:00\",\"2019-08-26T00:00:00\",\"2019-08-27T00:00:00\",\"2019-08-28T00:00:00\",\"2019-08-29T00:00:00\",\"2019-08-30T00:00:00\",\"2019-08-31T00:00:00\",\"2019-09-01T00:00:00\",\"2019-09-02T00:00:00\",\"2019-09-03T00:00:00\",\"2019-09-04T00:00:00\",\"2019-09-05T00:00:00\",\"2019-09-06T00:00:00\",\"2019-09-07T00:00:00\",\"2019-09-08T00:00:00\",\"2019-09-09T00:00:00\",\"2019-09-10T00:00:00\",\"2019-09-11T00:00:00\",\"2019-09-12T00:00:00\",\"2019-09-13T00:00:00\",\"2019-09-14T00:00:00\",\"2019-09-15T00:00:00\",\"2019-09-16T00:00:00\",\"2019-09-17T00:00:00\",\"2019-09-18T00:00:00\",\"2019-09-19T00:00:00\",\"2019-09-20T00:00:00\",\"2019-09-21T00:00:00\",\"2019-09-22T00:00:00\",\"2019-09-23T00:00:00\",\"2019-09-24T00:00:00\",\"2019-09-25T00:00:00\",\"2019-09-26T00:00:00\",\"2019-09-27T00:00:00\",\"2019-09-28T00:00:00\",\"2019-09-29T00:00:00\",\"2019-09-30T00:00:00\",\"2019-10-01T00:00:00\",\"2019-10-02T00:00:00\",\"2019-10-03T00:00:00\",\"2019-10-04T00:00:00\",\"2019-10-05T00:00:00\",\"2019-10-06T00:00:00\",\"2019-10-07T00:00:00\",\"2019-10-08T00:00:00\",\"2019-10-09T00:00:00\",\"2019-10-10T00:00:00\",\"2019-10-11T00:00:00\",\"2019-10-12T00:00:00\",\"2019-10-13T00:00:00\",\"2019-10-14T00:00:00\",\"2019-10-15T00:00:00\",\"2019-10-16T00:00:00\",\"2019-10-17T00:00:00\",\"2019-10-18T00:00:00\",\"2019-10-19T00:00:00\",\"2019-10-20T00:00:00\",\"2019-10-21T00:00:00\",\"2019-10-22T00:00:00\",\"2019-10-23T00:00:00\",\"2019-10-24T00:00:00\",\"2019-10-25T00:00:00\",\"2019-10-26T00:00:00\",\"2019-10-27T00:00:00\",\"2019-10-28T00:00:00\",\"2019-10-29T00:00:00\",\"2019-10-30T00:00:00\",\"2019-10-31T00:00:00\",\"2019-11-01T00:00:00\",\"2019-11-02T00:00:00\",\"2019-11-03T00:00:00\",\"2019-11-04T00:00:00\",\"2019-11-05T00:00:00\",\"2019-11-06T00:00:00\",\"2019-11-07T00:00:00\",\"2019-11-08T00:00:00\",\"2019-11-09T00:00:00\",\"2019-11-10T00:00:00\",\"2019-11-11T00:00:00\",\"2019-11-12T00:00:00\",\"2019-11-13T00:00:00\",\"2019-11-14T00:00:00\",\"2019-11-15T00:00:00\",\"2019-11-16T00:00:00\",\"2019-11-17T00:00:00\",\"2019-11-18T00:00:00\",\"2019-11-19T00:00:00\",\"2019-11-20T00:00:00\",\"2019-11-21T00:00:00\",\"2019-11-22T00:00:00\",\"2019-11-23T00:00:00\",\"2019-11-24T00:00:00\",\"2019-11-25T00:00:00\",\"2019-11-26T00:00:00\",\"2019-11-27T00:00:00\",\"2019-11-28T00:00:00\",\"2019-11-29T00:00:00\",\"2019-11-30T00:00:00\",\"2019-12-01T00:00:00\",\"2019-12-02T00:00:00\",\"2019-12-03T00:00:00\",\"2019-12-04T00:00:00\",\"2019-12-05T00:00:00\",\"2019-12-06T00:00:00\",\"2019-12-07T00:00:00\",\"2019-12-08T00:00:00\",\"2019-12-09T00:00:00\",\"2019-12-10T00:00:00\",\"2019-12-11T00:00:00\",\"2019-12-12T00:00:00\",\"2019-12-13T00:00:00\",\"2019-12-14T00:00:00\",\"2019-12-15T00:00:00\",\"2019-12-16T00:00:00\",\"2019-12-17T00:00:00\",\"2019-12-18T00:00:00\",\"2019-12-19T00:00:00\",\"2019-12-20T00:00:00\",\"2019-12-21T00:00:00\",\"2019-12-22T00:00:00\",\"2019-12-23T00:00:00\",\"2019-12-24T00:00:00\",\"2019-12-25T00:00:00\",\"2019-12-26T00:00:00\",\"2019-12-27T00:00:00\",\"2019-12-28T00:00:00\",\"2019-12-29T00:00:00\",\"2019-12-30T00:00:00\",\"2019-12-31T00:00:00\",\"2020-01-01T00:00:00\",\"2020-01-02T00:00:00\",\"2020-01-03T00:00:00\",\"2020-01-04T00:00:00\",\"2020-01-05T00:00:00\",\"2020-01-06T00:00:00\",\"2020-01-07T00:00:00\",\"2020-01-08T00:00:00\",\"2020-01-09T00:00:00\",\"2020-01-10T00:00:00\",\"2020-01-11T00:00:00\",\"2020-01-12T00:00:00\",\"2020-01-13T00:00:00\",\"2020-01-14T00:00:00\",\"2020-01-15T00:00:00\",\"2020-01-16T00:00:00\",\"2020-01-17T00:00:00\",\"2020-01-18T00:00:00\",\"2020-01-19T00:00:00\",\"2020-01-20T00:00:00\",\"2020-01-21T00:00:00\",\"2020-01-22T00:00:00\",\"2020-01-23T00:00:00\",\"2020-01-24T00:00:00\",\"2020-01-25T00:00:00\",\"2020-01-26T00:00:00\",\"2020-01-27T00:00:00\",\"2020-01-28T00:00:00\",\"2020-01-29T00:00:00\",\"2020-01-30T00:00:00\",\"2020-01-31T00:00:00\",\"2020-02-01T00:00:00\",\"2020-02-02T00:00:00\",\"2020-02-03T00:00:00\",\"2020-02-04T00:00:00\",\"2020-02-05T00:00:00\",\"2020-02-06T00:00:00\",\"2020-02-07T00:00:00\",\"2020-02-08T00:00:00\",\"2020-02-09T00:00:00\",\"2020-02-10T00:00:00\",\"2020-02-11T00:00:00\",\"2020-02-12T00:00:00\",\"2020-02-13T00:00:00\",\"2020-02-14T00:00:00\",\"2020-02-15T00:00:00\",\"2020-02-16T00:00:00\",\"2020-02-17T00:00:00\",\"2020-02-18T00:00:00\",\"2020-02-19T00:00:00\",\"2020-02-20T00:00:00\",\"2020-02-21T00:00:00\",\"2020-02-22T00:00:00\",\"2020-02-23T00:00:00\",\"2020-02-24T00:00:00\",\"2020-02-25T00:00:00\",\"2020-02-26T00:00:00\",\"2020-02-27T00:00:00\",\"2020-02-28T00:00:00\",\"2020-02-29T00:00:00\",\"2020-03-01T00:00:00\",\"2020-03-02T00:00:00\",\"2020-03-03T00:00:00\",\"2020-03-04T00:00:00\",\"2020-03-05T00:00:00\",\"2020-03-06T00:00:00\",\"2020-03-07T00:00:00\",\"2020-03-08T00:00:00\",\"2020-03-09T00:00:00\",\"2020-03-10T00:00:00\",\"2020-03-11T00:00:00\",\"2020-03-12T00:00:00\",\"2020-03-13T00:00:00\",\"2020-03-14T00:00:00\",\"2020-03-15T00:00:00\",\"2020-03-16T00:00:00\",\"2020-03-17T00:00:00\",\"2020-03-18T00:00:00\",\"2020-03-19T00:00:00\",\"2020-03-20T00:00:00\",\"2020-03-21T00:00:00\",\"2020-03-22T00:00:00\",\"2020-03-23T00:00:00\",\"2020-03-24T00:00:00\",\"2020-03-25T00:00:00\",\"2020-03-26T00:00:00\",\"2020-03-27T00:00:00\",\"2020-03-28T00:00:00\",\"2020-03-29T00:00:00\",\"2020-03-30T00:00:00\",\"2020-03-31T00:00:00\",\"2020-04-01T00:00:00\",\"2020-04-02T00:00:00\",\"2020-04-03T00:00:00\",\"2020-04-04T00:00:00\",\"2020-04-05T00:00:00\",\"2020-04-06T00:00:00\",\"2020-04-07T00:00:00\",\"2020-04-08T00:00:00\",\"2020-04-09T00:00:00\",\"2020-04-10T00:00:00\",\"2020-04-11T00:00:00\",\"2020-04-12T00:00:00\",\"2020-04-13T00:00:00\",\"2020-04-14T00:00:00\",\"2020-04-15T00:00:00\",\"2020-04-16T00:00:00\",\"2020-04-17T00:00:00\",\"2020-04-18T00:00:00\",\"2020-04-19T00:00:00\",\"2020-04-20T00:00:00\",\"2020-04-21T00:00:00\",\"2020-04-22T00:00:00\",\"2020-04-23T00:00:00\",\"2020-04-24T00:00:00\",\"2020-04-25T00:00:00\",\"2020-04-26T00:00:00\",\"2020-04-27T00:00:00\",\"2020-04-28T00:00:00\",\"2020-04-29T00:00:00\",\"2020-04-30T00:00:00\",\"2020-05-01T00:00:00\",\"2020-05-02T00:00:00\",\"2020-05-03T00:00:00\",\"2020-05-04T00:00:00\",\"2020-05-05T00:00:00\",\"2020-05-06T00:00:00\",\"2020-05-07T00:00:00\",\"2020-05-08T00:00:00\",\"2020-05-09T00:00:00\",\"2020-05-10T00:00:00\",\"2020-05-11T00:00:00\",\"2020-05-12T00:00:00\",\"2020-05-13T00:00:00\",\"2020-05-14T00:00:00\",\"2020-05-15T00:00:00\",\"2020-05-16T00:00:00\",\"2020-05-17T00:00:00\",\"2020-05-18T00:00:00\",\"2020-05-19T00:00:00\",\"2020-05-20T00:00:00\",\"2020-05-21T00:00:00\",\"2020-05-22T00:00:00\",\"2020-05-23T00:00:00\",\"2020-05-24T00:00:00\",\"2020-05-25T00:00:00\",\"2020-05-26T00:00:00\",\"2020-05-27T00:00:00\",\"2020-05-28T00:00:00\",\"2020-05-29T00:00:00\",\"2020-05-30T00:00:00\",\"2020-05-31T00:00:00\",\"2020-06-01T00:00:00\",\"2020-06-02T00:00:00\",\"2020-06-03T00:00:00\",\"2020-06-04T00:00:00\",\"2020-06-05T00:00:00\",\"2020-06-06T00:00:00\",\"2020-06-07T00:00:00\",\"2020-06-08T00:00:00\",\"2020-06-09T00:00:00\",\"2020-06-10T00:00:00\",\"2020-06-11T00:00:00\",\"2020-06-12T00:00:00\",\"2020-06-13T00:00:00\",\"2020-06-14T00:00:00\",\"2020-06-15T00:00:00\",\"2020-06-16T00:00:00\",\"2020-06-17T00:00:00\",\"2020-06-18T00:00:00\",\"2020-06-19T00:00:00\",\"2020-06-20T00:00:00\",\"2020-06-21T00:00:00\",\"2020-06-22T00:00:00\",\"2020-06-23T00:00:00\",\"2020-06-24T00:00:00\",\"2020-06-25T00:00:00\",\"2020-06-26T00:00:00\",\"2020-06-27T00:00:00\",\"2020-06-28T00:00:00\",\"2020-06-29T00:00:00\",\"2020-06-30T00:00:00\",\"2020-07-01T00:00:00\",\"2020-07-02T00:00:00\",\"2020-07-03T00:00:00\",\"2020-07-04T00:00:00\",\"2020-07-05T00:00:00\",\"2020-07-06T00:00:00\",\"2020-07-07T00:00:00\",\"2020-07-08T00:00:00\",\"2020-07-09T00:00:00\",\"2020-07-10T00:00:00\",\"2020-07-11T00:00:00\",\"2020-07-12T00:00:00\",\"2020-07-13T00:00:00\",\"2020-07-14T00:00:00\",\"2020-07-15T00:00:00\",\"2020-07-16T00:00:00\",\"2020-07-17T00:00:00\",\"2020-07-18T00:00:00\",\"2020-07-19T00:00:00\",\"2020-07-20T00:00:00\",\"2020-07-21T00:00:00\",\"2020-07-22T00:00:00\",\"2020-07-23T00:00:00\",\"2020-07-24T00:00:00\",\"2020-07-25T00:00:00\",\"2020-07-26T00:00:00\",\"2020-07-27T00:00:00\",\"2020-07-28T00:00:00\",\"2020-07-29T00:00:00\",\"2020-07-30T00:00:00\",\"2020-07-31T00:00:00\",\"2020-08-01T00:00:00\",\"2020-08-02T00:00:00\",\"2020-08-03T00:00:00\",\"2020-08-04T00:00:00\",\"2020-08-05T00:00:00\",\"2020-08-06T00:00:00\",\"2020-08-07T00:00:00\",\"2020-08-08T00:00:00\",\"2020-08-09T00:00:00\",\"2020-08-10T00:00:00\",\"2020-08-11T00:00:00\",\"2020-08-12T00:00:00\",\"2020-08-13T00:00:00\",\"2020-08-14T00:00:00\",\"2020-08-15T00:00:00\",\"2020-08-16T00:00:00\",\"2020-08-17T00:00:00\",\"2020-08-18T00:00:00\",\"2020-08-19T00:00:00\",\"2020-08-20T00:00:00\",\"2020-08-21T00:00:00\",\"2020-08-22T00:00:00\",\"2020-08-23T00:00:00\",\"2020-08-24T00:00:00\",\"2020-08-25T00:00:00\",\"2020-08-26T00:00:00\",\"2020-08-27T00:00:00\",\"2020-08-28T00:00:00\",\"2020-08-29T00:00:00\",\"2020-08-30T00:00:00\",\"2020-08-31T00:00:00\",\"2020-09-01T00:00:00\",\"2020-09-02T00:00:00\",\"2020-09-03T00:00:00\",\"2020-09-04T00:00:00\",\"2020-09-05T00:00:00\",\"2020-09-06T00:00:00\",\"2020-09-07T00:00:00\",\"2020-09-08T00:00:00\",\"2020-09-09T00:00:00\",\"2020-09-10T00:00:00\",\"2020-09-11T00:00:00\",\"2020-09-12T00:00:00\",\"2020-09-13T00:00:00\",\"2020-09-14T00:00:00\",\"2020-09-15T00:00:00\",\"2020-09-16T00:00:00\",\"2020-09-17T00:00:00\",\"2020-09-18T00:00:00\",\"2020-09-19T00:00:00\",\"2020-09-20T00:00:00\",\"2020-09-21T00:00:00\",\"2020-09-22T00:00:00\",\"2020-09-23T00:00:00\",\"2020-09-24T00:00:00\",\"2020-09-25T00:00:00\",\"2020-09-26T00:00:00\",\"2020-09-27T00:00:00\",\"2020-09-28T00:00:00\",\"2020-09-29T00:00:00\",\"2020-09-30T00:00:00\",\"2020-10-01T00:00:00\",\"2020-10-02T00:00:00\",\"2020-10-03T00:00:00\",\"2020-10-04T00:00:00\",\"2020-10-05T00:00:00\",\"2020-10-06T00:00:00\",\"2020-10-07T00:00:00\",\"2020-10-08T00:00:00\",\"2020-10-09T00:00:00\",\"2020-10-10T00:00:00\",\"2020-10-11T00:00:00\",\"2020-10-12T00:00:00\",\"2020-10-13T00:00:00\",\"2020-10-14T00:00:00\",\"2020-10-15T00:00:00\",\"2020-10-16T00:00:00\",\"2020-10-17T00:00:00\",\"2020-10-18T00:00:00\",\"2020-10-19T00:00:00\",\"2020-10-20T00:00:00\",\"2020-10-21T00:00:00\",\"2020-10-22T00:00:00\",\"2020-10-23T00:00:00\",\"2020-10-24T00:00:00\",\"2020-10-25T00:00:00\",\"2020-10-26T00:00:00\",\"2020-10-27T00:00:00\",\"2020-10-28T00:00:00\",\"2020-10-29T00:00:00\",\"2020-10-30T00:00:00\",\"2020-10-31T00:00:00\",\"2020-11-01T00:00:00\",\"2020-11-02T00:00:00\",\"2020-11-03T00:00:00\",\"2020-11-04T00:00:00\",\"2020-11-05T00:00:00\",\"2020-11-06T00:00:00\",\"2020-11-07T00:00:00\",\"2020-11-08T00:00:00\",\"2020-11-09T00:00:00\",\"2020-11-10T00:00:00\",\"2020-11-11T00:00:00\",\"2020-11-12T00:00:00\",\"2020-11-13T00:00:00\",\"2020-11-14T00:00:00\",\"2020-11-15T00:00:00\",\"2020-11-16T00:00:00\",\"2020-11-17T00:00:00\",\"2020-11-18T00:00:00\",\"2020-11-19T00:00:00\",\"2020-11-20T00:00:00\",\"2020-11-21T00:00:00\",\"2020-11-22T00:00:00\",\"2020-11-23T00:00:00\",\"2020-11-24T00:00:00\",\"2020-11-25T00:00:00\",\"2020-11-26T00:00:00\",\"2020-11-27T00:00:00\",\"2020-11-28T00:00:00\",\"2020-11-29T00:00:00\",\"2020-11-30T00:00:00\",\"2020-12-01T00:00:00\",\"2020-12-02T00:00:00\",\"2020-12-03T00:00:00\",\"2020-12-04T00:00:00\",\"2020-12-05T00:00:00\",\"2020-12-06T00:00:00\",\"2020-12-07T00:00:00\",\"2020-12-08T00:00:00\",\"2020-12-09T00:00:00\",\"2020-12-10T00:00:00\",\"2020-12-11T00:00:00\",\"2020-12-12T00:00:00\",\"2020-12-13T00:00:00\",\"2020-12-14T00:00:00\",\"2020-12-15T00:00:00\",\"2020-12-16T00:00:00\",\"2020-12-17T00:00:00\",\"2020-12-18T00:00:00\",\"2020-12-19T00:00:00\",\"2020-12-20T00:00:00\",\"2020-12-21T00:00:00\",\"2020-12-22T00:00:00\",\"2020-12-23T00:00:00\",\"2020-12-24T00:00:00\",\"2020-12-25T00:00:00\",\"2020-12-26T00:00:00\",\"2020-12-27T00:00:00\",\"2020-12-28T00:00:00\",\"2020-12-29T00:00:00\",\"2020-12-30T00:00:00\",\"2020-12-31T00:00:00\",\"2021-01-01T00:00:00\",\"2021-01-02T00:00:00\",\"2021-01-03T00:00:00\",\"2021-01-04T00:00:00\",\"2021-01-05T00:00:00\",\"2021-01-06T00:00:00\",\"2021-01-07T00:00:00\",\"2021-01-08T00:00:00\",\"2021-01-09T00:00:00\",\"2021-01-10T00:00:00\",\"2021-01-11T00:00:00\",\"2021-01-12T00:00:00\",\"2021-01-13T00:00:00\",\"2021-01-14T00:00:00\",\"2021-01-15T00:00:00\",\"2021-01-16T00:00:00\",\"2021-01-17T00:00:00\",\"2021-01-18T00:00:00\",\"2021-01-19T00:00:00\",\"2021-01-20T00:00:00\",\"2021-01-21T00:00:00\",\"2021-01-22T00:00:00\",\"2021-01-23T00:00:00\",\"2021-01-24T00:00:00\",\"2021-01-25T00:00:00\",\"2021-01-26T00:00:00\",\"2021-01-27T00:00:00\",\"2021-01-28T00:00:00\",\"2021-01-29T00:00:00\",\"2021-01-30T00:00:00\",\"2021-01-31T00:00:00\",\"2021-02-01T00:00:00\",\"2021-02-02T00:00:00\",\"2021-02-03T00:00:00\",\"2021-02-04T00:00:00\",\"2021-02-05T00:00:00\",\"2021-02-06T00:00:00\",\"2021-02-07T00:00:00\",\"2021-02-08T00:00:00\",\"2021-02-09T00:00:00\",\"2021-02-10T00:00:00\",\"2021-02-11T00:00:00\",\"2021-02-12T00:00:00\",\"2021-02-13T00:00:00\",\"2021-02-14T00:00:00\",\"2021-02-15T00:00:00\",\"2021-02-16T00:00:00\",\"2021-02-17T00:00:00\",\"2021-02-18T00:00:00\",\"2021-02-19T00:00:00\",\"2021-02-20T00:00:00\",\"2021-02-21T00:00:00\",\"2021-02-22T00:00:00\",\"2021-02-23T00:00:00\",\"2021-02-24T00:00:00\",\"2021-02-25T00:00:00\",\"2021-02-26T00:00:00\",\"2021-02-27T00:00:00\",\"2021-02-28T00:00:00\",\"2021-03-01T00:00:00\",\"2021-03-02T00:00:00\",\"2021-03-03T00:00:00\",\"2021-03-04T00:00:00\",\"2021-03-05T00:00:00\",\"2021-03-06T00:00:00\",\"2021-03-07T00:00:00\",\"2021-03-08T00:00:00\",\"2021-03-09T00:00:00\",\"2021-03-10T00:00:00\",\"2021-03-11T00:00:00\",\"2021-03-12T00:00:00\",\"2021-03-13T00:00:00\",\"2021-03-14T00:00:00\",\"2021-03-15T00:00:00\",\"2021-03-16T00:00:00\",\"2021-03-17T00:00:00\",\"2021-03-18T00:00:00\",\"2021-03-19T00:00:00\",\"2021-03-20T00:00:00\",\"2021-03-21T00:00:00\",\"2021-03-22T00:00:00\",\"2021-03-23T00:00:00\",\"2021-03-24T00:00:00\",\"2021-03-25T00:00:00\",\"2021-03-26T00:00:00\",\"2021-03-27T00:00:00\",\"2021-03-28T00:00:00\",\"2021-03-29T00:00:00\",\"2021-03-30T00:00:00\",\"2021-03-31T00:00:00\",\"2021-04-01T00:00:00\",\"2021-04-02T00:00:00\",\"2021-04-03T00:00:00\",\"2021-04-04T00:00:00\",\"2021-04-05T00:00:00\",\"2021-04-06T00:00:00\",\"2021-04-07T00:00:00\",\"2021-04-08T00:00:00\",\"2021-04-09T00:00:00\",\"2021-04-10T00:00:00\",\"2021-04-11T00:00:00\",\"2021-04-12T00:00:00\",\"2021-04-13T00:00:00\",\"2021-04-14T00:00:00\",\"2021-04-15T00:00:00\",\"2021-04-16T00:00:00\",\"2021-04-17T00:00:00\",\"2021-04-18T00:00:00\",\"2021-04-19T00:00:00\",\"2021-04-20T00:00:00\",\"2021-04-21T00:00:00\",\"2021-04-22T00:00:00\",\"2021-04-23T00:00:00\",\"2021-04-24T00:00:00\",\"2021-04-25T00:00:00\",\"2021-04-26T00:00:00\",\"2021-04-27T00:00:00\",\"2021-04-28T00:00:00\",\"2021-04-29T00:00:00\",\"2021-04-30T00:00:00\",\"2021-05-01T00:00:00\",\"2021-05-02T00:00:00\",\"2021-05-03T00:00:00\",\"2021-05-04T00:00:00\",\"2021-05-05T00:00:00\",\"2021-05-06T00:00:00\",\"2021-05-07T00:00:00\",\"2021-05-08T00:00:00\",\"2021-05-09T00:00:00\",\"2021-05-10T00:00:00\",\"2021-05-11T00:00:00\",\"2021-05-12T00:00:00\",\"2021-05-13T00:00:00\",\"2021-05-14T00:00:00\",\"2021-05-15T00:00:00\",\"2021-05-16T00:00:00\",\"2021-05-17T00:00:00\",\"2021-05-18T00:00:00\",\"2021-05-19T00:00:00\",\"2021-05-20T00:00:00\",\"2021-05-21T00:00:00\",\"2021-05-22T00:00:00\",\"2021-05-23T00:00:00\",\"2021-05-24T00:00:00\",\"2021-05-25T00:00:00\",\"2021-05-26T00:00:00\",\"2021-05-27T00:00:00\",\"2021-05-28T00:00:00\",\"2021-05-29T00:00:00\",\"2021-05-30T00:00:00\",\"2021-05-31T00:00:00\",\"2021-06-01T00:00:00\",\"2021-06-02T00:00:00\",\"2021-06-03T00:00:00\",\"2021-06-04T00:00:00\",\"2021-06-05T00:00:00\",\"2021-06-06T00:00:00\",\"2021-06-07T00:00:00\",\"2021-06-08T00:00:00\",\"2021-06-09T00:00:00\",\"2021-06-10T00:00:00\",\"2021-06-11T00:00:00\",\"2021-06-12T00:00:00\",\"2021-06-13T00:00:00\",\"2021-06-14T00:00:00\",\"2021-06-15T00:00:00\",\"2021-06-16T00:00:00\",\"2021-06-17T00:00:00\",\"2021-06-18T00:00:00\",\"2021-06-19T00:00:00\",\"2021-06-20T00:00:00\",\"2021-06-21T00:00:00\",\"2021-06-22T00:00:00\",\"2021-06-23T00:00:00\",\"2021-06-24T00:00:00\",\"2021-06-25T00:00:00\",\"2021-06-26T00:00:00\",\"2021-06-27T00:00:00\",\"2021-06-28T00:00:00\",\"2021-06-29T00:00:00\",\"2021-06-30T00:00:00\",\"2021-07-01T00:00:00\",\"2021-07-02T00:00:00\",\"2021-07-03T00:00:00\",\"2021-07-04T00:00:00\",\"2021-07-05T00:00:00\",\"2021-07-06T00:00:00\",\"2021-07-07T00:00:00\",\"2021-07-08T00:00:00\",\"2021-07-09T00:00:00\",\"2021-07-10T00:00:00\",\"2021-07-11T00:00:00\",\"2021-07-12T00:00:00\",\"2021-07-13T00:00:00\",\"2021-07-14T00:00:00\",\"2021-07-15T00:00:00\",\"2021-07-16T00:00:00\",\"2021-07-17T00:00:00\",\"2021-07-18T00:00:00\",\"2021-07-19T00:00:00\",\"2021-07-20T00:00:00\",\"2021-07-21T00:00:00\",\"2021-07-22T00:00:00\",\"2021-07-23T00:00:00\",\"2021-07-24T00:00:00\",\"2021-07-25T00:00:00\",\"2021-07-26T00:00:00\",\"2021-07-27T00:00:00\",\"2021-07-28T00:00:00\",\"2021-07-29T00:00:00\",\"2021-07-30T00:00:00\",\"2021-07-31T00:00:00\",\"2021-08-01T00:00:00\",\"2021-08-02T00:00:00\",\"2021-08-03T00:00:00\",\"2021-08-04T00:00:00\",\"2021-08-05T00:00:00\",\"2021-08-06T00:00:00\",\"2021-08-07T00:00:00\",\"2021-08-08T00:00:00\",\"2021-08-09T00:00:00\",\"2021-08-10T00:00:00\",\"2021-08-11T00:00:00\",\"2021-08-12T00:00:00\",\"2021-08-13T00:00:00\",\"2021-08-14T00:00:00\",\"2021-08-15T00:00:00\",\"2021-08-16T00:00:00\",\"2021-08-17T00:00:00\",\"2021-08-18T00:00:00\",\"2021-08-19T00:00:00\",\"2021-08-20T00:00:00\",\"2021-08-21T00:00:00\",\"2021-08-22T00:00:00\",\"2021-08-23T00:00:00\",\"2021-08-24T00:00:00\",\"2021-08-25T00:00:00\",\"2021-08-26T00:00:00\",\"2021-08-27T00:00:00\",\"2021-08-28T00:00:00\",\"2021-08-29T00:00:00\",\"2021-08-30T00:00:00\",\"2021-08-31T00:00:00\",\"2021-09-01T00:00:00\",\"2021-09-02T00:00:00\",\"2021-09-03T00:00:00\",\"2021-09-04T00:00:00\",\"2021-09-05T00:00:00\",\"2021-09-06T00:00:00\",\"2021-09-07T00:00:00\",\"2021-09-08T00:00:00\",\"2021-09-09T00:00:00\",\"2021-09-10T00:00:00\"],\"xaxis\":\"x\",\"y\":[621.65,633.63,632.38,634.4799999999999,632.92,634.1899999999999,636.26,631.7199999999999,633.1099999999999,634.64,644.42,645.88,649.2199999999999,639.42,641.56,644.3199999999999,637.3399999999999,637.63,639.4499999999999,633.91,628.4799999999999,630.3199999999999,631.4499999999999,633.68,636.12,631.2199999999999,626.0899999999999,628.56,627.65,625.76,628.53,608.29,607.3399999999999,609.28,605.3599999999999,601.88,604.3299999999999,607.01,614.05,615.5799999999999,611.38,587.8199999999999,590.05,591.91,587.99,591.8499999999999,561.0799999999999,555.62,557.39,545.3,541.3,546.29,509.9699999999999,557.13,559.6099999999999,538.51,531.3,533.3399999999999,534.3299999999999,522.37,521.8,527.8499999999999,539.5899999999999,541.02,536.8399999999999,532.39,531.5699999999999,506.3399999999999,495.78,494.31999999999994,514.24,504.77,506.3299999999999,501.66999999999996,505.8499999999999,507.65999999999997,509.63,515.3399999999999,515.99,510.5899999999999,500.6099999999999,501.25,470.87,478.9699999999999,480.3299999999999,488.66999999999996,486.67999999999995,484.30999999999995,473.5799999999999,473.28,474.2199999999999,466.29999999999995,466.29999999999995,465.30999999999995,468.40999999999997,466.54999999999995,467.41999999999996,454.5999999999999,454.9,454.63,450.2099999999999,419.0899999999999,408.3299999999999,341.99,351.68999999999994,350.69999999999993,345.9699999999999,312.92999999999995,312.9599999999999,279.91999999999996,291.15,285.17999999999995,245.57999999999993,227.32999999999993,229.88,116.53999999999996,248.63,243.64999999999998,346.4699999999999,335.15999999999997,332.80999999999995,337.25,458.0799999999999,462.38,417.01,425.39,423.75,412.79999999999995,339.4599999999999,336.69999999999993,347.501125,349.6789124999999,347.65999999999997,324.696375,321.2263875,323.3299999999999,350.254375,327.34375,328.25,322.98775,327.36699999999996,331.1099999999999,278.5939249999999,263.5961249999999,259.88,230.2729999999999,212.30058699999995,209.23000000000002,219.28624999999988,193.19000000000005,190.46000000000004,267.197,244.1964999999999,245.70999999999992,242.6953749999999,243.42262499999993,242.0999999999999,230.9740119999999,208.09187499999985,208.0999999999999,186.6628619999999,190.520714,192.43000000000006,119.511571,120.07681200000002,113.28999999999996,68.43374999999992,92.69428599999992,92.93000000000006,52.548049999999876,55.73471399999994,52.409999999999854,0.0,0.0,0.0,16.066700000000083,11.802625000000035,7.5,129.60670000000005,94.53085699999997,94.91000000000008,107.84012499999994,59.50537499999996,59.319999999999936,41.62921400000005,29.600374999999985,30.0,195.82811199999992,334.7676375,319.30999999999995,237.91551200000004,168.3699570000001,174.0,248.211,345.0802857,359.07000000000005,330.2136875,249.7707499999999,245.96000000000004,246.42450000000008,249.09545000000003,252.26,200.0704290000001,187.8308750000001,207.01,145.39963699999998,153.92068599999993,153.47000000000003,96.54575,105.85016199999995,102.97000000000003,79.25512499999991,60.38296199999991,67.91000000000008,106.97628699999996,101.73994300000004,116.6400000000001,100.07258700000011,81.36512500000003,110.02999999999997,69.06991199999993,45.3136750000001,50.36999999999989,25.688775000000078,29.011886999999888,46.16000000000008,7.585311999999931,0.0,21.08987500000012,0.0,0.0,1.979037999999946,0.0,0.0,4.526288000000022,0.0,0.0,14.58507099999997,0.0,0.0,0.0,0.0,0.0,0.0,57.079987000000074,52.683500000000095,49.0,89.9680249999999,21.514936999999918,45.01999999999998,0.0,0.0,44.06978800000002,0.0,0.0,31.390288000000055,0.0,175.22942899999998,107.38628599999993,194.22548600000027,111.2755860000002,97.18628600000011,101.2723719999999,0.0,0.0,0.0,9.592015000000174,0.0,0.0,218.39283400000022,200.28369699999985,55.82239699999991,37.94084000000021,0.0,240.95493699999997,150.44491400000015,192.6300000000001,456.14975000000004,433.6701860000003,420.69000000000005,391.24074800000017,281.41973700000017,298.6300000000001,227.58674999999994,171.34198700000024,185.1500000000001,309.4651120000003,386.26371400000016,396.5999999999999,380.7268859999999,313.2808140000002,340.3299999999999,420.98862499999996,397.43865700000015,477.3699999999999,337.40457100000003,298.9001619999999,299.74000000000024,288.6622500000003,407.42878599999995,395.77,362.39106200000015,532.4598570000003,571.54,512.881429,543.846583,560.1400000000003,839.6343999999999,967.4157,987.6700000000001,578.5077500000002,633.8643000000002,633.4200000000001,216.4346370000003,91.02014300000019,73.11999999999989,146.80897099999993,337.63208299999997,334.3299999999999,251.0050000000001,116.99341700000014,113.82999999999993,152.67458299999998,32.198333000000275,35.73000000000002,204.99601699999994,104.51228300000002,108.63000000000011,0.0,0.0,42.362532999999985,0.0,100.04801600000019,117.094333,0.0,0.0,0.0,0.0,65.96367100000043,103.02199999999993,0.0,198.2856499999998,224.01571700000022,170.76768400000037,285.00371700000005,342.44571700000006,153.77571700000044,0.0,8.546716999999262,0.0,6.204983999999968,21.463316999999734,0.0,12.997599999999693,25.46544999999969,0.0,331.35253699999976,326.17001700000037,567.6417000000001,423.0198769999997,526.7200169999996,257.08151700000053,600.989834,599.4800169999999,581.7850170000002,663.6499999999996,713.1500169999999,950.46875,1592.110017,1695.3100170000002,1148.113977,1165.6792340000002,1233.2100169999999,968.3266840000001,934.1783500000001,1017.2600170000001,1274.237467,1135.353117,1134.1200170000002,969.1850170000002,1001.4326340000002,1030.740017,709.750967,718.1653500000002,747.4700169999996,551.0170500000004,524.8562670000001,517.6700170000004,686.5650169999999,572.8880170000002,593.1600170000002,535.54835,309.4591339999997,306.0800170000002,129.46001700000033,92.25424999999996,89.570017,0.0,0.0,0.0,114.78413300000011,222.27705999999944,231.32999999999993,98.35649999999987,0.0,0.0,3.255449999999655,110.36013299999922,85.84999999999945,316.81746699999985,93.30158299999948,96.61999999999989,209.74305000000004,0.0,18.94401999999991,0.0,0.0,0.0,0.0,0.0,71.55331699999988,448.4716499999995,345.4160839999995,308.90331699999933,279.506257,718.1454669999994,868.3233169999994,1721.2417339999993,887.3157839999994,943.9133169999996,136.11339700000008,0.0,0.0,51.62961700000051,0.0,0.0,0.0,0.0,31.31500000000051,17.056667000000743,0.0,0.0,0.0,0.0,33.508819999999105,0.0,0.0,4.032000000001062,0.0,0.0,0.0,0.0,0.0,0.0,1718.0458500000004,1991.0750000000007,1791.9300000000003,0.0,468.02665999999954,1024.0833299999995,0.0,0.0,218.7833299999984,536.8266599999988,1761.5716600000014,2115.743330000001,3451.1733299999996,4307.73833,5722.073329999999,5549.508330000001,5379.655000000001,5757.743329999999,3909.3616600000005,5118.10166,5284.67333,6283.109329999999,5333.108329999999,5702.68333,4492.826660000001,4445.42166,4459.44333,2324.563330000001,2179.4853299999995,2342.733329999999,4232.7766599999995,4784.43,5061.26333,6201.88933,5585.80133,5707.493329999999,5645.76333,5486.48733,5944.54333,8381.73666,8153.26,8323.86333,6547.889999999999,7993.455330000001,7985.26333,8275.61933,8216.425,8106.653329999999,8528.86833,7973.906660000001,8067.313329999999,8286.02833,9314.62166,9416.16333,10415.424997,10596.781663,10645.95333,11098.034997,12659.866663,12573.22333,11398.724997000001,11258.146663,11253.60333,11178.806763999999,11155.22833,11430.66333,10900.91583,10164.049997,10027.04333,9371.52166,8656.69166,8412.85333,8387.71833,8108.291659999999,8252.70333,9567.611663,9336.56666,9327.38333,9802.089997,9150.08,9179.22333,9128.518329999999,8489.30166,8567.563329999999,8171.735000000001,8068.50166,7981.85333,8735.485,9380.625329999999,9578.22333,10409.404997,10752.68133,10721.313329999999,10315.839997,10343.98333,10345.143329999999,11140.561663,10968.28133,11200.79333,11327.268329999999,11086.649997,10893.04333,10550.929997,10808.274997,10773.313329999999,10836.304997,10881.386663,11040.72333,11622.48833,11538.303329999999,11548.07333,12616.151662999999,12563.20333,12561.123329999999,12462.834996999998,12088.248329999999,12072.20333,12672.17333,12894.806663,12864.00333,12481.026663,12799.409997,12714.27333,12572.416663,11650.838329999999,11557.223329999999,11462.172278999999,11157.934997,11130.58333,11603.266404,11333.745904,11322.72333,10645.964997,10691.47833,10568.08333,10564.821663,9943.14133,9840.57333,10240.284997,10488.36333,10570.13333,10164.401663,10239.11333,10259.13333,10277.25733,9859.414997,9745.33333,9695.376663,9868.547053,9867.24333,10270.074997,10176.641663,10188.313329999999,11029.89533,11014.336663,11035.16333,10846.644997,10987.22533,11027.62333,11392.564997,11258.62833,11259.60333,10991.276663,11113.127143,11102.05333,11942.94333,11932.489997,11921.903330000001,12155.774997,12136.851663000001,12154.123330000002,12038.806663000003,12113.288330000003,12111.963330000002,11963.536663000003,11853.536663000003,11861.543330000002,11998.409997000002,11882.961188000003,11879.253330000003,11822.411663000003,11878.529997000001,11883.143330000003,12721.794997000003,12623.004997000004,12619.373330000002,13182.983330000003,12851.669997000003,12861.273330000004,12989.256663000004,13034.271663000003,13052.893330000003,12761.683330000003,12783.964997000003,12737.853330000002,13166.109997000001,13357.077497000002,13328.22333,13287.235830000001,13280.088330000002,13401.223330000003,13390.787176000002,13589.98083,13275.403330000001,13123.929163,13032.614163000002,12872.243330000001,12905.394163,12895.306663,12943.17333,12904.401663,12745.124163,12774.64333,12987.891663,13121.319997,13095.50333,13254.32583,13257.68333,13249.84333,12984.292496999999,12628.772496999998,12177.063329999997,12102.281662999998,12070.638329999998,12163.693329999998,12047.394162999997,11808.799162999996,11783.583329999996,11247.518329999995,11311.359162999997,11558.873329999997,11316.431662999996,11292.341662999996,11277.773329999996,11581.882496999995,11927.814162999995,11890.983329999995,12104.184162999994,12250.914162999994,12487.403329999994,12510.604162999993,12505.169996999994,12780.453329999993,13101.910829999993,13102.188662999994,13359.723329999993,13187.551662999995,13151.613329999995,13246.553329999995,13136.006406999995,13156.054098999995,13185.933329999996,13061.962496999997,13094.619996999998,13012.103329999998,13097.437175999998,12923.454162999999,13144.11333,12955.037616,12779.254099,12760.41333,12779.417175999999,12498.643329999999,12420.49333,12566.020829999998,12516.737175999999,12479.90333,12250.747945,12237.734099,12233.603329999998,12385.614098999999,13065.411662999999,12997.49333,13132.575829999998,13212.257496999999,13257.70333,13202.362497,13225.54583,13172.643329999999,12999.62083,12980.028330000001,12976.29333,13098.082497,13202.051663,13156.29333,13080.120663000002,12828.692497000002,12742.56333,12788.23833,12859.379163,12915.23333,13030.051662999998,12963.206662999997,12812.603329999998,12948.209162999998,12905.548329999998,12887.663329999998,12936.041662999998,13028.280829999998,13014.953329999998,12930.134162999999,12917.196662999999,12925.063329999999,12880.115638,12876.971663,12871.83333,13250.047497,13238.152497,13251.68333,13199.284163,13046.111663,12899.51333,12902.407176,12930.642561,12989.073330000001,13009.857497,12967.081663,12993.903330000001,13017.25733,12990.37333,13008.91333,13024.929997,13032.76583,13041.47333,13116.014997000002,13189.230473000001,13197.073330000003,13156.402497000003,13111.009163000002,13108.263330000002,13106.809997000002,13061.718330000002,13060.923330000001,12959.893330000003,13012.431663000003,13053.713330000002,13099.649997000002,13120.414997000003,13090.153330000005,13126.619997000005,13322.528330000005,13741.913330000003,13902.490830000002,13940.439997000001,13926.243330000001,14194.74083,14826.71333,15006.70333,14949.88583,15189.345829999998,15145.703329999998,15675.171662999997,15578.146662999998,15705.333329999998,15395.229483999998,15234.899996999997,15219.913329999996,15381.905829999996,15331.136662999997,15358.083329999998,15537.189996999998,15640.334162999998,15753.703329999997,16093.039996999996,16063.343329999996,16045.023329999996,15974.723329999997,16072.493329999998,16091.403329999997,16091.920829999997,16220.309162999998,16256.263329999998,16227.444996999999,16106.278329999997,15942.143329999997,15690.640829999997,15515.832496999996,15363.493329999996,15587.709996999994,15471.204996999993,15503.513329999994,15684.803329999995,15673.304162999995,15650.473329999995,15754.778329999996,15586.397496999996,15728.323329999996,15707.137496999996,15746.411662999995,15642.803329999995,15632.885829999996,15676.056662999996,15642.063329999995,15578.226662999996,15462.589996999995,15466.433329999996,15464.544996999997,15686.097496999997,15852.433329999996,15852.337496999997,15898.841662999996,15966.723329999995,15841.898329999996,15877.412496999996,15877.253329999996,15866.288329999996,15808.159996999995,15794.703329999995,15950.255829999995,15958.663329999994,15911.333329999994,15932.283329999995,15912.433329999994,15920.893329999995,15922.383329999995,15944.183329999996,16050.103329999996,16077.083329999996,16028.683329999996,16053.873329999997,16023.353329999996,15992.463329999997,16042.663329999996,16046.333329999996,16031.343329999996,16092.983329999995,16103.923329999994,15837.603329999994,15834.303329999995,15812.393329999995,15869.213329999995,15866.263329999994,15884.183329999994,15898.373329999995,15890.483329999995,15879.133329999995,15827.943329999995,15587.883329999993,15572.153329999994,15511.503329999994,15553.803329999993,15515.203329999993,15359.083329999992,15731.793329999993,15650.353329999993,15667.233329999992,15665.453329999991,15648.60332999999,15664.06332999999,15667.24332999999,15684.10332999999,15767.703329999991,15626.43332999999,15621.83332999999,15611.86332999999,15622.72332999999,15548.12332999999,15570.513329999989,15617.593329999989,15606.173329999989,15620.913329999988,15613.473329999988,15562.183329999989,15482.733329999988,15503.763329999987,15499.913329999987,15469.573329999987,15442.283329999987,15506.343329999987,15499.623329999988,15486.763329999987,15504.573329999987,15562.793329999986,15550.943329999986,15450.173329999985,15461.313329999986,15383.133329999986,15384.243329999987,15384.523329999985,15346.153329999986,14615.803329999986,14538.873329999986,14587.443329999987,14469.913329999987,14445.163329999987,14309.293329999986,14229.973329999986,14322.873329999986,14188.503329999985,14463.663329999985,14425.833329999985,14434.063329999986,14346.173329999987,14462.503329999987,14302.033329999987,14278.313329999986,14222.373329999986,14221.643329999986,14189.403329999986,14216.853329999987,14121.493329999987,13980.523329999985,14064.493329999987,14303.073329999987,14205.853329999987,14218.493329999987,14197.393329999986,14238.033329999987,14228.893329999988,14172.013329999987,14108.523329999987,13841.543329999986,13727.603329999987,13781.023329999987,13814.213329999988,13742.963329999988,13561.963329999988,13351.773329999989,13150.663329999988,12250.373329999988,12520.053329999988,11675.573329999988,11505.993329999988,11291.783329999987,11612.723329999986,12136.463329999986,12233.633329999986,11304.983329999985,11491.653329999986,11546.183329999985,11872.753329999985,11618.393329999984,11495.423329999983,11427.233329999985,10754.263329999983,10728.623329999984,10778.803329999984,10835.703329999984,11226.223329999984,10944.873329999984,10943.883329999984,10759.653329999983,11363.763329999983,11822.883329999982,11712.643329999983,11695.453329999982,11500.39332999998,11567.343329999982,11858.073329999981,11478.30332999998,11581.10332999998,11325.62332999998,11260.80332999998,10805.08332999998,10639.21332999998,10519.69332999998,10171.20332999998,10422.50332999998,10216.98332999998,9967.33332999998,9289.30332999998,8830.05332999998,8684.20332999998,8481.18332999998,7732.28332999998,6566.133329999981,8365.83332999998,7124.3333299999795,7608.303329999981,8760.95332999998,8919.96332999998,8693.28332999998,7513.92332999998,8338.19332999998,8543.52332999998,8266.643329999979,8021.67332999998,7185.60332999998,6911.903329999979,7398.78332999998,8145.813329999979,7694.7133299999805,8109.5833299999795,9311.72332999998,8625.18332999998,10100.96332999998,9824.40332999998,8860.12332999998,8962.93332999998,8733.78332999998,8911.27332999998,9175.063329999979,9654.38332999998,9726.51332999998,9623.51332999998,9650.03332999998,10024.69332999998,9968.68332999998,9997.35332999998,9909.55332999998,9413.98332999998,9091.51332999998,8969.13332999998,8684.11332999998,8518.45332999998,7710.69332999998,8033.01332999998,7537.86332999998,7502.27332999998,7642.04332999998,8216.46332999998,7931.84332999998,8112.42332999998,8640.563329999979,9481.72332999998,9196.51332999998,9139.24332999998,9284.16332999998,9181.08332999998,8581.42332999998,8738.12332999998,9369.28332999998,9386.70332999998,9092.87332999998,9354.88332999998,9363.62332999998,9138.40332999998,9326.73332999998,9780.86332999998,10014.13332999998,9920.69332999998,9897.78332999998,9728.893329999979,9112.04332999998,8877.393329999979,8914.52332999998,8920.88332999998,9181.21332999998,9011.47332999998,9092.37332999998,9185.02332999998,9397.65332999998,9339.36332999998,9078.52332999998,9134.78332999998,9137.35332999998,9188.25332999998,9233.05332999998,9308.32332999998,9341.09332999998,9222.80332999998,9325.57332999998,9519.19332999998,9465.63332999998,9815.30332999998,10945.07332999998,11066.45332999998,11443.043329999979,11304.78332999998,11273.68332999998,11441.94332999998,11190.94332999998,11175.76332999998,11116.65332999998,11262.51332999998,11343.20332999998,11350.99332999998,11628.94332999998,11286.66332999998,11308.68332999998,10910.76332999998,10911.78332999998,11228.95332999998,11190.67332999998,11214.92332999998,11145.143329999979,11336.52332999998,11496.17332999998,11421.90332999998,11544.53332999998,11538.64332999998,11267.62332999998,11276.16332999998,11471.92332999998,12029.12332999998,12066.80332999998,10831.89332999998,10238.883329999982,9947.14332999998,10279.923329999981,10065.333329999981,10334.06332999998,10350.703329999982,10245.693329999982,10197.503329999981,10292.523329999982,10080.633329999982,10188.493329999981,10155.343329999982,10294.443329999982,10732.64332999998,10689.273329999982,10461.56332999998,10780.873329999982,10697.163329999981,10736.263329999982,10866.363329999982,11040.993329999981,11015.983329999981,10994.753329999981,11322.693329999982,11377.883329999982,11416.873329999982,11881.613329999982,12212.333329999981,12174.653329999983,12591.283329999984,12368.433329999983,12335.053329999984,11974.853329999984,12067.683329999983,11741.213329999984,11940.963329999984,12095.993329999985,12189.093329999985,12201.913329999985,12305.833329999985,12102.713329999984,11951.493329999985,11993.853329999984,11976.293329999982,12161.263329999983,12277.923329999983,12296.373329999984,12309.523329999985,12246.813329999986,12430.943329999987,12387.543329999986,12619.143329999986,12886.563329999986,12214.393329999986,12347.823329999987,12308.513329999987,12355.483329999988,11984.273329999989,12176.603329999989,12247.99332999999,12305.96332999999,12304.283329999991,12254.75332999999,12197.613329999991,12113.323329999992,12279.08332999999,12330.373329999991,12323.003329999992,12554.353329999993,12172.333329999992,12150.793329999993,12147.113329999993,11739.443329999993,11333.213329999991,11456.033329999991,11680.76332999999,11314.02332999999,11477.193329999991,11324.713329999991,11393.443329999991,10656.26332999999,10684.793329999991,10776.65332999999,10598.34332999999,10587.83332999999,10795.32332999999,10872.213329999991,10776.42332999999,10839.74332999999,11110.57332999999,11070.51332999999,11171.32332999999,10910.26332999999,10602.90332999999,10112.99332999999,10218.873329999991,9996.31332999999,10164.91332999999,10120.59332999999,10184.123329999991,10336.543329999991,10336.543329999991,9883.783329999991,9743.02332999999,9691.14332999999,9591.56332999999,9336.27332999999,9643.89332999999,9223.303329999992,9144.383329999991,9256.25332999999,9130.15332999999,9594.51332999999,9561.01332999999,9794.75332999999,9318.033329999991,9893.963329999991,9891.82332999999,9802.10332999999,9829.053329999992,9509.293329999991,9834.93332999999,10189.533329999991,10713.16332999999,10693.963329999991,10786.33332999999,10964.51332999999,10585.863329999991,10585.863329999991,10744.34332999999,10739.783329999991,10431.293329999991,10342.793329999991,10600.053329999992,11459.303329999992,11566.743329999992,11613.223329999992,11562.033329999991,14668.47332999999,13889.65332999999,14332.42332999999,14150.24332999999,14472.33332999999,14141.073329999992,14088.453329999993,13303.483329999992,13272.243329999992,13308.83332999999,13676.06332999999,12996.523329999989,12730.19332999999,12800.22332999999,12734.933329999989,13129.593329999989,13237.733329999988,13613.273329999989,13093.393329999988,13070.403329999988,12845.813329999988,12689.573329999988,12757.013329999987,12626.993329999987,12721.243329999987,12155.483329999986,12293.133329999986,12133.653329999986,12204.993329999987,12625.443329999987,12607.083329999987,12583.313329999986,12641.023329999985,12626.733329999985,12874.883329999984,12386.413329999985,12463.793329999986,12239.323329999987,12368.643329999986,12658.443329999987,12646.163329999987,12367.693329999987,12021.083329999987,11991.603329999987,11949.163329999987,11799.413329999987,11708.023329999987,11733.353329999987,10721.053329999988,10869.913329999987,10674.023329999987,10524.863329999987,10594.733329999986,10612.753329999987,10467.723329999988,10327.893329999986,9496.203329999988,9676.883329999988,9971.293329999988,10744.223329999988,10881.433329999987,10684.153329999986,10192.753329999987,9708.373329999988,10195.093329999987,10112.983329999986,9829.283329999987,9779.313329999986,9712.943329999987,9988.013329999987,10441.113329999987,10331.423329999987,10320.363329999987,10767.953329999988,10599.023329999987,10655.833329999987,10301.143329999986,9929.473329999988,10072.703329999988,9800.583329999987,10047.843329999987,9294.453329999988,9973.113329999987,9840.643329999986,9704.123329999988,9874.933329999987,9828.253329999987,9744.833329999987,9716.093329999987,9723.533329999987,9606.553329999988,10212.263329999987,10038.713329999988,10025.183329999987,10168.613329999987,10071.983329999986,9972.683329999987,10043.873329999988,10118.653329999986,10198.533329999987,10141.253329999987,10213.903329999986,9806.783329999987,9877.193329999987,10222.103329999987,10257.833329999987,10344.233329999986,10494.453329999988,10371.213329999988,10313.333329999987,10364.713329999988,10262.303329999988,10410.703329999988,10426.263329999987,10367.373329999988,10409.593329999987,10149.773329999987,10242.453329999988,10058.613329999987,10260.643329999986,10211.283329999987,10262.723329999988,10202.783329999987,10260.553329999988,10244.163329999987,10305.173329999987,10367.683329999987,10344.493329999987,10323.973329999988,10284.023329999987,10334.813329999986,10106.023329999987,9961.283329999987,9885.573329999987,9947.403329999986,9791.183329999987,9559.853329999987,8456.283329999987,8563.743329999987,8396.013329999987,8383.753329999987,8154.803329999988,7674.993329999987,8420.913329999987,8256.113329999987,8304.433329999987,7748.403329999986,7725.743329999987,7893.083329999987,7731.083329999987,7814.623329999988,7605.653329999986,8106.253329999987,7925.5733299999865,7721.253329999987,7724.303329999988,7624.703329999988,7584.673329999987,7204.963329999988,7529.153329999986,7764.683329999987,7632.863329999987,7975.883329999988,7815.243329999987,7845.663329999987,7734.753329999987,8161.2833299999875,8031.313329999986,8196.673329999987,7963.933329999987,8017.043329999988,7790.903329999986,7839.113329999987,7575.433329999987,8101.243329999987,9311.173329999987,9030.793329999988,9339.063329999986,9243.753329999987,9130.943329999987,9377.163329999987,9270.853329999987,9146.023329999987,9103.243329999987,9052.243329999987,9167.913329999987,8824.043329999988,8713.063329999986,8550.253329999987,8554.793329999988,8566.893329999986,8417.253329999987,8579.033329999987,9068.223329999988,8966.463329999988,9264.203329999988,8766.253329999987,8805.843329999987,8766.283329999987,8724.443329999987,8806.353329999987,8657.883329999988,8720.763329999987,8879.443329999987,8923.623329999988,8946.913329999987,8825.223329999988,8710.123329999988,8894.943329999987,8827.883329999988,8575.383329999988,8435.493329999987,8196.013329999987,8122.0733299999865,7958.643329999986,8070.443329999987,8067.363329999987,7994.953329999988,8171.113329999987,8132.173329999987,7990.483329999986,7740.523329999987,7573.223329999988,6667.123329999988,6508.433329999987,6554.163329999987,6370.223329999988,6461.913329999987,6422.313329999986,5847.213329999988,6209.683329999987,6040.023329999987,5933.963329999988,5688.363329999987,5739.803329999988,5923.513329999987,5475.373329999988,5343.093329999987,3907.2933299999877,3902.9133299999867,4658.843329999987,4008.0833299999867,4170.153329999986,4181.643329999986,3790.0333299999875,3203.1133299999874,3159.353329999987,3407.6133299999874,3530.5233299999873,2773.5333299999857,1818.963329999986,1700.2333299999864,1678.1133299999874,811.2333299999864,798.9333299999871,1076.4033299999883,1099.7733299999873,326.1633299999867,758.8833299999878,2347.2433299999884,2359.813329999988,1766.2633299999889,1307.0833299999886,0.0,917.2099999999991,482.7599999999984,255.1899999999987,1039.239999999998,554.6699999999983,332.0699999999997,528.3199999999997,1390.8600000000006,1155.579999999998,1461.9700000000012,1680.369999999999,906.2900000000009,545.25,433.1399999999994,269.97999999999956,0.0,0.0,0.0,0.0,379.3399999999965,1124.4399999999987,44.92999999999665,616.5499999999993,154.38999999999942,0.0,0.0,196.62999999999738,0.0,0.0,0.0,0.0,0.0,0.0,0.0,965.75,0.0,0.0,0.0,0.0,429.52999999999884,2430.1600000000035,5125.309999999998,6658.43,3277.1200000000026,1511.7799999999988,3841.730000000003,4605.050000000003,4877.239999999998,4037.9000000000015,4650.120000000003,5131.269999999997,9872.369999999999,7667.870000000003,8570.509999999998,8393.41,8426.990000000002,8128.450000000001,10251.080000000002,7267.080000000002,6355.989999999998,6352.1500000000015,7533.790000000001,7147.3499999999985,5140.5899999999965,2994.0,3668.1600000000035,2391.6399999999994,1346.989999999998,1742.1500000000015,0.0,0.0,1711.4100000000035,0.0,541.9799999999959,828.189999999995,0.0,768.5200000000041,0.0,0.0,510.08000000000175,0.0,0.0,0.0,3364.459999999999,8607.43,6863.020000000004,10687.440000000002,11147.550000000003,11331.989999999998,12373.940000000002,7869.43,9131.82,7010.1600000000035,9038.949999999997,8626.480000000003,8606.270000000004,6318.1600000000035,5188.529999999999,2606.340000000004,1490.6299999999974,0.0,510.72000000000116,0.0,2125.260000000002,5504.010000000002,4386.350000000006,2345.730000000003,3592.8300000000017,3183.6300000000047,3172.9300000000003,3847.560000000005,7053.770000000004,6781.270000000004,8750.5,9842.810000000005,6184.260000000002,5394.800000000003,5475.020000000004,3631.060000000005,2528.600000000006,2523.480000000003,2521.810000000005,2227.4100000000035,4182.240000000005,3052.1800000000003,2204.6300000000047,3238.270000000004,5311.460000000006,3210.1400000000067,3156.1500000000015,1484.7300000000032,1293.8600000000006,1423.9900000000052,0.0,585.3199999999997,301.81000000000495,2098.459999999999,3467.350000000006,7302.959999999999,7851.300000000003,7046.529999999999,9745.64,11822.730000000003,12401.310000000005,13443.910000000003,14478.86,9497.800000000003,8482.980000000003,8670.340000000004,9970.29,5757.82,5696.940000000002,6943.980000000003,6341.110000000001,10312.720000000001,6081.209999999999,7126.279999999999,6174.1700000000055,4625.630000000005,5273.709999999999,7670.940000000002,6804.440000000002,14547.350000000006,13852.170000000006,13631.920000000006,16817.86,17112.800000000003,19958.200000000004,20642.25,26590.170000000006,22770.120000000003,26274.090000000004,26026.14,28799.9,24825.850000000006,25143.940000000002,24288.4,25109.15,27864.82,28906.770000000004,27869.850000000006,26243.9,26891.800000000003,25969.200000000004,24365.850000000006,26668.93,28024.060000000005,27738.270000000004,30039.57,30104.25,26216.08,26849.870000000003,26241.260000000002,28059.54,24487.620000000003,23028.64,23365.880000000005,25229.57,25486.4,27824.620000000003,28030.270000000004,27962.090000000004,31867.890000000003,31106.850000000002,29879.78,28915.060000000005,31913.86,31393.530000000002,28909.990000000005,29097.770000000004,27706.740000000005,28507.08,30017.560000000005,29697.58,28865.46,28245.14,29806.47,29343.43,29715.4,30677.03,29735.920000000006,30038.870000000003,29326.800000000003,30396.190000000002,30867.88,30739.83,31815.850000000002,32133.190000000002,32033.780000000002,31770.95,32738.500000000004,33764.2,31436.379999999997,31256.55,29972.81,29275.100000000002,28189.24,26236.3,24148.49,23551.91,23548.51,21340.289999999997,21895.38,23553.98,24360.499999999996,25416.44,23804.3,22672.44,20728.49,18920.31,19738.3,17220.98,17946.069999999996,17942.98,19136.66,15720.459999999995,16442.249999999996,16498.029999999995,17571.889999999996,18905.87,18776.579999999998,16819.789999999997,14226.689999999999,14622.420000000002,14218.759999999998,14030.939999999999,15809.859999999997,14582.350000000002,16591.639999999996,14497.579999999998,14656.789999999997,14747.66,16479.670000000002,16398.569999999996,14691.679999999997,14225.429999999997,13519.109999999997,13607.060000000001,11785.380000000001,10877.039999999997,16745.27,17476.06,17185.749999999996],\"yaxis\":\"y\",\"type\":\"scattergl\",\"opacity\":0.8}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Date\"},\"showgrid\":false},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Value\"},\"showgrid\":false},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"Inverted Bitcoin Price Over Time\"},\"font\":{\"size\":15,\"color\":\"black\"},\"plot_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a8c19ae1-3bb4-4856-966a-a4e857f26afb');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "collapsed": true,
        "id": "zTynzcM2_c62",
        "outputId": "86a178a3-bcf8-48f6-9ab9-974905adb493"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: \"/content/drive/My Drive/csv's/BCHAIN-MKPRU.csv\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1a3194d9ece9>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Load the Bitcoin data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/csv's/BCHAIN-MKPRU.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: \"/content/drive/My Drive/csv's/BCHAIN-MKPRU.csv\""
          ]
        }
      ],
      "source": [
        "from numpy import array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "alloc = [1000, 0, 0]\n",
        "buy_dates, sell_dates = [], []\n",
        "buy_prices, sell_prices = [], []\n",
        "predicted_prices = []\n",
        "\n",
        "# Load the Bitcoin data\n",
        "file_path = \"/content/drive/My Drive/csv's/BCHAIN-MKPRU.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Invert Bitcoin prices and shift up\n",
        "inverted_df = df.copy()\n",
        "inverted_df['Value'] = -inverted_df['Value']  # Make prices negative\n",
        "\n",
        "# Find the minimum value of the inverted Bitcoin prices\n",
        "min_value = inverted_df['Value'].min()\n",
        "\n",
        "# Shift prices up so that the minimum value is 100\n",
        "shift_amount = 100 - min_value\n",
        "inverted_df['Value'] += shift_amount  # Shift all prices\n",
        "\n",
        "for i, row in inverted_df.iterrows():\n",
        "    cut_off_date = pd.to_datetime('9/11/17')\n",
        "    if inverted_df.iloc[i]['Date'] < cut_off_date:\n",
        "        continue\n",
        "    else:\n",
        "        if inverted_df.iloc[i]['Date'].day == 1:\n",
        "            date = inverted_df.iloc[i]['Date']\n",
        "            bc_price = inverted_df.iloc[i]['Value']\n",
        "            unit = 0.05 * alloc[0]\n",
        "            enter_cash = unit\n",
        "\n",
        "            # Define a year ago date\n",
        "            year_ago = inverted_df.iloc[i]['Date'].replace(year=inverted_df.iloc[i]['Date'].year - 1)\n",
        "            print(year_ago)\n",
        "\n",
        "            # Create a dataset of the last year\n",
        "            temp_df = cut_df(inverted_df, year_ago, inverted_df.iloc[i]['Date'])\n",
        "            close_stock = temp_df.copy()\n",
        "\n",
        "            # Drop the Date column for scaling\n",
        "            del temp_df['Date']\n",
        "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "            temp_df = scaler.fit_transform(np.array(temp_df).reshape(-1, 1))\n",
        "\n",
        "            # Split into training and testing data\n",
        "            training_size = int(len(temp_df) * 0.60)\n",
        "            test_size = len(temp_df) - training_size\n",
        "            train_data, test_data = temp_df[0:training_size, :], temp_df[training_size:len(temp_df), :1]\n",
        "\n",
        "            # Create datasets\n",
        "            time_step = 15\n",
        "            X_train, y_train = create_dataset(train_data, time_step)\n",
        "            X_test, y_test = create_dataset(test_data, time_step)\n",
        "\n",
        "            # Reshape for LSTM\n",
        "            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "            # Define the LSTM model\n",
        "            model = Sequential()\n",
        "            model.add(LSTM(10, input_shape=(None, 1), activation=\"relu\"))\n",
        "            model.add(Dense(1))\n",
        "\n",
        "            # Compile and train the model\n",
        "            model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
        "            history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, batch_size=32, verbose=1)\n",
        "\n",
        "            # Predictions\n",
        "            train_predict = model.predict(X_train)\n",
        "            test_predict = model.predict(X_test)\n",
        "\n",
        "            # Inverse transform predictions\n",
        "            train_predict = scaler.inverse_transform(train_predict)\n",
        "            test_predict = scaler.inverse_transform(test_predict)\n",
        "            original_ytrain = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
        "            original_ytest = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "            # Plot the predictions\n",
        "            look_back = time_step\n",
        "            trainPredictPlot = np.empty_like(temp_df)\n",
        "            trainPredictPlot[:, :] = np.nan\n",
        "            trainPredictPlot[look_back:len(train_predict) + look_back, :] = train_predict\n",
        "            print(\"Train predicted data: \", trainPredictPlot.shape)\n",
        "\n",
        "            # Shift test predictions for plotting\n",
        "            testPredictPlot = np.empty_like(temp_df)\n",
        "            testPredictPlot[:, :] = np.nan\n",
        "            testPredictPlot[len(train_predict) + (look_back * 2) + 1:len(temp_df) - 1, :] = test_predict\n",
        "            print(\"Test predicted data: \", testPredictPlot.shape)\n",
        "\n",
        "            # Predict next 30 days\n",
        "            x_input = test_data[len(test_data) - time_step:].reshape(1, -1)\n",
        "            temp_input = list(x_input)\n",
        "            temp_input = temp_input[0].tolist()\n",
        "\n",
        "            lst_output = []\n",
        "            n_steps = time_step\n",
        "            i = 0\n",
        "            pred_days = 30\n",
        "            while i < pred_days:\n",
        "                if len(temp_input) > time_step:\n",
        "                    x_input = np.array(temp_input[1:])\n",
        "                    x_input = x_input.reshape(1, -1)\n",
        "                    x_input = x_input.reshape((1, n_steps, 1))\n",
        "\n",
        "                    yhat = model.predict(x_input, verbose=0)\n",
        "                    temp_input.extend(yhat[0].tolist())\n",
        "                    temp_input = temp_input[1:]\n",
        "\n",
        "                    lst_output.extend(yhat.tolist())\n",
        "                    i += 1\n",
        "                else:\n",
        "                    x_input = x_input.reshape((1, n_steps, 1))\n",
        "                    yhat = model.predict(x_input, verbose=0)\n",
        "                    temp_input.extend(yhat[0].tolist())\n",
        "\n",
        "                    lst_output.extend(yhat.tolist())\n",
        "                    i += 1\n",
        "\n",
        "            print(\"Output of predicted next days: \", len(lst_output))\n",
        "\n",
        "            # Combine predictions with original data\n",
        "            lstmdf = temp_df.tolist()\n",
        "            lstmdf.extend((np.array(lst_output).reshape(-1, 1)).tolist())\n",
        "            lstmdf = scaler.inverse_transform(lstmdf).reshape(1, -1).tolist()[0]\n",
        "\n",
        "            predict = lstmdf[(len(lstmdf) - 31):]\n",
        "            if predict[-1] > predict[0]:\n",
        "                if alloc[0] - enter_cash >= 0:\n",
        "                    print(f\"Buying {enter_cash / bc_price} bitcoins on {date}\")\n",
        "                    print(f\"Price = {bc_price}, unit = {enter_cash}\")\n",
        "                    alloc[0] -= 1.02 * enter_cash\n",
        "                    alloc[2] += enter_cash / bc_price\n",
        "                    buy_dates.append(date)\n",
        "                    buy_prices.append(bc_price)\n",
        "                    print(f\"Cash: {alloc[0]}\")\n",
        "                    print(f\"Bitcoin: {alloc[2]}\")\n",
        "            else:\n",
        "                cash = bc_price * alloc[2]\n",
        "                if alloc[2] - alloc[2] >= 0:\n",
        "                    print(f\"Selling {alloc[2]} bitcoins on {date}\")\n",
        "                    print(f\"Price = {bc_price}, unit = {enter_cash}\")\n",
        "                    print(\"SELL\")\n",
        "                    sell_dates.append(date)\n",
        "                    sell_prices.append(bc_price)\n",
        "                    alloc[0] += 0.98 * cash\n",
        "                    alloc[2] = 0\n",
        "                    print(f\"Cash: {alloc[0]}\")\n",
        "                    print(f\"Bitcoin: {alloc[2]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecmCk9ltWp-O",
        "outputId": "0bff9fcd-e3ae-4de2-ec6f-b6bc5ab37494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cash: 298.1252643464489\n",
            "Gold: 0\n",
            "Bitcoin: 0.11371333824037086\n",
            "Total Money: 5570.863794079351\n",
            "Total Gains 4570.863794079351\n",
            "Returns: 457.08637940793506%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Cash: {alloc[0]}\")\n",
        "print(f\"Gold: {alloc[1]}\")\n",
        "print(f\"Bitcoin: {alloc[2]}\")\n",
        "total_money = alloc[0] + alloc[2]*df.iloc[-1]['Value'] #+ alloc[1]*df.iloc[-1]['USD (PM)']\n",
        "print(f\"Total Money: {total_money}\")\n",
        "print(f\"Total Gains {total_money-1000}\")\n",
        "print(f\"Returns: {100*((total_money -1000)/ (1000))}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "HcX27wRMjuwi",
        "outputId": "479eabe7-ebdb-4846-8f4e-7f93f36523d2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1400x700 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot predictions and actual prices\n",
        "prediction_dates = df['Date'][-len(predicted_prices):]\n",
        "\n",
        "# Plot actual and predicted prices\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(df['Date'], df['Value'], label=\"Actual Prices\", color=\"blue\")\n",
        "#plt.plot(prediction_dates, predicted_prices, label=\"Predicted Prices\", color=\"orange\")\n",
        "\n",
        "# Mark buy and sell points\n",
        "plt.scatter(buy_dates, buy_prices, marker=\"^\", color=\"green\", label=\"Buy\", s=100)\n",
        "plt.scatter(sell_dates, sell_prices, marker=\"v\", color=\"red\", label=\"Sell\", s=100)\n",
        "\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Bitcoin Price\")\n",
        "plt.title(\"Bitcoin Price Prediction and Trading\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJHW-0MxsNzF"
      },
      "source": [
        "## LSTM for Gold Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IVhxrZopmnrt",
        "outputId": "b73fe227-4932-4a59-9f11-33f7db120a30"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1265,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1265,\n        \"samples\": [\n          \"6/24/21\",\n          \"3/16/21\",\n          \"5/15/18\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"USD (PM)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 249.29181162273784,\n        \"min\": 1125.7,\n        \"max\": 2067.15,\n        \"num_unique_values\": 1182,\n        \"samples\": [\n          1643.3,\n          1843.0,\n          1773.15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-fb3eb421-3ef7-4747-91f0-3c42b63d619f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>USD (PM)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9/12/16</td>\n",
              "      <td>1324.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9/13/16</td>\n",
              "      <td>1323.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9/14/16</td>\n",
              "      <td>1321.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9/15/16</td>\n",
              "      <td>1310.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9/16/16</td>\n",
              "      <td>1308.35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb3eb421-3ef7-4747-91f0-3c42b63d619f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fb3eb421-3ef7-4747-91f0-3c42b63d619f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fb3eb421-3ef7-4747-91f0-3c42b63d619f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e6ad4caa-fe17-48c8-bb79-a92b6add9346\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e6ad4caa-fe17-48c8-bb79-a92b6add9346')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e6ad4caa-fe17-48c8-bb79-a92b6add9346 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "      Date  USD (PM)\n",
              "0  9/12/16   1324.60\n",
              "1  9/13/16   1323.65\n",
              "2  9/14/16   1321.75\n",
              "3  9/15/16   1310.80\n",
              "4  9/16/16   1308.35"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_path = \"/content/drive/My Drive/modelingComp/LBMA-GOLD.csv\"\n",
        "df = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlhYjwojnLjO",
        "outputId": "12601543-d36d-4694-fbab-f31938502ee8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-d559ee71922d>:8: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Date'] = pd.to_datetime(df['Date'])\n",
            "<ipython-input-14-d559ee71922d>:22: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  merged_df['USD (PM)'] = merged_df['USD (PM)'].fillna(method='ffill')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1825"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame\n",
        "file_path = \"/content/drive/My Drive/modelingComp/LBMA-GOLD.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert 'date' to datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Generate a complete date range\n",
        "start_date = df['Date'].min()\n",
        "end_date = df['Date'].max()\n",
        "all_dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "# Create a new DataFrame with all dates\n",
        "all_dates_df = pd.DataFrame({'Date': all_dates})\n",
        "\n",
        "# Merge the original DataFrame with the new DataFrame\n",
        "merged_df = pd.merge(all_dates_df, df, on='Date', how='left')\n",
        "\n",
        "# Forward fill prices to propagate Friday's price to Saturday and Sunday\n",
        "merged_df['USD (PM)'] = merged_df['USD (PM)'].fillna(method='ffill')\n",
        "len(merged_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqbHjARGsNXZ",
        "outputId": "de66410c-7d0b-4a71-db20-88ce70678153"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-7285119aedb8>:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Date'] = pd.to_datetime(df['Date'])\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2016-10-01 00:00:00\n",
            "Epoch 1/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 0.2539 - val_loss: 0.4705\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2317 - val_loss: 0.4343\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1919 - val_loss: 0.4032\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1863 - val_loss: 0.3750\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1930 - val_loss: 0.3475\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1570 - val_loss: 0.3218\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.1426 - val_loss: 0.2957\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.1230 - val_loss: 0.2685\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1062 - val_loss: 0.2405\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0905 - val_loss: 0.2117\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0932 - val_loss: 0.1816\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0724 - val_loss: 0.1517\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0556 - val_loss: 0.1210\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0430 - val_loss: 0.0892\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0320 - val_loss: 0.0582\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.0202 - val_loss: 0.0321\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0126 - val_loss: 0.0135\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0063 - val_loss: 0.0054\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0051 - val_loss: 0.0033\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0037 - val_loss: 0.0027\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0043 - val_loss: 0.0027\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0037 - val_loss: 0.0025\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0037 - val_loss: 0.0024\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0037 - val_loss: 0.0023\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0037 - val_loss: 0.0026\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035 - val_loss: 0.0022\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0034 - val_loss: 0.0023\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0037 - val_loss: 0.0021\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0034 - val_loss: 0.0021\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0035 - val_loss: 0.0020\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0033 - val_loss: 0.0020\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0020\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0034 - val_loss: 0.0019\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0019\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0020\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0028 - val_loss: 0.0018\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0031 - val_loss: 0.0018\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0017\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0018\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0027 - val_loss: 0.0019\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0017\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0027 - val_loss: 0.0018\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0029 - val_loss: 0.0017\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0018\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0017\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0018\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0025 - val_loss: 0.0017\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0024 - val_loss: 0.0017\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0019\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0028 - val_loss: 0.0017\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024 - val_loss: 0.0021\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0027 - val_loss: 0.0019\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0017\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0019\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0026 - val_loss: 0.0017\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 0.0017\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0026 - val_loss: 0.0017\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0026 - val_loss: 0.0016\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0026 - val_loss: 0.0018\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 0.0018\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0026 - val_loss: 0.0017\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0022 - val_loss: 0.0019\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0017\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0025 - val_loss: 0.0017\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0016\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0024 - val_loss: 0.0016\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0017\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0028 - val_loss: 0.0018\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0024 - val_loss: 0.0016\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0027 - val_loss: 0.0019\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0023 - val_loss: 0.0017\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0016\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0017\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0023 - val_loss: 0.0017\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0017\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024 - val_loss: 0.0017\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0018\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0016\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0015\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0019\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 0.0016\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 0.0015\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0023 - val_loss: 0.0015\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0016\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 0.0016\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0016\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0026 - val_loss: 0.0017\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0016\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022 - val_loss: 0.0016\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0023 - val_loss: 0.0016\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0016\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0016\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0023 - val_loss: 0.0016\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0015\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0015\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0025 - val_loss: 0.0015\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0018\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0017\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.0015\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0021 - val_loss: 0.0015\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0023 - val_loss: 0.0014\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.0015\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0021 - val_loss: 0.0018\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.0014\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0023 - val_loss: 0.0019\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0020 - val_loss: 0.0017\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0022 - val_loss: 0.0014\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0024 - val_loss: 0.0015\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.0018\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0023 - val_loss: 0.0015\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0022 - val_loss: 0.0016\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0023 - val_loss: 0.0015\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0016\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024 - val_loss: 0.0017\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0014\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0017\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0015\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0024 - val_loss: 0.0016\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0015\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0017\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0020 - val_loss: 0.0014\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0021 - val_loss: 0.0015\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0014\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0015\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0014\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0014\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0021 - val_loss: 0.0017\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.0014\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 0.0016\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 0.0015\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0020 - val_loss: 0.0014\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0015\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0017\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0015\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0014\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0015\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0019 - val_loss: 0.0016\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0015\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0021 - val_loss: 0.0014\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0025 - val_loss: 0.0014\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025 - val_loss: 0.0014\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0017\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.0013\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0015\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022 - val_loss: 0.0014\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0016\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0015\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0015\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0014\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0017\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0014\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0013\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0014\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0016\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0013\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0014\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0013\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0014\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0014\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0014\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0014\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022 - val_loss: 0.0013\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0013\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0013\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0013\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0014\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0014\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0013\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0014\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0021 - val_loss: 0.0014\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.0013\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0013\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0013\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "2016-11-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 0.1366 - val_loss: 0.2800\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1155 - val_loss: 0.2323\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0889 - val_loss: 0.1755\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0639 - val_loss: 0.1175\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0406 - val_loss: 0.0629\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0205 - val_loss: 0.0203\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0095 - val_loss: 0.0040\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0087 - val_loss: 0.0043\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0085 - val_loss: 0.0043\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0069 - val_loss: 0.0070\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0066 - val_loss: 0.0070\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0069 - val_loss: 0.0045\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0058 - val_loss: 0.0039\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0054 - val_loss: 0.0039\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0047 - val_loss: 0.0038\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0046 - val_loss: 0.0037\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0043 - val_loss: 0.0037\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0039 - val_loss: 0.0038\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0038 - val_loss: 0.0037\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0036\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0038 - val_loss: 0.0036\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0039 - val_loss: 0.0035\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0036\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0034 - val_loss: 0.0035\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0030 - val_loss: 0.0034\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0032\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0031\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0031 - val_loss: 0.0031\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0030 - val_loss: 0.0027\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0031 - val_loss: 0.0028\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0025\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0027 - val_loss: 0.0022\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0021\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0021 - val_loss: 0.0025\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024 - val_loss: 0.0020\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0020\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0020\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024 - val_loss: 0.0018\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0022 - val_loss: 0.0018\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0017\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 0.0017\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0017\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0017\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 0.0017\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0017\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0023 - val_loss: 0.0016\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0021 - val_loss: 0.0017\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0017\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0017\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0018 - val_loss: 0.0016\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0016\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0018\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0016\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0015\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0017 - val_loss: 0.0016\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 0.0016\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0018 - val_loss: 0.0016\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0020 - val_loss: 0.0015\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0022 - val_loss: 0.0016\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0018 - val_loss: 0.0016\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0015\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0015\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0016\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0014\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0015\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0014\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0014\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0014\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0014\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0016\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0014\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0016\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0014\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0014\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0014\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0014\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0014\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0018 - val_loss: 0.0013\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0019 - val_loss: 0.0013\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0013\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0017 - val_loss: 0.0013\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0017 - val_loss: 0.0013\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0013\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0013\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0013\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0019 - val_loss: 0.0013\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0013\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0013\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "2016-12-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 0.3484 - val_loss: 0.6972\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3167 - val_loss: 0.6526\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2845 - val_loss: 0.6133\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2610 - val_loss: 0.5788\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2517 - val_loss: 0.5433\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2225 - val_loss: 0.5008\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2048 - val_loss: 0.4582\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1678 - val_loss: 0.4050\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1402 - val_loss: 0.3245\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1015 - val_loss: 0.2371\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0632 - val_loss: 0.1499\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0388 - val_loss: 0.0775\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0300 - val_loss: 0.0452\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0266 - val_loss: 0.0476\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0225 - val_loss: 0.0567\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0242 - val_loss: 0.0546\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0183 - val_loss: 0.0379\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0159 - val_loss: 0.0273\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0112 - val_loss: 0.0233\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0108 - val_loss: 0.0204\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0081 - val_loss: 0.0135\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0064 - val_loss: 0.0085\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0050 - val_loss: 0.0059\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0044 - val_loss: 0.0048\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0043 - val_loss: 0.0035\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0042 - val_loss: 0.0036\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - val_loss: 0.0030\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0041 - val_loss: 0.0038\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0043 - val_loss: 0.0036\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0044 - val_loss: 0.0030\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0038 - val_loss: 0.0048\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0042 - val_loss: 0.0033\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0034\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0036 - val_loss: 0.0033\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0031\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0035 - val_loss: 0.0030\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0035 - val_loss: 0.0031\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0027\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0035 - val_loss: 0.0031\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0025\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0038 - val_loss: 0.0030\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0032 - val_loss: 0.0024\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0026\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0033 - val_loss: 0.0023\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0025\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0026 - val_loss: 0.0020\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0024\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0025 - val_loss: 0.0017\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0025 - val_loss: 0.0017\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0026 - val_loss: 0.0022\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0067\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 0.0027\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0024 - val_loss: 0.0028\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 0.0046\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0024\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0029\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022 - val_loss: 0.0059\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0030\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0021\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0056\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0035\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0025 - val_loss: 0.0035\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0032\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0022 - val_loss: 0.0031\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0025\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024 - val_loss: 0.0028\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0044\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0037\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0029\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0029\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 0.0066\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0020 - val_loss: 0.0030\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0023 - val_loss: 0.0021\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0022 - val_loss: 0.0036\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0020 - val_loss: 0.0039\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024 - val_loss: 0.0022\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.0039\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0021 - val_loss: 0.0033\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0021 - val_loss: 0.0025\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0067\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0028\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0022 - val_loss: 0.0026\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0021 - val_loss: 0.0067\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0034\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0031\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0028\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0037\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0029\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0040\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0022 - val_loss: 0.0034\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0039\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0022 - val_loss: 0.0043\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0022\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0020 - val_loss: 0.0046\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0021 - val_loss: 0.0030\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0044\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0027\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0030\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.0054\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0020 - val_loss: 0.0027\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0020 - val_loss: 0.0024\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0043\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.0026\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0018 - val_loss: 0.0035\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0020 - val_loss: 0.0035\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.0026\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0027\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019 - val_loss: 0.0051\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0020\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0020 - val_loss: 0.0034\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0020 - val_loss: 0.0027\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0020 - val_loss: 0.0054\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0023 - val_loss: 0.0020\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0038\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019 - val_loss: 0.0045\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0019 - val_loss: 0.0039\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0023\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0045\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0056\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0021\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0021\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0032\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0019 - val_loss: 0.0024\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0026\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0038\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022 - val_loss: 0.0052\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0023\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0035\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0042\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0019 - val_loss: 0.0023\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0049\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0018\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0036\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0028\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0038\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0028\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0035\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0024\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0035\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0025\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0040\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0023\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0038\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0024\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0041\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0025\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0023\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0040\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0031\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0032\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0036\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0016 - val_loss: 0.0035\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0023\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0030\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0033\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0017 - val_loss: 0.0025\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0034\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0037\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.0044\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0019\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0016 - val_loss: 0.0037\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0036\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0018\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0063\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0047\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0026\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0037\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0014\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0021 - val_loss: 0.0038\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0022\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0031\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0034\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0054\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0017\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0048\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0052\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0015\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0049\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0027\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0028\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0019\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0041\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0019 - val_loss: 0.0066\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "2017-01-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 0.2832 - val_loss: 0.5138\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2460 - val_loss: 0.4618\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2175 - val_loss: 0.4141\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1864 - val_loss: 0.3702\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1752 - val_loss: 0.3290\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1501 - val_loss: 0.2908\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1273 - val_loss: 0.2546\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1064 - val_loss: 0.2200\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0920 - val_loss: 0.1871\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0729 - val_loss: 0.1546\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0578 - val_loss: 0.1201\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0395 - val_loss: 0.0731\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0188 - val_loss: 0.0236\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0078 - val_loss: 0.0068\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0095 - val_loss: 0.0105\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0070 - val_loss: 0.0167\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0068 - val_loss: 0.0181\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0071 - val_loss: 0.0151\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0069 - val_loss: 0.0118\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0057 - val_loss: 0.0102\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0065 - val_loss: 0.0116\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0065 - val_loss: 0.0123\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0061 - val_loss: 0.0107\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0061 - val_loss: 0.0095\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0057 - val_loss: 0.0092\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0060 - val_loss: 0.0093\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0059 - val_loss: 0.0098\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0053 - val_loss: 0.0103\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0056 - val_loss: 0.0102\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0060 - val_loss: 0.0088\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0060 - val_loss: 0.0084\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0055 - val_loss: 0.0083\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0053 - val_loss: 0.0083\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0056 - val_loss: 0.0079\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0053 - val_loss: 0.0078\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0060 - val_loss: 0.0079\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0057 - val_loss: 0.0069\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0048 - val_loss: 0.0064\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0050 - val_loss: 0.0069\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0056 - val_loss: 0.0082\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0049 - val_loss: 0.0079\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0055 - val_loss: 0.0066\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0049 - val_loss: 0.0057\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0048 - val_loss: 0.0059\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0050 - val_loss: 0.0062\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0048 - val_loss: 0.0060\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0051 - val_loss: 0.0060\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0050 - val_loss: 0.0058\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0047 - val_loss: 0.0055\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0049 - val_loss: 0.0056\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0046 - val_loss: 0.0056\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0054 - val_loss: 0.0058\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0052 - val_loss: 0.0057\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0052 - val_loss: 0.0056\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0050 - val_loss: 0.0055\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0044 - val_loss: 0.0057\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0049 - val_loss: 0.0057\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0046 - val_loss: 0.0055\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0050 - val_loss: 0.0051\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0046 - val_loss: 0.0054\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0049 - val_loss: 0.0054\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0050 - val_loss: 0.0054\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0044 - val_loss: 0.0053\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0047 - val_loss: 0.0051\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0050 - val_loss: 0.0053\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045 - val_loss: 0.0051\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0047 - val_loss: 0.0051\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0052 - val_loss: 0.0052\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0046 - val_loss: 0.0051\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0042 - val_loss: 0.0051\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0044 - val_loss: 0.0051\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - val_loss: 0.0052\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0043 - val_loss: 0.0053\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045 - val_loss: 0.0051\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0047 - val_loss: 0.0050\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0045 - val_loss: 0.0050\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0044 - val_loss: 0.0050\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0048 - val_loss: 0.0051\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0046 - val_loss: 0.0051\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0050\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0046 - val_loss: 0.0051\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0044 - val_loss: 0.0050\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0042 - val_loss: 0.0050\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0047 - val_loss: 0.0049\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0049\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0043 - val_loss: 0.0049\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0045 - val_loss: 0.0049\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0043 - val_loss: 0.0049\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0041 - val_loss: 0.0049\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0049\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0049\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0046 - val_loss: 0.0049\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0041 - val_loss: 0.0049\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0048\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0048\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0042 - val_loss: 0.0048\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0043 - val_loss: 0.0048\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0048\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0040 - val_loss: 0.0048\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0041 - val_loss: 0.0048\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0045 - val_loss: 0.0047\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0047\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0045 - val_loss: 0.0048\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0047\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0040 - val_loss: 0.0047\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0047\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0043 - val_loss: 0.0047\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0045 - val_loss: 0.0047\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0039 - val_loss: 0.0047\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0042 - val_loss: 0.0046\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0046\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0041 - val_loss: 0.0046\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0046\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0046\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0046\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0046\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0038 - val_loss: 0.0047\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0039 - val_loss: 0.0047\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0041 - val_loss: 0.0047\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0039 - val_loss: 0.0045\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0036 - val_loss: 0.0046\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0041 - val_loss: 0.0046\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0039 - val_loss: 0.0045\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0037 - val_loss: 0.0046\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0045\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0041 - val_loss: 0.0045\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0040 - val_loss: 0.0045\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0045\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0036 - val_loss: 0.0044\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0039 - val_loss: 0.0044\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0035 - val_loss: 0.0044\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0044\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0038 - val_loss: 0.0044\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0038 - val_loss: 0.0044\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0045\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0044\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0039 - val_loss: 0.0043\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0043\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0044\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0036 - val_loss: 0.0044\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0033 - val_loss: 0.0043\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0035 - val_loss: 0.0044\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0043\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0042\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0037 - val_loss: 0.0042\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0044\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0033 - val_loss: 0.0044\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0042\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0042\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0042\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0036 - val_loss: 0.0042\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0041 - val_loss: 0.0042\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0034 - val_loss: 0.0042\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0037 - val_loss: 0.0042\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0041\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0036 - val_loss: 0.0042\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0035 - val_loss: 0.0042\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038 - val_loss: 0.0041\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.0041\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038 - val_loss: 0.0042\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0037 - val_loss: 0.0041\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0034 - val_loss: 0.0040\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0033 - val_loss: 0.0040\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0034 - val_loss: 0.0040\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0035 - val_loss: 0.0040\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0033 - val_loss: 0.0040\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0041\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0035 - val_loss: 0.0040\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0041\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0040\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0039\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0039\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0033 - val_loss: 0.0040\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0035 - val_loss: 0.0041\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0030 - val_loss: 0.0039\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0039\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0039\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0031 - val_loss: 0.0039\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0039\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0039\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0039\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0033 - val_loss: 0.0039\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0040\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0038\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0037\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0039\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0035 - val_loss: 0.0039\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0037\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0037\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0031 - val_loss: 0.0037\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0037 - val_loss: 0.0040\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0029 - val_loss: 0.0038\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0030 - val_loss: 0.0036\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0032 - val_loss: 0.0036\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0036\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0031 - val_loss: 0.0037\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0032 - val_loss: 0.0036\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0036\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0029 - val_loss: 0.0036\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0030 - val_loss: 0.0038\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "2017-02-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 0.1575 - val_loss: 0.3505\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1502 - val_loss: 0.3140\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1223 - val_loss: 0.2782\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1168 - val_loss: 0.2408\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0943 - val_loss: 0.1993\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0762 - val_loss: 0.1535\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0545 - val_loss: 0.1026\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0384 - val_loss: 0.0497\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0194 - val_loss: 0.0117\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0119 - val_loss: 0.0132\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0124 - val_loss: 0.0110\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0120 - val_loss: 0.0078\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0104 - val_loss: 0.0082\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0103 - val_loss: 0.0079\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0103 - val_loss: 0.0087\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0094 - val_loss: 0.0089\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0097 - val_loss: 0.0089\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0099 - val_loss: 0.0091\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0087 - val_loss: 0.0103\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0097 - val_loss: 0.0119\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0101 - val_loss: 0.0125\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0095 - val_loss: 0.0165\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0090 - val_loss: 0.0156\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0079 - val_loss: 0.0175\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0090 - val_loss: 0.0183\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0077 - val_loss: 0.0229\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0081 - val_loss: 0.0235\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0083 - val_loss: 0.0251\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0072 - val_loss: 0.0341\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0084 - val_loss: 0.0289\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0082 - val_loss: 0.0380\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0078 - val_loss: 0.0368\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0086 - val_loss: 0.0330\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0081 - val_loss: 0.0422\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0086 - val_loss: 0.0438\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0076 - val_loss: 0.0510\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0074 - val_loss: 0.0457\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0079 - val_loss: 0.0479\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0072 - val_loss: 0.0453\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0081 - val_loss: 0.0486\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0075 - val_loss: 0.0578\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0071 - val_loss: 0.0534\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0078 - val_loss: 0.0447\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0077 - val_loss: 0.0580\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0074 - val_loss: 0.0598\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0077 - val_loss: 0.0465\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0075 - val_loss: 0.0631\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0069 - val_loss: 0.0683\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0075 - val_loss: 0.0696\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0069 - val_loss: 0.0542\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0071 - val_loss: 0.0682\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0069 - val_loss: 0.0675\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0073 - val_loss: 0.0659\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0071 - val_loss: 0.0607\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0067 - val_loss: 0.0606\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0067 - val_loss: 0.0662\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0074 - val_loss: 0.0839\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0068 - val_loss: 0.0731\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0070 - val_loss: 0.0641\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0070 - val_loss: 0.0675\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0064 - val_loss: 0.0829\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0066 - val_loss: 0.0795\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0068 - val_loss: 0.0535\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0068 - val_loss: 0.0665\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0059 - val_loss: 0.0936\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0070 - val_loss: 0.0701\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0063 - val_loss: 0.0681\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0055 - val_loss: 0.0666\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0064 - val_loss: 0.0861\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0057 - val_loss: 0.0720\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0055 - val_loss: 0.0591\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0062 - val_loss: 0.0685\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0055 - val_loss: 0.0700\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0060 - val_loss: 0.0793\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0060 - val_loss: 0.0693\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0057 - val_loss: 0.0627\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0057 - val_loss: 0.0793\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0061 - val_loss: 0.0733\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0050 - val_loss: 0.0628\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0053 - val_loss: 0.0734\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0054 - val_loss: 0.0790\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0053 - val_loss: 0.0727\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0056 - val_loss: 0.0821\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0055 - val_loss: 0.0730\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0053 - val_loss: 0.0719\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0053 - val_loss: 0.0666\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0046 - val_loss: 0.0820\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0049 - val_loss: 0.0767\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0043 - val_loss: 0.0755\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0050 - val_loss: 0.0729\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0050 - val_loss: 0.0772\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0047 - val_loss: 0.0637\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0049 - val_loss: 0.0751\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0051 - val_loss: 0.0624\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0048 - val_loss: 0.0704\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0050 - val_loss: 0.0645\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0043 - val_loss: 0.0639\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045 - val_loss: 0.0706\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0040 - val_loss: 0.0648\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045 - val_loss: 0.0748\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0035 - val_loss: 0.0709\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038 - val_loss: 0.0832\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0033 - val_loss: 0.0606\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0037 - val_loss: 0.0409\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0033 - val_loss: 0.0367\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0339\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0029 - val_loss: 0.0352\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0030 - val_loss: 0.0398\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0033 - val_loss: 0.0279\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0034 - val_loss: 0.0284\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0038 - val_loss: 0.0290\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0033 - val_loss: 0.0244\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0027 - val_loss: 0.0369\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0033 - val_loss: 0.0297\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0029 - val_loss: 0.0277\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0029 - val_loss: 0.0265\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0307\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0029 - val_loss: 0.0326\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0286\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0300\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0033 - val_loss: 0.0249\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0248\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0288\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0231\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0291\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0029 - val_loss: 0.0194\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0034 - val_loss: 0.0238\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0030 - val_loss: 0.0231\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0282\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0252\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0221\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0227\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0262\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0029 - val_loss: 0.0224\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0224\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0030 - val_loss: 0.0241\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0031 - val_loss: 0.0179\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0243\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0229\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0191\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0225\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0203\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0213\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0209\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0204\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0211\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0184\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0029 - val_loss: 0.0174\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0204\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0164\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0032 - val_loss: 0.0180\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0189\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0163\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0176\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0162\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0031 - val_loss: 0.0146\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0178\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0160\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0027 - val_loss: 0.0134\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0221\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0150\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0175\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0165\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0142\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0137\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0028 - val_loss: 0.0157\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0141\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0148\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0140\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0131\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0148\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0174\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0141\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0023 - val_loss: 0.0143\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0145\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0147\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0132\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0123\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0144\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0109\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0125\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0033 - val_loss: 0.0118\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0027 - val_loss: 0.0122\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0125\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - val_loss: 0.0123\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0025 - val_loss: 0.0143\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0027 - val_loss: 0.0106\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0116\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0025 - val_loss: 0.0133\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0024 - val_loss: 0.0115\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0027 - val_loss: 0.0129\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0025 - val_loss: 0.0111\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0025 - val_loss: 0.0120\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0027 - val_loss: 0.0105\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0028 - val_loss: 0.0119\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0025 - val_loss: 0.0113\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.0101\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0025 - val_loss: 0.0110\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0028 - val_loss: 0.0111\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0028 - val_loss: 0.0106\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1341.35, Last: 133215.06648559577\n",
            "\t\t Buying 0.03727587877884221 units of gold on 2018-02-01 00:00:00\n",
            "\t\t Price: 1341.35, unit: 50.0\n",
            "2017-03-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.1993 - val_loss: 0.4309\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2056 - val_loss: 0.4094\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1930 - val_loss: 0.3872\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1841 - val_loss: 0.3636\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1528 - val_loss: 0.3390\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1399 - val_loss: 0.3122\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1361 - val_loss: 0.2822\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1182 - val_loss: 0.2490\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1050 - val_loss: 0.2107\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0813 - val_loss: 0.1642\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0535 - val_loss: 0.1066\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0293 - val_loss: 0.0440\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0271 - val_loss: 0.0206\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0231 - val_loss: 0.0216\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0185 - val_loss: 0.0242\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0148 - val_loss: 0.0177\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0106 - val_loss: 0.0095\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0081 - val_loss: 0.0064\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0068 - val_loss: 0.0061\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0064 - val_loss: 0.0059\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0062 - val_loss: 0.0056\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0062 - val_loss: 0.0052\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0066 - val_loss: 0.0050\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0053 - val_loss: 0.0051\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0056 - val_loss: 0.0045\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0057 - val_loss: 0.0043\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0057 - val_loss: 0.0046\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0048 - val_loss: 0.0039\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0048 - val_loss: 0.0038\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0050 - val_loss: 0.0039\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0045 - val_loss: 0.0035\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0044 - val_loss: 0.0035\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0042 - val_loss: 0.0036\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0043 - val_loss: 0.0033\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0038 - val_loss: 0.0034\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035 - val_loss: 0.0032\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036 - val_loss: 0.0034\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0037 - val_loss: 0.0033\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038 - val_loss: 0.0034\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0036 - val_loss: 0.0037\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0037 - val_loss: 0.0033\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0036 - val_loss: 0.0034\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0040 - val_loss: 0.0036\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0035 - val_loss: 0.0031\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0037 - val_loss: 0.0033\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0031 - val_loss: 0.0031\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0037 - val_loss: 0.0038\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0033 - val_loss: 0.0031\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0040 - val_loss: 0.0035\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0031\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0035 - val_loss: 0.0032\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0038 - val_loss: 0.0032\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0030 - val_loss: 0.0029\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0034 - val_loss: 0.0037\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0029\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0030\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0033\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0029\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0030\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0032 - val_loss: 0.0030\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0033 - val_loss: 0.0029\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0031\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0029\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0028\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0034 - val_loss: 0.0029\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0031\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0028\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0028\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0033 - val_loss: 0.0030\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0028\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0033 - val_loss: 0.0029\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0028\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0027\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0030\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0029 - val_loss: 0.0027\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0031 - val_loss: 0.0027\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0029 - val_loss: 0.0027\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0026\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0028\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0026\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0027\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0029 - val_loss: 0.0026\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0030 - val_loss: 0.0027\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0026\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0027\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0025\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0026\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0031 - val_loss: 0.0026\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0030 - val_loss: 0.0026\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0026\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0029 - val_loss: 0.0026\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0025\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0025\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0029 - val_loss: 0.0025\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0024\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0025\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0024\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.0026\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0026 - val_loss: 0.0024\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0030 - val_loss: 0.0024\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0030 - val_loss: 0.0026\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0023\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0026 - val_loss: 0.0024\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0025 - val_loss: 0.0024\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0028 - val_loss: 0.0022\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0031 - val_loss: 0.0026\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0027 - val_loss: 0.0022\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0028 - val_loss: 0.0023\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0022\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.0021\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0026 - val_loss: 0.0022\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0021\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0026 - val_loss: 0.0022\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0021\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0022\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0023 - val_loss: 0.0022\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0022\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0022\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0021\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0020\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0023 - val_loss: 0.0022\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0021\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0021\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0022\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0021\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0023\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0021\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0027 - val_loss: 0.0022\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0020\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0022\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0020\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0020\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0020\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0021\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0020\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0019\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 0.0022\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0019\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0021\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0020\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0020\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0023\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0027 - val_loss: 0.0019\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0021\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0020\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0019\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0026 - val_loss: 0.0020\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0020\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0019\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0020\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0019\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0019\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0019\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0022 - val_loss: 0.0019\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0020\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0019\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - val_loss: 0.0019\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0019\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0026 - val_loss: 0.0020\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0018\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0019\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0027 - val_loss: 0.0021\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0018\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0019\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024 - val_loss: 0.0018\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0022 - val_loss: 0.0019\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0019\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0018\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0018\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0020\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0017\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0019\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024 - val_loss: 0.0018\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0018\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0018\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t Selling 0.03630281017013955 units of gold on 2018-03-01 00:00:00\n",
            "\t\t Price: 1307.75, unit: 47.475\n",
            "2017-04-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.1904 - val_loss: 0.4069\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1570 - val_loss: 0.3552\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1383 - val_loss: 0.3063\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1199 - val_loss: 0.2604\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0980 - val_loss: 0.2172\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0851 - val_loss: 0.1745\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0647 - val_loss: 0.1332\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0543 - val_loss: 0.0938\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0334 - val_loss: 0.0589\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0208 - val_loss: 0.0304\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0154 - val_loss: 0.0139\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0099 - val_loss: 0.0094\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0091 - val_loss: 0.0099\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0085 - val_loss: 0.0092\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0074 - val_loss: 0.0087\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0082 - val_loss: 0.0082\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0069 - val_loss: 0.0079\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0075 - val_loss: 0.0075\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0072 - val_loss: 0.0074\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0068 - val_loss: 0.0072\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0060 - val_loss: 0.0066\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0057 - val_loss: 0.0064\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0062 - val_loss: 0.0069\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0057 - val_loss: 0.0062\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0049 - val_loss: 0.0067\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0054 - val_loss: 0.0061\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0054 - val_loss: 0.0058\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0050 - val_loss: 0.0077\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0052 - val_loss: 0.0058\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0049 - val_loss: 0.0064\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0048 - val_loss: 0.0065\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0049 - val_loss: 0.0062\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0049 - val_loss: 0.0066\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0055 - val_loss: 0.0068\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0054 - val_loss: 0.0059\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0045 - val_loss: 0.0066\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0049 - val_loss: 0.0063\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0041 - val_loss: 0.0064\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0063\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0043 - val_loss: 0.0061\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0043 - val_loss: 0.0061\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0047 - val_loss: 0.0064\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0051 - val_loss: 0.0059\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0051 - val_loss: 0.0067\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0037 - val_loss: 0.0059\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0048 - val_loss: 0.0057\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0044 - val_loss: 0.0072\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0046 - val_loss: 0.0056\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0039 - val_loss: 0.0061\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0049 - val_loss: 0.0060\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0056\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0045 - val_loss: 0.0061\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0043 - val_loss: 0.0057\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0044 - val_loss: 0.0055\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0042 - val_loss: 0.0067\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0054\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0053\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0049 - val_loss: 0.0074\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0041 - val_loss: 0.0052\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0041 - val_loss: 0.0055\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0063\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0044 - val_loss: 0.0052\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0052\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0058\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036 - val_loss: 0.0052\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - val_loss: 0.0052\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0039 - val_loss: 0.0058\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0041 - val_loss: 0.0050\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0037 - val_loss: 0.0052\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0045 - val_loss: 0.0052\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0037 - val_loss: 0.0050\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0042 - val_loss: 0.0056\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0037 - val_loss: 0.0050\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0041 - val_loss: 0.0054\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0036 - val_loss: 0.0049\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0041 - val_loss: 0.0047\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0040 - val_loss: 0.0055\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0041 - val_loss: 0.0047\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0042 - val_loss: 0.0050\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.0048\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0037 - val_loss: 0.0047\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038 - val_loss: 0.0053\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0046\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0045\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0039 - val_loss: 0.0052\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0035 - val_loss: 0.0050\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0044\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0044\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0050\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0038 - val_loss: 0.0050\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0038 - val_loss: 0.0047\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0042\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0048\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0034 - val_loss: 0.0045\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0039 - val_loss: 0.0041\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0047\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0042 - val_loss: 0.0045\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0037 - val_loss: 0.0045\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0040\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0032 - val_loss: 0.0043\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0040 - val_loss: 0.0046\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0041\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0038 - val_loss: 0.0041\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0040\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0042\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0038\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0039\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0035 - val_loss: 0.0045\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0038\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0031 - val_loss: 0.0045\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0037\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0037\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0038 - val_loss: 0.0041\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0036 - val_loss: 0.0037\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0034 - val_loss: 0.0044\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0035\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0033 - val_loss: 0.0038\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0039\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0035\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0032 - val_loss: 0.0035\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0037\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0033 - val_loss: 0.0036\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0032\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0042\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0032\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0035\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0032\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0033\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0032 - val_loss: 0.0037\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0035 - val_loss: 0.0030\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0034 - val_loss: 0.0035\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0030 - val_loss: 0.0030\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0033\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0029\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0033\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - val_loss: 0.0029\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0030\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0034\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0031 - val_loss: 0.0030\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0029 - val_loss: 0.0028\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0031 - val_loss: 0.0029\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0032 - val_loss: 0.0030\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0027\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0030\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0030 - val_loss: 0.0031\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0026\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0035\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0031 - val_loss: 0.0024\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0029\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0025\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0028 - val_loss: 0.0025\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0032 - val_loss: 0.0027\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0035 - val_loss: 0.0026\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0032 - val_loss: 0.0024\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0027 - val_loss: 0.0029\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0024\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0029\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0023\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0026\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0032 - val_loss: 0.0025\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0030 - val_loss: 0.0031\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0032 - val_loss: 0.0025\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0030 - val_loss: 0.0026\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0029 - val_loss: 0.0022\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0034 - val_loss: 0.0035\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0022\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0033\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.0022\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0027\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0027 - val_loss: 0.0023\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0022\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0025\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0024\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0024\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0023\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0023\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0022\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0025\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0021\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0021\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - val_loss: 0.0024\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0022\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0023\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0027 - val_loss: 0.0022\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024 - val_loss: 0.0022\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0026 - val_loss: 0.0021\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0024\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "2017-05-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.2571 - val_loss: 0.6802\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2234 - val_loss: 0.5875\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1709 - val_loss: 0.5042\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1482 - val_loss: 0.4274\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1292 - val_loss: 0.3570\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1030 - val_loss: 0.2945\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0898 - val_loss: 0.2366\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0687 - val_loss: 0.1790\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0503 - val_loss: 0.1197\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0318 - val_loss: 0.0641\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0161 - val_loss: 0.0235\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0101 - val_loss: 0.0091\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0119 - val_loss: 0.0082\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0129 - val_loss: 0.0093\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0102 - val_loss: 0.0132\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0099 - val_loss: 0.0143\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0102 - val_loss: 0.0131\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0100 - val_loss: 0.0115\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0096 - val_loss: 0.0109\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0095 - val_loss: 0.0117\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0093 - val_loss: 0.0120\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0105 - val_loss: 0.0099\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0089 - val_loss: 0.0107\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0086 - val_loss: 0.0105\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0084 - val_loss: 0.0101\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0093 - val_loss: 0.0096\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0086 - val_loss: 0.0098\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0089 - val_loss: 0.0096\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0090 - val_loss: 0.0096\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0083 - val_loss: 0.0096\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0093 - val_loss: 0.0089\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0075 - val_loss: 0.0096\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0082 - val_loss: 0.0086\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0075 - val_loss: 0.0092\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0078 - val_loss: 0.0082\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0079 - val_loss: 0.0080\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0075 - val_loss: 0.0083\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0071 - val_loss: 0.0088\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0068 - val_loss: 0.0073\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0075 - val_loss: 0.0076\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0067 - val_loss: 0.0075\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0060 - val_loss: 0.0084\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0070 - val_loss: 0.0068\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0061 - val_loss: 0.0078\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0054 - val_loss: 0.0073\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0064 - val_loss: 0.0073\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0056 - val_loss: 0.0072\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0060 - val_loss: 0.0078\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0057 - val_loss: 0.0079\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0053 - val_loss: 0.0093\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0052 - val_loss: 0.0089\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0057 - val_loss: 0.0093\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0055 - val_loss: 0.0119\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0050 - val_loss: 0.0065\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0053 - val_loss: 0.0131\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0055 - val_loss: 0.0070\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0052 - val_loss: 0.0105\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0052 - val_loss: 0.0083\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0050 - val_loss: 0.0085\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0047 - val_loss: 0.0099\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0045 - val_loss: 0.0078\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0049 - val_loss: 0.0107\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0050 - val_loss: 0.0089\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0049 - val_loss: 0.0096\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0092\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0045 - val_loss: 0.0092\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0047 - val_loss: 0.0098\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0051 - val_loss: 0.0085\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0052 - val_loss: 0.0098\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0045 - val_loss: 0.0083\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0051 - val_loss: 0.0083\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0050 - val_loss: 0.0099\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0049 - val_loss: 0.0076\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0050 - val_loss: 0.0097\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0047 - val_loss: 0.0081\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0048 - val_loss: 0.0111\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0056 - val_loss: 0.0082\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0052 - val_loss: 0.0095\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0050 - val_loss: 0.0090\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0050 - val_loss: 0.0093\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0047 - val_loss: 0.0094\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0050 - val_loss: 0.0082\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0045 - val_loss: 0.0114\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0046 - val_loss: 0.0074\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0046 - val_loss: 0.0088\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0051 - val_loss: 0.0100\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0051 - val_loss: 0.0082\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0048 - val_loss: 0.0104\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0047 - val_loss: 0.0072\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0044 - val_loss: 0.0106\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0044 - val_loss: 0.0071\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0042 - val_loss: 0.0086\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0052 - val_loss: 0.0104\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0045 - val_loss: 0.0068\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0046 - val_loss: 0.0086\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0045 - val_loss: 0.0091\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0050 - val_loss: 0.0074\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0045 - val_loss: 0.0082\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0043 - val_loss: 0.0082\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0043 - val_loss: 0.0080\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0048 - val_loss: 0.0075\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0045 - val_loss: 0.0106\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0043 - val_loss: 0.0072\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0044 - val_loss: 0.0097\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0046 - val_loss: 0.0086\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0048 - val_loss: 0.0074\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0044 - val_loss: 0.0103\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0047 - val_loss: 0.0085\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0042 - val_loss: 0.0088\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0046 - val_loss: 0.0074\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0075\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0043 - val_loss: 0.0082\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0049 - val_loss: 0.0083\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0039 - val_loss: 0.0076\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0085\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0078\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0042 - val_loss: 0.0093\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0082\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0045 - val_loss: 0.0076\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0046 - val_loss: 0.0087\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0071\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0076\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0040 - val_loss: 0.0079\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0057\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0047 - val_loss: 0.0096\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0065\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0041 - val_loss: 0.0065\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0040 - val_loss: 0.0075\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0040 - val_loss: 0.0065\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0089\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0039 - val_loss: 0.0056\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0041 - val_loss: 0.0073\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0038 - val_loss: 0.0071\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0039 - val_loss: 0.0070\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0060\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.0066\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0073\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0056\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0069\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0036 - val_loss: 0.0059\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0037 - val_loss: 0.0058\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0060\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0053\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0051\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0054\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0060\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0030 - val_loss: 0.0044\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0064\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0045\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0051\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0035 - val_loss: 0.0050\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0054\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0032 - val_loss: 0.0041\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0048\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0038\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0047\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.0041\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0038\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0030 - val_loss: 0.0051\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0039\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0045\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0034\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.0033\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0041\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0028 - val_loss: 0.0032\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0035\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0033\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0034\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0032\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0036\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0030\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0032 - val_loss: 0.0037\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0033\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0025 - val_loss: 0.0030\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0023 - val_loss: 0.0034\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0023 - val_loss: 0.0029\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0028 - val_loss: 0.0033\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0031 - val_loss: 0.0029\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0024 - val_loss: 0.0028\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0024 - val_loss: 0.0034\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0025 - val_loss: 0.0035\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025 - val_loss: 0.0027\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0028 - val_loss: 0.0035\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0023 - val_loss: 0.0033\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0021 - val_loss: 0.0026\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0025 - val_loss: 0.0038\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.0026\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0020 - val_loss: 0.0039\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0023 - val_loss: 0.0026\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0030\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.0027\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0029\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0022 - val_loss: 0.0025\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0025\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.0025\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0031\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0027\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0025\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "2017-06-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.2062 - val_loss: 0.4673\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1686 - val_loss: 0.3895\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1462 - val_loss: 0.3190\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1142 - val_loss: 0.2579\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0882 - val_loss: 0.2047\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0808 - val_loss: 0.1556\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0543 - val_loss: 0.1113\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0426 - val_loss: 0.0711\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0265 - val_loss: 0.0380\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0231 - val_loss: 0.0158\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0117 - val_loss: 0.0070\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0084 - val_loss: 0.0069\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0067 - val_loss: 0.0087\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0070 - val_loss: 0.0089\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0065 - val_loss: 0.0059\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0059 - val_loss: 0.0050\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0055 - val_loss: 0.0052\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0046\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0048 - val_loss: 0.0043\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0044\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0037 - val_loss: 0.0054\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0049\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0064\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0053\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0034 - val_loss: 0.0062\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0032 - val_loss: 0.0068\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0050\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0033 - val_loss: 0.0068\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.0056\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0030 - val_loss: 0.0052\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0032 - val_loss: 0.0055\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0033 - val_loss: 0.0051\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0032 - val_loss: 0.0067\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0048\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0032 - val_loss: 0.0062\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0032 - val_loss: 0.0052\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0035 - val_loss: 0.0051\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0032 - val_loss: 0.0052\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0032 - val_loss: 0.0051\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0030 - val_loss: 0.0055\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0032 - val_loss: 0.0056\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.0061\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0050\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0029 - val_loss: 0.0054\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0064\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0047\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0062\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0048\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0029 - val_loss: 0.0055\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0054\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0044\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0058\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0047\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0032 - val_loss: 0.0042\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0067\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035 - val_loss: 0.0045\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0052\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0046\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0032 - val_loss: 0.0052\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0045\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0060\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0040\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0027 - val_loss: 0.0050\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0042\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0046\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0048\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0047\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0047\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0047\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0027 - val_loss: 0.0040\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0054\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0041\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0025 - val_loss: 0.0044\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0047\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0038\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0051\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0030 - val_loss: 0.0038\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0041\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0027 - val_loss: 0.0043\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0036\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0052\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0039\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0029 - val_loss: 0.0040\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0045\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0041\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0037\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0043\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0036\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0041\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0044\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0039\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0043\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0037\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0037\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0035\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0039\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0040\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0033\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0037\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0038\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0033\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0026 - val_loss: 0.0034\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0034\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.0034\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0036\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0031\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0026 - val_loss: 0.0037\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.0030\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0035\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0024 - val_loss: 0.0036\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0024 - val_loss: 0.0029\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0026 - val_loss: 0.0036\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0022 - val_loss: 0.0034\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0021 - val_loss: 0.0029\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0033\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0030\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0038\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0022 - val_loss: 0.0027\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0021 - val_loss: 0.0037\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0023 - val_loss: 0.0027\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0022 - val_loss: 0.0030\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0024 - val_loss: 0.0033\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0025 - val_loss: 0.0035\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0021 - val_loss: 0.0027\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0034\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0026\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0027\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0033\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.0026\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0030\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0030\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0027\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0027\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0028\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0026\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0025\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0024\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0023 - val_loss: 0.0031\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0024\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0029\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0025\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0024\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0029\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.0024\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0032\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0024\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0024\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024 - val_loss: 0.0031\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0024\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0025\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0026\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0027\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0021 - val_loss: 0.0026\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0025\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0025\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.0025\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0026\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0018 - val_loss: 0.0026\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0026\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.0024\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0025\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0024\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0027\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0023 - val_loss: 0.0024\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0023\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0027\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0025\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0023\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0024\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0022\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0027\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0022\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0020 - val_loss: 0.0025\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0022\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0024\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - val_loss: 0.0022\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.0027\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0020 - val_loss: 0.0024\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0024\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0026\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0024 - val_loss: 0.0023\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0019 - val_loss: 0.0023\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.0022\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0020 - val_loss: 0.0023\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0018 - val_loss: 0.0024\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0020 - val_loss: 0.0022\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0018 - val_loss: 0.0022\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1294.6, Last: 1303.5363764524461\n",
            "\t\t Buying 0.03848680094237603 units of gold on 2018-06-01 00:00:00\n",
            "\t\t Price: 1294.6, unit: 49.82501250000001\n",
            "2017-07-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.3707 - val_loss: 0.5218\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3181 - val_loss: 0.4775\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2953 - val_loss: 0.4333\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2644 - val_loss: 0.3883\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2281 - val_loss: 0.3419\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2097 - val_loss: 0.2923\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1707 - val_loss: 0.2389\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1277 - val_loss: 0.1753\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1005 - val_loss: 0.1029\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0603 - val_loss: 0.0365\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0235 - val_loss: 0.0115\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0216 - val_loss: 0.0107\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0155 - val_loss: 0.0096\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0148 - val_loss: 0.0101\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0129 - val_loss: 0.0083\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0125 - val_loss: 0.0077\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0103 - val_loss: 0.0064\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0091 - val_loss: 0.0068\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0083 - val_loss: 0.0066\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0079 - val_loss: 0.0068\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0081 - val_loss: 0.0061\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0076 - val_loss: 0.0057\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0074 - val_loss: 0.0067\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0076 - val_loss: 0.0064\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0070 - val_loss: 0.0053\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0073 - val_loss: 0.0061\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0067 - val_loss: 0.0057\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0071 - val_loss: 0.0056\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0058 - val_loss: 0.0057\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0058 - val_loss: 0.0054\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0058 - val_loss: 0.0055\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0058 - val_loss: 0.0052\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0062 - val_loss: 0.0051\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0052 - val_loss: 0.0054\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0054 - val_loss: 0.0051\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0055 - val_loss: 0.0052\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0048 - val_loss: 0.0054\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0048 - val_loss: 0.0054\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0044 - val_loss: 0.0058\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0045 - val_loss: 0.0057\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0059\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0043 - val_loss: 0.0064\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0050 - val_loss: 0.0060\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0070\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0061\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0069\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0044 - val_loss: 0.0060\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0040 - val_loss: 0.0066\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0039 - val_loss: 0.0058\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0045 - val_loss: 0.0063\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045 - val_loss: 0.0056\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0042 - val_loss: 0.0055\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0042 - val_loss: 0.0064\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0043 - val_loss: 0.0055\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0039 - val_loss: 0.0072\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0038 - val_loss: 0.0054\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038 - val_loss: 0.0061\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0037 - val_loss: 0.0057\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0036 - val_loss: 0.0058\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0040 - val_loss: 0.0060\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0040 - val_loss: 0.0065\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0041 - val_loss: 0.0057\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0040 - val_loss: 0.0062\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0040 - val_loss: 0.0061\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0035 - val_loss: 0.0058\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0062\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0056\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0039 - val_loss: 0.0059\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0035 - val_loss: 0.0058\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0055\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0038 - val_loss: 0.0055\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0058\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0056\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0065\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0057\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0054\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0055\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0037 - val_loss: 0.0054\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0055\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0036 - val_loss: 0.0054\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0055\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0037 - val_loss: 0.0055\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0040 - val_loss: 0.0058\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0039 - val_loss: 0.0051\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0067\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0045 - val_loss: 0.0052\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0055\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0053\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0064\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0054\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0052\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0038 - val_loss: 0.0056\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0053\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0038 - val_loss: 0.0062\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0037 - val_loss: 0.0052\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0054\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0053\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0055\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0053\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0056\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0037 - val_loss: 0.0052\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0054\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0033 - val_loss: 0.0054\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0052\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0052\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0050\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036 - val_loss: 0.0055\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0051\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0056\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0035 - val_loss: 0.0052\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0038 - val_loss: 0.0054\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - val_loss: 0.0049\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0048\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0050\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0050\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0055\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0051\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0057\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0036 - val_loss: 0.0049\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0052\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0038 - val_loss: 0.0049\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0048\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0049\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0035 - val_loss: 0.0046\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0052\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0045\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0055\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.0043\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0048\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0047\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0050\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0034 - val_loss: 0.0051\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0044\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.0050\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0033 - val_loss: 0.0049\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0037 - val_loss: 0.0048\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0028 - val_loss: 0.0047\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0038 - val_loss: 0.0047\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0045\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0034 - val_loss: 0.0048\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0033 - val_loss: 0.0048\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0039 - val_loss: 0.0044\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0033 - val_loss: 0.0058\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0030 - val_loss: 0.0043\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0031 - val_loss: 0.0045\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0031 - val_loss: 0.0046\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0032 - val_loss: 0.0047\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0030 - val_loss: 0.0044\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0032 - val_loss: 0.0049\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0030 - val_loss: 0.0044\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0048\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0043\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0027 - val_loss: 0.0047\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0029 - val_loss: 0.0044\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0044\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0044\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0041\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0042\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0044\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0044\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0039\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0049\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0039\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0052\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0030 - val_loss: 0.0038\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0047\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0039\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0040\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0041\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0040\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0045\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0039\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0031 - val_loss: 0.0039\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0039\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0038\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0040\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0039\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0042\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0040\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0042\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0039\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0036\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0036\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0036\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0027 - val_loss: 0.0038\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0036\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0028 - val_loss: 0.0042\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0026 - val_loss: 0.0035\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0041\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0035\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0036\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0026 - val_loss: 0.0034\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0025 - val_loss: 0.0036\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0035\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0038\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0038\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0033\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025 - val_loss: 0.0034\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0033\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0026 - val_loss: 0.0032\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1250.4500000000003, Last: 1296.8287517666818\n",
            "\t\t Buying 0.03783345944959814 units of gold on 2018-07-01 00:00:00\n",
            "\t\t Price: 1250.45, unit: 47.30884936875\n",
            "2017-08-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 0.6265 - val_loss: 0.5478\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.5932 - val_loss: 0.5093\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5154 - val_loss: 0.4756\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4973 - val_loss: 0.4481\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.4810 - val_loss: 0.4218\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4390 - val_loss: 0.3967\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3958 - val_loss: 0.3722\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3811 - val_loss: 0.3479\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3917 - val_loss: 0.3236\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3314 - val_loss: 0.2996\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3135 - val_loss: 0.2751\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2981 - val_loss: 0.2500\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2650 - val_loss: 0.2239\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2391 - val_loss: 0.1965\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2003 - val_loss: 0.1681\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1825 - val_loss: 0.1380\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1466 - val_loss: 0.1069\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1121 - val_loss: 0.0764\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0687 - val_loss: 0.0488\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0373 - val_loss: 0.0291\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0188 - val_loss: 0.0241\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0147 - val_loss: 0.0261\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0159 - val_loss: 0.0216\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0149 - val_loss: 0.0182\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0122 - val_loss: 0.0172\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0123 - val_loss: 0.0170\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0120 - val_loss: 0.0175\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0131 - val_loss: 0.0165\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0109 - val_loss: 0.0151\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0110 - val_loss: 0.0144\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0117 - val_loss: 0.0137\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0113 - val_loss: 0.0141\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0096 - val_loss: 0.0134\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0101 - val_loss: 0.0124\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0087 - val_loss: 0.0121\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0100 - val_loss: 0.0127\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0095 - val_loss: 0.0117\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0089 - val_loss: 0.0109\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0095 - val_loss: 0.0105\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0084 - val_loss: 0.0107\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0095 - val_loss: 0.0099\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0085 - val_loss: 0.0093\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0090 - val_loss: 0.0095\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0085 - val_loss: 0.0095\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0077 - val_loss: 0.0089\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0075 - val_loss: 0.0084\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0072 - val_loss: 0.0081\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0070 - val_loss: 0.0080\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0064 - val_loss: 0.0076\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0064 - val_loss: 0.0074\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0068 - val_loss: 0.0071\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0069 - val_loss: 0.0069\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0070 - val_loss: 0.0067\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0059 - val_loss: 0.0065\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0069 - val_loss: 0.0068\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0059 - val_loss: 0.0062\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0061 - val_loss: 0.0064\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0057 - val_loss: 0.0059\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0059 - val_loss: 0.0059\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0055 - val_loss: 0.0058\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0053 - val_loss: 0.0057\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0050 - val_loss: 0.0058\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0052 - val_loss: 0.0057\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0049 - val_loss: 0.0055\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0047 - val_loss: 0.0056\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0050 - val_loss: 0.0056\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0046 - val_loss: 0.0054\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0054\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0048 - val_loss: 0.0054\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0046 - val_loss: 0.0052\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0049 - val_loss: 0.0055\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0049 - val_loss: 0.0052\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0049 - val_loss: 0.0051\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0045 - val_loss: 0.0052\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0052 - val_loss: 0.0052\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0040 - val_loss: 0.0050\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0044 - val_loss: 0.0050\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0041 - val_loss: 0.0050\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0045 - val_loss: 0.0054\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0051 - val_loss: 0.0049\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0048 - val_loss: 0.0048\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0043 - val_loss: 0.0050\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0046 - val_loss: 0.0050\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0046 - val_loss: 0.0047\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0045 - val_loss: 0.0047\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0049 - val_loss: 0.0049\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0049 - val_loss: 0.0048\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0044 - val_loss: 0.0047\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0044 - val_loss: 0.0047\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0047\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0047\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0046\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0047 - val_loss: 0.0045\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0045\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0045\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0044 - val_loss: 0.0046\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0045\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0041 - val_loss: 0.0045\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0046\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0044\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0040 - val_loss: 0.0044\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0043\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0041 - val_loss: 0.0045\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0043\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0038 - val_loss: 0.0043\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0043\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0042 - val_loss: 0.0043\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0043 - val_loss: 0.0043\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0038 - val_loss: 0.0043\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0039 - val_loss: 0.0041\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0042\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0041\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0041\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0042\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0040\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0045 - val_loss: 0.0040\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0040\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0040 - val_loss: 0.0043\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0040\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0038 - val_loss: 0.0039\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0042\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0045 - val_loss: 0.0039\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0039\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0042 - val_loss: 0.0039\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0040 - val_loss: 0.0040\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0041 - val_loss: 0.0039\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0040 - val_loss: 0.0038\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0039\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0038 - val_loss: 0.0040\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0038\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0038\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0038\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0040\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0039\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0037\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0040 - val_loss: 0.0037\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0037 - val_loss: 0.0038\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0040 - val_loss: 0.0038\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0041 - val_loss: 0.0037\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0037\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0039\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0037\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0036\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0036\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0038\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0038 - val_loss: 0.0037\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0036\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0038 - val_loss: 0.0037\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0040 - val_loss: 0.0036\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0039 - val_loss: 0.0037\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0036\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0039 - val_loss: 0.0037\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0042 - val_loss: 0.0036\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0037\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0035 - val_loss: 0.0036\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0038 - val_loss: 0.0036\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0035 - val_loss: 0.0036\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038 - val_loss: 0.0036\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0036 - val_loss: 0.0036\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0036 - val_loss: 0.0038\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0040 - val_loss: 0.0035\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0038 - val_loss: 0.0035\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0038 - val_loss: 0.0038\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038 - val_loss: 0.0037\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0038 - val_loss: 0.0035\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0037 - val_loss: 0.0036\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0031 - val_loss: 0.0036\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0041 - val_loss: 0.0036\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0042 - val_loss: 0.0035\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0034 - val_loss: 0.0035\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0036\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0035\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0036\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0035\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0035\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0036\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0040 - val_loss: 0.0035\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0034\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0035\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0035\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0035\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0035\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0033 - val_loss: 0.0035\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0034\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0035 - val_loss: 0.0034\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0035\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0035\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0035 - val_loss: 0.0034\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0034\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0035 - val_loss: 0.0035\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0034\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0034\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0035 - val_loss: 0.0033\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1219.0, Last: 1280.7270038723946\n",
            "\t\t Buying 0.03684967389305015 units of gold on 2018-08-01 00:00:00\n",
            "\t\t Price: 1219.0, unit: 44.91975247562813\n",
            "2017-09-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 0.8376 - val_loss: 0.4197\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.7831 - val_loss: 0.3776\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.7152 - val_loss: 0.3405\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.6464 - val_loss: 0.3078\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.5796 - val_loss: 0.2784\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.5367 - val_loss: 0.2516\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.5022 - val_loss: 0.2269\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4439 - val_loss: 0.2043\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4094 - val_loss: 0.1832\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3634 - val_loss: 0.1633\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3300 - val_loss: 0.1443\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2848 - val_loss: 0.1263\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2662 - val_loss: 0.1095\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2251 - val_loss: 0.0942\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2031 - val_loss: 0.0796\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1678 - val_loss: 0.0660\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1309 - val_loss: 0.0531\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1136 - val_loss: 0.0410\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0856 - val_loss: 0.0304\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0646 - val_loss: 0.0216\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0360 - val_loss: 0.0157\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0206 - val_loss: 0.0134\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0097 - val_loss: 0.0145\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0065 - val_loss: 0.0171\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0062 - val_loss: 0.0185\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0063 - val_loss: 0.0183\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0065 - val_loss: 0.0173\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0059 - val_loss: 0.0164\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0059 - val_loss: 0.0160\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0053 - val_loss: 0.0159\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0052 - val_loss: 0.0161\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0058 - val_loss: 0.0164\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0055 - val_loss: 0.0167\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0052 - val_loss: 0.0166\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0054 - val_loss: 0.0166\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0058 - val_loss: 0.0164\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0061 - val_loss: 0.0165\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0056 - val_loss: 0.0164\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0058 - val_loss: 0.0164\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0054 - val_loss: 0.0165\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0058 - val_loss: 0.0165\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0054 - val_loss: 0.0166\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0054 - val_loss: 0.0167\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0057 - val_loss: 0.0167\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0052 - val_loss: 0.0169\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0053 - val_loss: 0.0168\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0058 - val_loss: 0.0168\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0053 - val_loss: 0.0165\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0055 - val_loss: 0.0165\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0055 - val_loss: 0.0168\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0059 - val_loss: 0.0170\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0054 - val_loss: 0.0169\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0055 - val_loss: 0.0165\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0054 - val_loss: 0.0164\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0054 - val_loss: 0.0166\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0057 - val_loss: 0.0165\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0058 - val_loss: 0.0164\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0056 - val_loss: 0.0163\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0052 - val_loss: 0.0164\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0058 - val_loss: 0.0165\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0056 - val_loss: 0.0164\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0049 - val_loss: 0.0166\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0054 - val_loss: 0.0171\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0053 - val_loss: 0.0162\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0052 - val_loss: 0.0163\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0051 - val_loss: 0.0161\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0049 - val_loss: 0.0159\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0053 - val_loss: 0.0163\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0052 - val_loss: 0.0160\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0055 - val_loss: 0.0164\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0054 - val_loss: 0.0160\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0054 - val_loss: 0.0155\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0054 - val_loss: 0.0165\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0048 - val_loss: 0.0158\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0048 - val_loss: 0.0160\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0047 - val_loss: 0.0158\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0051 - val_loss: 0.0161\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0050 - val_loss: 0.0150\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0044 - val_loss: 0.0155\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0049 - val_loss: 0.0162\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0052 - val_loss: 0.0154\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0050 - val_loss: 0.0144\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0054 - val_loss: 0.0148\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0048 - val_loss: 0.0153\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0047 - val_loss: 0.0143\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0048 - val_loss: 0.0146\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0047 - val_loss: 0.0148\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0047 - val_loss: 0.0139\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0049 - val_loss: 0.0144\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0046 - val_loss: 0.0133\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0047 - val_loss: 0.0138\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0047 - val_loss: 0.0133\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0044 - val_loss: 0.0134\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0043 - val_loss: 0.0126\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0046 - val_loss: 0.0130\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0048 - val_loss: 0.0122\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0044 - val_loss: 0.0118\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0039 - val_loss: 0.0126\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0043 - val_loss: 0.0114\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0039 - val_loss: 0.0121\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0047 - val_loss: 0.0125\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0046 - val_loss: 0.0107\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0041 - val_loss: 0.0132\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - val_loss: 0.0099\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0038 - val_loss: 0.0118\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0043 - val_loss: 0.0118\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0097\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0114\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - val_loss: 0.0101\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0097\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0040 - val_loss: 0.0102\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0092\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0096\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0089\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0039 - val_loss: 0.0092\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0087\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0090\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0078\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035 - val_loss: 0.0091\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0075\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0041 - val_loss: 0.0084\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0038 - val_loss: 0.0073\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0079\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0070\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0037 - val_loss: 0.0075\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0069\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0074\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0066\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0076\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0040 - val_loss: 0.0073\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0038 - val_loss: 0.0065\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0078\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0035 - val_loss: 0.0061\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0067\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0062\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0038 - val_loss: 0.0063\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0064\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0055\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0061\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0040 - val_loss: 0.0054\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0056\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0057\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0058\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0053\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0057\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0030 - val_loss: 0.0054\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0057\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0057\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0049\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0060\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0033 - val_loss: 0.0045\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0035 - val_loss: 0.0055\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0048\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0052\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0035 - val_loss: 0.0057\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.0046\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0057\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0049\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0046\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0048\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0049\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0048\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0046\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0046\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0048\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0046\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0045\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0048\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0047\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0030 - val_loss: 0.0046\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0029 - val_loss: 0.0047\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0030 - val_loss: 0.0046\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0047\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0030 - val_loss: 0.0046\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0028 - val_loss: 0.0048\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0030 - val_loss: 0.0047\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0031 - val_loss: 0.0047\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0030 - val_loss: 0.0047\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.0048\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0031 - val_loss: 0.0049\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0032 - val_loss: 0.0047\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0028 - val_loss: 0.0050\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0031 - val_loss: 0.0046\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0027 - val_loss: 0.0051\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0029 - val_loss: 0.0049\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0029 - val_loss: 0.0048\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0030 - val_loss: 0.0049\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0032 - val_loss: 0.0049\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0028 - val_loss: 0.0048\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0051\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0048\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0050\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0049\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0051\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0050\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0050\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0029 - val_loss: 0.0050\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0051\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0032 - val_loss: 0.0051\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0051\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t Selling 0.035470335544603854 units of gold on 2018-09-01 00:00:00\n",
            "\t\t Price: 1202.45, unit: 42.65130497560891\n",
            "2017-10-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 1.0240 - val_loss: 0.2381\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.8716 - val_loss: 0.2017\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.7591 - val_loss: 0.1696\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.6761 - val_loss: 0.1412\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.5826 - val_loss: 0.1163\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4656 - val_loss: 0.0927\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3849 - val_loss: 0.0707\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3149 - val_loss: 0.0512\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2179 - val_loss: 0.0353\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1467 - val_loss: 0.0244\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0767 - val_loss: 0.0209\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0284 - val_loss: 0.0260\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0054 - val_loss: 0.0324\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0064 - val_loss: 0.0269\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0048 - val_loss: 0.0199\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0050 - val_loss: 0.0212\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0232\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0041 - val_loss: 0.0215\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0039 - val_loss: 0.0210\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0044 - val_loss: 0.0210\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0208\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0038 - val_loss: 0.0208\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0049 - val_loss: 0.0207\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0193\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0039 - val_loss: 0.0203\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0037 - val_loss: 0.0202\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0039 - val_loss: 0.0191\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0040 - val_loss: 0.0203\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0037 - val_loss: 0.0193\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0037 - val_loss: 0.0180\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0038 - val_loss: 0.0195\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036 - val_loss: 0.0194\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0042 - val_loss: 0.0184\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0043 - val_loss: 0.0181\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0045 - val_loss: 0.0183\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0040 - val_loss: 0.0183\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0036 - val_loss: 0.0177\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0042 - val_loss: 0.0183\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0038 - val_loss: 0.0173\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0037 - val_loss: 0.0172\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0035 - val_loss: 0.0174\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0038 - val_loss: 0.0167\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0164\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0171\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0162\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0158\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.0158\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0164\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0037 - val_loss: 0.0154\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0157\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0037 - val_loss: 0.0151\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0152\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0037 - val_loss: 0.0151\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0145\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0156\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0150\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0149\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0150\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0144\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038 - val_loss: 0.0145\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0035 - val_loss: 0.0153\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0144\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0133\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0150\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0138\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0036 - val_loss: 0.0137\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0143\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0136\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0135\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0135\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0133\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0133\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0127\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0133\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0130\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0038 - val_loss: 0.0121\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0131\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0121\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0034 - val_loss: 0.0126\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0122\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0126\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0118\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0121\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0122\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - val_loss: 0.0116\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0120\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0122\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0118\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0121\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0116\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0032 - val_loss: 0.0111\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0117\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0116\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0112\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0109\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0113\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0116\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0031 - val_loss: 0.0108\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0107\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0034 - val_loss: 0.0116\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0028 - val_loss: 0.0106\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0106\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0110\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0105\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0102\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0028 - val_loss: 0.0102\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0027 - val_loss: 0.0100\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0030 - val_loss: 0.0099\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0029 - val_loss: 0.0096\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0030 - val_loss: 0.0093\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0023 - val_loss: 0.0090\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0029 - val_loss: 0.0093\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0027 - val_loss: 0.0085\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0026 - val_loss: 0.0091\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0026 - val_loss: 0.0084\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0028 - val_loss: 0.0085\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0029 - val_loss: 0.0082\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0031 - val_loss: 0.0079\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0028 - val_loss: 0.0075\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0031 - val_loss: 0.0078\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0026 - val_loss: 0.0074\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0076\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0027 - val_loss: 0.0074\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0073\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0071\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0070\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0071\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0070\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0067\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0066\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0065\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0065\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0065\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0061\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0023 - val_loss: 0.0066\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0061\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0064\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0058\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0062\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0067\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0053\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0030 - val_loss: 0.0059\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.0056\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0056\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0056\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0051\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0057\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0050\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0057\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0024 - val_loss: 0.0050\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0052\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0051\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.0049\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0050\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0046\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0050\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0049\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0045\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0047\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0044\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0047\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0023 - val_loss: 0.0045\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0046\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0043\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0041\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0041\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0042\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0042\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0024 - val_loss: 0.0041\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0023 - val_loss: 0.0040\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0039\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0037\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0037\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0038\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.0038\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0022 - val_loss: 0.0036\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0041\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0023 - val_loss: 0.0035\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0037\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0038\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0034\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0040\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0034\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0037\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0035\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0037\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.0035\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0026 - val_loss: 0.0036\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0023 - val_loss: 0.0032\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0021 - val_loss: 0.0039\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.0031\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0021 - val_loss: 0.0034\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0019 - val_loss: 0.0039\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0020 - val_loss: 0.0030\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0020 - val_loss: 0.0038\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0021 - val_loss: 0.0032\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0020 - val_loss: 0.0033\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0020 - val_loss: 0.0033\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0022 - val_loss: 0.0030\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0022 - val_loss: 0.0032\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1189.35, Last: 1249.9654474362733\n",
            "\t\t Buying 0.037636141230000884 units of gold on 2018-10-01 00:00:00\n",
            "\t\t Price: 1189.35, unit: 44.76254457190155\n",
            "2017-11-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.5992 - val_loss: 0.0562\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.5243 - val_loss: 0.0478\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4898 - val_loss: 0.0406\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4490 - val_loss: 0.0342\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4188 - val_loss: 0.0286\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3765 - val_loss: 0.0236\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3304 - val_loss: 0.0193\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2933 - val_loss: 0.0156\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2505 - val_loss: 0.0126\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2168 - val_loss: 0.0103\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1805 - val_loss: 0.0087\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1467 - val_loss: 0.0080\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1049 - val_loss: 0.0084\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0719 - val_loss: 0.0100\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0424 - val_loss: 0.0129\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0216 - val_loss: 0.0169\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0079 - val_loss: 0.0205\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0052 - val_loss: 0.0231\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0058 - val_loss: 0.0236\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0063 - val_loss: 0.0227\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0052 - val_loss: 0.0218\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0056 - val_loss: 0.0213\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0051 - val_loss: 0.0210\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0051 - val_loss: 0.0212\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0053 - val_loss: 0.0215\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0050 - val_loss: 0.0217\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0049 - val_loss: 0.0219\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0050 - val_loss: 0.0220\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0049 - val_loss: 0.0221\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0046 - val_loss: 0.0220\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0045 - val_loss: 0.0219\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0050 - val_loss: 0.0220\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0047 - val_loss: 0.0222\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0051 - val_loss: 0.0225\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0050 - val_loss: 0.0226\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0226\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0045 - val_loss: 0.0227\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0045 - val_loss: 0.0226\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0044 - val_loss: 0.0227\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0052 - val_loss: 0.0229\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0054 - val_loss: 0.0230\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0046 - val_loss: 0.0231\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0045 - val_loss: 0.0230\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0047 - val_loss: 0.0232\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0048 - val_loss: 0.0232\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0044 - val_loss: 0.0233\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0050 - val_loss: 0.0233\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045 - val_loss: 0.0235\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0051 - val_loss: 0.0236\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0046 - val_loss: 0.0236\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0043 - val_loss: 0.0236\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0048 - val_loss: 0.0237\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0051 - val_loss: 0.0237\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0043 - val_loss: 0.0238\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0047 - val_loss: 0.0241\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0046 - val_loss: 0.0243\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0047 - val_loss: 0.0241\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0049 - val_loss: 0.0240\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0051 - val_loss: 0.0241\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0050 - val_loss: 0.0246\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0244\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0048 - val_loss: 0.0244\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0043 - val_loss: 0.0244\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0049 - val_loss: 0.0245\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0245\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0045 - val_loss: 0.0246\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0043 - val_loss: 0.0247\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0047 - val_loss: 0.0247\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0047 - val_loss: 0.0249\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0051 - val_loss: 0.0248\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0247\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0044 - val_loss: 0.0250\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0046 - val_loss: 0.0249\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0040 - val_loss: 0.0248\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0047 - val_loss: 0.0250\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0048 - val_loss: 0.0253\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0047 - val_loss: 0.0253\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0045 - val_loss: 0.0254\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0046 - val_loss: 0.0253\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0042 - val_loss: 0.0254\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0048 - val_loss: 0.0254\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0255\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0048 - val_loss: 0.0259\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0047 - val_loss: 0.0261\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0046 - val_loss: 0.0259\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0043 - val_loss: 0.0254\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0045 - val_loss: 0.0255\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0049 - val_loss: 0.0259\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0045 - val_loss: 0.0261\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0052 - val_loss: 0.0262\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0044 - val_loss: 0.0261\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0042 - val_loss: 0.0260\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0048 - val_loss: 0.0262\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0048 - val_loss: 0.0264\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0043 - val_loss: 0.0262\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0047 - val_loss: 0.0259\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0046 - val_loss: 0.0262\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0047 - val_loss: 0.0262\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0048 - val_loss: 0.0265\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0266\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0048 - val_loss: 0.0266\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045 - val_loss: 0.0264\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0042 - val_loss: 0.0265\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0050 - val_loss: 0.0266\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0266\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0047 - val_loss: 0.0268\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0046 - val_loss: 0.0268\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0043 - val_loss: 0.0267\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0043 - val_loss: 0.0268\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0041 - val_loss: 0.0270\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0269\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0042 - val_loss: 0.0270\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0046 - val_loss: 0.0270\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0046 - val_loss: 0.0271\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0046 - val_loss: 0.0270\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0045 - val_loss: 0.0267\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0044 - val_loss: 0.0267\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0044 - val_loss: 0.0271\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0046 - val_loss: 0.0275\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0044 - val_loss: 0.0270\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0045 - val_loss: 0.0267\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0044 - val_loss: 0.0272\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0046 - val_loss: 0.0270\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0045 - val_loss: 0.0270\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0048 - val_loss: 0.0271\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0042 - val_loss: 0.0274\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0046 - val_loss: 0.0271\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0044 - val_loss: 0.0271\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0044 - val_loss: 0.0274\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0041 - val_loss: 0.0274\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0045 - val_loss: 0.0276\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0041 - val_loss: 0.0272\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0046 - val_loss: 0.0270\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0043 - val_loss: 0.0274\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0047 - val_loss: 0.0276\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0046 - val_loss: 0.0278\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0043 - val_loss: 0.0278\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0048 - val_loss: 0.0274\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0045 - val_loss: 0.0276\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0045 - val_loss: 0.0275\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0279\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0046 - val_loss: 0.0277\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0278\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0042 - val_loss: 0.0274\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0272\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0040 - val_loss: 0.0276\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0042 - val_loss: 0.0279\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0045 - val_loss: 0.0279\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0041 - val_loss: 0.0277\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0276\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0042 - val_loss: 0.0278\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0047 - val_loss: 0.0279\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0044 - val_loss: 0.0279\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0046 - val_loss: 0.0275\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0044 - val_loss: 0.0277\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0045 - val_loss: 0.0275\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0043 - val_loss: 0.0275\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0280\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0042 - val_loss: 0.0279\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0042 - val_loss: 0.0278\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0040 - val_loss: 0.0278\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0276\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0042 - val_loss: 0.0280\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0044 - val_loss: 0.0278\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0045 - val_loss: 0.0277\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0043 - val_loss: 0.0278\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0276\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0041 - val_loss: 0.0274\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0043 - val_loss: 0.0279\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0281\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0275\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0045 - val_loss: 0.0273\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0044 - val_loss: 0.0278\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0279\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0045 - val_loss: 0.0280\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0042 - val_loss: 0.0277\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0278\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0278\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0043 - val_loss: 0.0276\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0043 - val_loss: 0.0278\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0282\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0046 - val_loss: 0.0285\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0039 - val_loss: 0.0278\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0280\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0041 - val_loss: 0.0280\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0278\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0281\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0277\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0040 - val_loss: 0.0278\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0041 - val_loss: 0.0278\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0041 - val_loss: 0.0279\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0041 - val_loss: 0.0280\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0280\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0041 - val_loss: 0.0282\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0280\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0040 - val_loss: 0.0281\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0276\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0278\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0041 - val_loss: 0.0282\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0044 - val_loss: 0.0278\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1231.15, Last: 1284.172850739956\n",
            "\t\t Buying 0.03452222399465582 units of gold on 2018-11-01 00:00:00\n",
            "\t\t Price: 1231.15, unit: 42.50203607102052\n",
            "2017-12-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.4779 - val_loss: 0.0205\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3493 - val_loss: 0.0122\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2597 - val_loss: 0.0073\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1683 - val_loss: 0.0045\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1220 - val_loss: 0.0033\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0760 - val_loss: 0.0035\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0419 - val_loss: 0.0048\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0198 - val_loss: 0.0070\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0071 - val_loss: 0.0094\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0037 - val_loss: 0.0112\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0119\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0042 - val_loss: 0.0118\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0043 - val_loss: 0.0112\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0107\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0104\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - val_loss: 0.0103\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0103\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0035 - val_loss: 0.0104\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0104\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0105\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0106\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0036 - val_loss: 0.0107\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0105\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0105\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0105\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0107\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0107\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0037 - val_loss: 0.0106\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0105\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0036 - val_loss: 0.0105\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0038 - val_loss: 0.0106\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0106\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0107\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038 - val_loss: 0.0109\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0035 - val_loss: 0.0109\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0037 - val_loss: 0.0108\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0107\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0106\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0038 - val_loss: 0.0106\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0035 - val_loss: 0.0106\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0107\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0107\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0107\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0107\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0107\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0108\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0037 - val_loss: 0.0107\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0106\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0037 - val_loss: 0.0107\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0040 - val_loss: 0.0107\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0040 - val_loss: 0.0108\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0036 - val_loss: 0.0108\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0036 - val_loss: 0.0107\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0036 - val_loss: 0.0109\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0037 - val_loss: 0.0108\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0032 - val_loss: 0.0107\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0033 - val_loss: 0.0106\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0032 - val_loss: 0.0107\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0034 - val_loss: 0.0109\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - val_loss: 0.0109\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0036 - val_loss: 0.0107\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0038 - val_loss: 0.0106\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0039 - val_loss: 0.0107\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0040 - val_loss: 0.0109\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0036 - val_loss: 0.0109\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0032 - val_loss: 0.0108\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0032 - val_loss: 0.0106\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0034 - val_loss: 0.0107\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0109\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0109\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036 - val_loss: 0.0108\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0108\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0107\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0108\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0108\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0108\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038 - val_loss: 0.0107\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0106\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0105\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0108\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0109\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038 - val_loss: 0.0109\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.0109\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0106\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0035 - val_loss: 0.0106\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0108\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0107\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0109\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0108\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0034 - val_loss: 0.0109\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0108\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0035 - val_loss: 0.0108\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0106\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0107\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0110\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0033 - val_loss: 0.0109\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0107\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0107\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0108\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0035 - val_loss: 0.0109\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0109\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0032 - val_loss: 0.0107\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0107\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0107\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0109\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0108\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0105\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0106\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0108\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0107\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0107\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0106\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0105\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0106\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0030 - val_loss: 0.0106\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0105\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0108\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0108\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0106\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0105\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0107\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038 - val_loss: 0.0108\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0106\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0106\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0105\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0105\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0033 - val_loss: 0.0105\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0033 - val_loss: 0.0104\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0104\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0106\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0031 - val_loss: 0.0106\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0032 - val_loss: 0.0103\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0034 - val_loss: 0.0104\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0035 - val_loss: 0.0105\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0033 - val_loss: 0.0107\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0036 - val_loss: 0.0107\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0035 - val_loss: 0.0102\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0031 - val_loss: 0.0102\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0034 - val_loss: 0.0105\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0033 - val_loss: 0.0105\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0030 - val_loss: 0.0105\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0033 - val_loss: 0.0104\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0029 - val_loss: 0.0102\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0036 - val_loss: 0.0105\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0037 - val_loss: 0.0105\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - val_loss: 0.0103\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0034 - val_loss: 0.0104\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0105\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0105\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.0103\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0101\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0103\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0103\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0035 - val_loss: 0.0104\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0104\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0032 - val_loss: 0.0104\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0104\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0103\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0104\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0104\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0102\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0103\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0104\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0103\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0103\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0101\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0102\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0106\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0030 - val_loss: 0.0104\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0102\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0101\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0102\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0103\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0104\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0100\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0100\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0100\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0029 - val_loss: 0.0103\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0101\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0101\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0101\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0103\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0100\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0100\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0102\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0101\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0032 - val_loss: 0.0100\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0099\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0025 - val_loss: 0.0100\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0032 - val_loss: 0.0102\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.0098\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0097\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.0099\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0029 - val_loss: 0.0099\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0099\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0097\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0032 - val_loss: 0.0096\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0099\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0101\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0032 - val_loss: 0.0099\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1217.55, Last: 1273.1880926281215\n",
            "\t\t Buying 0.0331449905543378 units of gold on 2018-12-01 00:00:00\n",
            "\t\t Price: 1217.55, unit: 40.35568324943398\n",
            "2018-01-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - loss: 0.5815 - val_loss: 0.0688\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.5655 - val_loss: 0.0617\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.5471 - val_loss: 0.0554\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.4840 - val_loss: 0.0496\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4710 - val_loss: 0.0444\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4131 - val_loss: 0.0396\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4107 - val_loss: 0.0350\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3882 - val_loss: 0.0306\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3446 - val_loss: 0.0263\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3219 - val_loss: 0.0222\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2853 - val_loss: 0.0182\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2477 - val_loss: 0.0146\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2063 - val_loss: 0.0117\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1606 - val_loss: 0.0099\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1136 - val_loss: 0.0103\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0530 - val_loss: 0.0143\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0098 - val_loss: 0.0220\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0157 - val_loss: 0.0186\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0073 - val_loss: 0.0166\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0083 - val_loss: 0.0168\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0078 - val_loss: 0.0178\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0071 - val_loss: 0.0181\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0067 - val_loss: 0.0178\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0069 - val_loss: 0.0175\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0068 - val_loss: 0.0173\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0071 - val_loss: 0.0171\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0066 - val_loss: 0.0170\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0064 - val_loss: 0.0166\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0066 - val_loss: 0.0164\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0064 - val_loss: 0.0164\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0067 - val_loss: 0.0164\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0066 - val_loss: 0.0161\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0064 - val_loss: 0.0159\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0065 - val_loss: 0.0155\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0063 - val_loss: 0.0154\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0064 - val_loss: 0.0153\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0069 - val_loss: 0.0152\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0066 - val_loss: 0.0150\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0063 - val_loss: 0.0147\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0062 - val_loss: 0.0148\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0060 - val_loss: 0.0146\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0060 - val_loss: 0.0141\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0065 - val_loss: 0.0140\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0062 - val_loss: 0.0141\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0062 - val_loss: 0.0138\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0062 - val_loss: 0.0136\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0058 - val_loss: 0.0135\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0056 - val_loss: 0.0133\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0056 - val_loss: 0.0133\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0060 - val_loss: 0.0132\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0054 - val_loss: 0.0130\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0063 - val_loss: 0.0130\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0062 - val_loss: 0.0128\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0055 - val_loss: 0.0127\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0056 - val_loss: 0.0125\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0058 - val_loss: 0.0124\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0059 - val_loss: 0.0122\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0055 - val_loss: 0.0122\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0055 - val_loss: 0.0122\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0061 - val_loss: 0.0119\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0063 - val_loss: 0.0118\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0053 - val_loss: 0.0117\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0058 - val_loss: 0.0116\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0063 - val_loss: 0.0116\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0060 - val_loss: 0.0113\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0055 - val_loss: 0.0113\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0058 - val_loss: 0.0112\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0049 - val_loss: 0.0110\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0057 - val_loss: 0.0111\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0057 - val_loss: 0.0111\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0060 - val_loss: 0.0109\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0061 - val_loss: 0.0107\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0064 - val_loss: 0.0107\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0057 - val_loss: 0.0107\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0054 - val_loss: 0.0106\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0062 - val_loss: 0.0105\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0058 - val_loss: 0.0104\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0060 - val_loss: 0.0104\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0057 - val_loss: 0.0104\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0058 - val_loss: 0.0102\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0057 - val_loss: 0.0102\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0051 - val_loss: 0.0102\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0053 - val_loss: 0.0100\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0060 - val_loss: 0.0099\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0059 - val_loss: 0.0101\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0054 - val_loss: 0.0099\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0057 - val_loss: 0.0097\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0056 - val_loss: 0.0098\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0049 - val_loss: 0.0097\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0053 - val_loss: 0.0097\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0059 - val_loss: 0.0095\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0053 - val_loss: 0.0095\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0051 - val_loss: 0.0097\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0052 - val_loss: 0.0095\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0056 - val_loss: 0.0093\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0048 - val_loss: 0.0095\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0058 - val_loss: 0.0094\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0051 - val_loss: 0.0093\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0049 - val_loss: 0.0093\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0051 - val_loss: 0.0093\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0054 - val_loss: 0.0092\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0047 - val_loss: 0.0090\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0058 - val_loss: 0.0092\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0054 - val_loss: 0.0090\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0045 - val_loss: 0.0089\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0053 - val_loss: 0.0090\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0050 - val_loss: 0.0089\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0052 - val_loss: 0.0089\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0050 - val_loss: 0.0088\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0049 - val_loss: 0.0088\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0053 - val_loss: 0.0087\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0052 - val_loss: 0.0088\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0051 - val_loss: 0.0087\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0045 - val_loss: 0.0085\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0053 - val_loss: 0.0087\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0056 - val_loss: 0.0085\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0049 - val_loss: 0.0085\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0051 - val_loss: 0.0085\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0051 - val_loss: 0.0085\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0054 - val_loss: 0.0084\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0052 - val_loss: 0.0083\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0047 - val_loss: 0.0083\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0047 - val_loss: 0.0084\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0055 - val_loss: 0.0083\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0051 - val_loss: 0.0083\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0057 - val_loss: 0.0083\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0048 - val_loss: 0.0081\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0058 - val_loss: 0.0082\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0048 - val_loss: 0.0082\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0046 - val_loss: 0.0082\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0050 - val_loss: 0.0081\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0050 - val_loss: 0.0081\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0051 - val_loss: 0.0081\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0053 - val_loss: 0.0080\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0048 - val_loss: 0.0080\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0051 - val_loss: 0.0080\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0051 - val_loss: 0.0079\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0048 - val_loss: 0.0079\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0052 - val_loss: 0.0080\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0045 - val_loss: 0.0078\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0046 - val_loss: 0.0079\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0049 - val_loss: 0.0078\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0051 - val_loss: 0.0078\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0053 - val_loss: 0.0077\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0048 - val_loss: 0.0077\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0047 - val_loss: 0.0077\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0050 - val_loss: 0.0077\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0050 - val_loss: 0.0076\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0047 - val_loss: 0.0075\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0047 - val_loss: 0.0075\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0051 - val_loss: 0.0075\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0044 - val_loss: 0.0075\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0044 - val_loss: 0.0074\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0044 - val_loss: 0.0074\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0044 - val_loss: 0.0073\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0049 - val_loss: 0.0075\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0053 - val_loss: 0.0073\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0048 - val_loss: 0.0073\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0049 - val_loss: 0.0074\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0046 - val_loss: 0.0072\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0044 - val_loss: 0.0072\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0048 - val_loss: 0.0073\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0049 - val_loss: 0.0071\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0046 - val_loss: 0.0071\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0045 - val_loss: 0.0070\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0054 - val_loss: 0.0072\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0044 - val_loss: 0.0070\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0048 - val_loss: 0.0069\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0048 - val_loss: 0.0071\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0050 - val_loss: 0.0069\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0044 - val_loss: 0.0068\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0045 - val_loss: 0.0069\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0051 - val_loss: 0.0069\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0047 - val_loss: 0.0067\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0045 - val_loss: 0.0067\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0046 - val_loss: 0.0068\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0050 - val_loss: 0.0067\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0066\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0051 - val_loss: 0.0068\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0044 - val_loss: 0.0067\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0066\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0045 - val_loss: 0.0067\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0043 - val_loss: 0.0066\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0066\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0049 - val_loss: 0.0067\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0045 - val_loss: 0.0065\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0045 - val_loss: 0.0066\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0066\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0045 - val_loss: 0.0065\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0040 - val_loss: 0.0064\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0042 - val_loss: 0.0065\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0043 - val_loss: 0.0065\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0044 - val_loss: 0.0063\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0044 - val_loss: 0.0065\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0047 - val_loss: 0.0063\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0041 - val_loss: 0.0063\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0045 - val_loss: 0.0063\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0044 - val_loss: 0.0062\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0062\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0044 - val_loss: 0.0063\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t Selling 0.029959125289552437 units of gold on 2019-01-01 00:00:00\n",
            "\t\t Price: 1279.0, unit: 38.317721245337566\n",
            "2018-02-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.4349 - val_loss: 0.1274\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3487 - val_loss: 0.0914\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2432 - val_loss: 0.0580\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1744 - val_loss: 0.0304\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0830 - val_loss: 0.0138\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0313 - val_loss: 0.0138\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0162 - val_loss: 0.0211\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0223 - val_loss: 0.0165\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0157 - val_loss: 0.0119\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0153 - val_loss: 0.0105\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0121 - val_loss: 0.0101\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0125 - val_loss: 0.0101\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0115 - val_loss: 0.0095\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0103 - val_loss: 0.0086\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0095 - val_loss: 0.0078\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0093 - val_loss: 0.0072\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0086 - val_loss: 0.0068\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0082 - val_loss: 0.0063\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0067 - val_loss: 0.0060\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0073 - val_loss: 0.0056\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0068 - val_loss: 0.0055\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0058 - val_loss: 0.0053\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0065 - val_loss: 0.0051\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0055 - val_loss: 0.0051\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0055 - val_loss: 0.0051\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0055 - val_loss: 0.0050\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0045 - val_loss: 0.0052\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0055 - val_loss: 0.0051\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0055 - val_loss: 0.0053\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0047 - val_loss: 0.0056\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0048 - val_loss: 0.0055\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0049 - val_loss: 0.0054\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0041 - val_loss: 0.0062\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0056\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0057\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0043 - val_loss: 0.0060\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0045 - val_loss: 0.0060\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0044 - val_loss: 0.0065\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0064\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0061\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0065\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0044 - val_loss: 0.0068\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0045 - val_loss: 0.0061\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0065\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0043 - val_loss: 0.0065\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0060\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0068\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0063\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0041 - val_loss: 0.0064\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0041 - val_loss: 0.0066\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0036 - val_loss: 0.0066\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0043 - val_loss: 0.0063\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0040 - val_loss: 0.0068\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0044 - val_loss: 0.0067\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0042 - val_loss: 0.0068\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0042 - val_loss: 0.0068\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0041 - val_loss: 0.0065\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0038 - val_loss: 0.0070\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0040 - val_loss: 0.0062\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0065\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0045 - val_loss: 0.0073\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0041 - val_loss: 0.0061\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0065\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0044 - val_loss: 0.0068\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0063\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0068\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0039 - val_loss: 0.0064\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0065\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0066\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0038 - val_loss: 0.0065\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0040 - val_loss: 0.0063\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0037 - val_loss: 0.0064\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0037 - val_loss: 0.0068\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0037 - val_loss: 0.0064\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0040 - val_loss: 0.0063\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0036 - val_loss: 0.0067\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0036 - val_loss: 0.0062\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0036 - val_loss: 0.0070\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0030 - val_loss: 0.0062\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.0034 - val_loss: 0.0061\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0034 - val_loss: 0.0070\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0034 - val_loss: 0.0064\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.0034 - val_loss: 0.0063\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0036 - val_loss: 0.0064\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0033 - val_loss: 0.0067\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0037 - val_loss: 0.0062\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0033 - val_loss: 0.0064\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 0.0038 - val_loss: 0.0065\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0036 - val_loss: 0.0068\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0036 - val_loss: 0.0062\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0037 - val_loss: 0.0062\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0036 - val_loss: 0.0064\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0040 - val_loss: 0.0061\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0032 - val_loss: 0.0061\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0035 - val_loss: 0.0063\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0033 - val_loss: 0.0062\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0035 - val_loss: 0.0065\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0040 - val_loss: 0.0062\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0037 - val_loss: 0.0061\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0034 - val_loss: 0.0068\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0035 - val_loss: 0.0061\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0037 - val_loss: 0.0059\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - val_loss: 0.0062\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0037 - val_loss: 0.0062\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0035 - val_loss: 0.0060\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0035 - val_loss: 0.0066\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0035 - val_loss: 0.0061\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0033 - val_loss: 0.0063\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0033 - val_loss: 0.0059\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0033 - val_loss: 0.0064\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0037 - val_loss: 0.0063\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0037 - val_loss: 0.0061\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0036 - val_loss: 0.0060\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0035 - val_loss: 0.0062\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0032 - val_loss: 0.0059\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0033 - val_loss: 0.0062\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0042 - val_loss: 0.0059\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0034 - val_loss: 0.0060\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0036 - val_loss: 0.0063\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0033 - val_loss: 0.0061\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - val_loss: 0.0062\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0033 - val_loss: 0.0062\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0033 - val_loss: 0.0059\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0034 - val_loss: 0.0061\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0036 - val_loss: 0.0061\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0035 - val_loss: 0.0063\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0032 - val_loss: 0.0065\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0034 - val_loss: 0.0059\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0035 - val_loss: 0.0057\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0031 - val_loss: 0.0062\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0032 - val_loss: 0.0060\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0033 - val_loss: 0.0056\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0031 - val_loss: 0.0066\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - val_loss: 0.0059\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0057\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0062\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0061\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0061\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0029 - val_loss: 0.0062\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - val_loss: 0.0063\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0058\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0062\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0061\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0058\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0063\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.0059\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0028 - val_loss: 0.0058\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0061\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0055\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0063\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0061\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0058\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036 - val_loss: 0.0061\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0057\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0057\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0059\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0055\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0062\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0054\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0033 - val_loss: 0.0060\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0058\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0059\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0055\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0059\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0059\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0060\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0031 - val_loss: 0.0056\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0057\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0060\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0057\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0030 - val_loss: 0.0059\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0027 - val_loss: 0.0053\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0030 - val_loss: 0.0059\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0031 - val_loss: 0.0057\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0033 - val_loss: 0.0058\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0033 - val_loss: 0.0060\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0032 - val_loss: 0.0056\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.0056\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0031 - val_loss: 0.0059\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0029 - val_loss: 0.0057\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0028 - val_loss: 0.0055\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0031 - val_loss: 0.0057\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0028 - val_loss: 0.0056\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0028 - val_loss: 0.0056\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0029 - val_loss: 0.0056\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0031 - val_loss: 0.0053\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0032 - val_loss: 0.0056\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0029 - val_loss: 0.0058\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0031 - val_loss: 0.0050\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0060\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0053\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0054\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0029 - val_loss: 0.0052\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0053\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0055\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0029 - val_loss: 0.0052\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0055\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0054\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0052\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0059\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t Selling 0.03049552471902766 units of gold on 2019-02-01 00:00:00\n",
            "\t\t Price: 1318.7, unit: 40.21444844698178\n",
            "2018-03-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.2098 - val_loss: 0.1818\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1897 - val_loss: 0.1480\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1451 - val_loss: 0.1149\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1213 - val_loss: 0.0836\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0965 - val_loss: 0.0548\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0623 - val_loss: 0.0311\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0290 - val_loss: 0.0147\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0163 - val_loss: 0.0074\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0133 - val_loss: 0.0067\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0140 - val_loss: 0.0062\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0113 - val_loss: 0.0063\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0105 - val_loss: 0.0070\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0097 - val_loss: 0.0070\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0087 - val_loss: 0.0062\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0083 - val_loss: 0.0056\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0075 - val_loss: 0.0053\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0082 - val_loss: 0.0055\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0069 - val_loss: 0.0055\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0069 - val_loss: 0.0057\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0057 - val_loss: 0.0053\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0054 - val_loss: 0.0053\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0055 - val_loss: 0.0057\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0052 - val_loss: 0.0058\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0049 - val_loss: 0.0056\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0048 - val_loss: 0.0062\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0045 - val_loss: 0.0060\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0046 - val_loss: 0.0058\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0046 - val_loss: 0.0064\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0043 - val_loss: 0.0061\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0065\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0046 - val_loss: 0.0064\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0063\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0065\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0068\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0069\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0069\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0069\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0038 - val_loss: 0.0072\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0038 - val_loss: 0.0065\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0064\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - val_loss: 0.0078\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0041 - val_loss: 0.0073\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0070\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0071\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0073\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0072\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0036 - val_loss: 0.0074\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0038 - val_loss: 0.0071\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0040 - val_loss: 0.0074\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0072\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0071\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0076\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0077\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0031 - val_loss: 0.0060\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0072\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0075\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0037 - val_loss: 0.0071\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0068\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0070\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0036 - val_loss: 0.0076\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0066\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0039 - val_loss: 0.0069\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0031 - val_loss: 0.0080\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0067\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0064\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0038 - val_loss: 0.0079\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0034 - val_loss: 0.0066\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0065\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0069\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0071\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0040 - val_loss: 0.0067\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0035 - val_loss: 0.0067\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0065\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0036 - val_loss: 0.0071\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0072\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0060\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0075\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0032 - val_loss: 0.0069\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0035 - val_loss: 0.0061\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0035 - val_loss: 0.0067\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0071\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0059\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0068\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0066\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0038 - val_loss: 0.0060\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0028 - val_loss: 0.0072\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0069\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0061\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0064\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0071\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0027 - val_loss: 0.0067\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0035 - val_loss: 0.0061\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0028 - val_loss: 0.0068\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0030 - val_loss: 0.0066\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0033 - val_loss: 0.0067\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0035 - val_loss: 0.0064\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0032 - val_loss: 0.0070\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0031 - val_loss: 0.0063\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0032 - val_loss: 0.0073\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0065\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0030 - val_loss: 0.0062\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0028 - val_loss: 0.0066\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0029 - val_loss: 0.0067\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0032 - val_loss: 0.0065\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0030 - val_loss: 0.0062\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0032 - val_loss: 0.0062\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0032 - val_loss: 0.0067\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0029 - val_loss: 0.0065\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0031 - val_loss: 0.0057\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0033 - val_loss: 0.0069\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0066\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0061\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0066\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0062\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0028 - val_loss: 0.0069\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0061\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0032 - val_loss: 0.0062\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0064\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0063\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.0058\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - val_loss: 0.0063\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0066\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0062\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0063\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0059\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0032 - val_loss: 0.0061\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0062\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.0064\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0060\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.0062\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0059\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0027 - val_loss: 0.0063\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0030 - val_loss: 0.0058\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0031 - val_loss: 0.0066\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0055\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0062\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0060\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0060\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0029 - val_loss: 0.0065\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0058\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0055\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0032 - val_loss: 0.0054\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0064\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0064\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0027 - val_loss: 0.0060\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0055\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.0058\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.0055\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0028 - val_loss: 0.0059\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0058\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0057\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0029 - val_loss: 0.0058\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0053\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0063\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0060\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0052\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0054\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0056\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0059\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0057\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0057\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0061\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0028 - val_loss: 0.0056\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0057\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0055\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0052\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0027 - val_loss: 0.0063\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0054\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0030 - val_loss: 0.0052\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0026 - val_loss: 0.0061\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0029 - val_loss: 0.0062\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0024 - val_loss: 0.0046\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0026 - val_loss: 0.0054\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0027 - val_loss: 0.0058\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0028 - val_loss: 0.0053\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0030 - val_loss: 0.0055\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0026 - val_loss: 0.0056\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0024 - val_loss: 0.0056\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0025 - val_loss: 0.0061\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0026 - val_loss: 0.0050\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0026 - val_loss: 0.0054\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0028 - val_loss: 0.0052\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0024 - val_loss: 0.0051\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0026 - val_loss: 0.0055\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0053\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0048\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0057\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0028 - val_loss: 0.0053\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0056\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0049\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0050\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0054\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0023 - val_loss: 0.0051\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0054\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0025 - val_loss: 0.0054\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0048\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0051\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0052\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0027 - val_loss: 0.0057\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0023 - val_loss: 0.0056\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t Selling 0.03216971961210974 units of gold on 2019-03-01 00:00:00\n",
            "\t\t Price: 1311.95, unit: 42.20506364510737\n",
            "2018-04-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.1520 - val_loss: 0.2725\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1496 - val_loss: 0.2468\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1360 - val_loss: 0.2188\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1021 - val_loss: 0.1886\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0921 - val_loss: 0.1543\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0742 - val_loss: 0.1172\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0537 - val_loss: 0.0775\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0291 - val_loss: 0.0389\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0131 - val_loss: 0.0117\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0052 - val_loss: 0.0045\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0045 - val_loss: 0.0067\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0099\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0031 - val_loss: 0.0054\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0030 - val_loss: 0.0060\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0026 - val_loss: 0.0075\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0031 - val_loss: 0.0061\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0026 - val_loss: 0.0057\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0025 - val_loss: 0.0058\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0026 - val_loss: 0.0054\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0026 - val_loss: 0.0050\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0022 - val_loss: 0.0053\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0025 - val_loss: 0.0046\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0023 - val_loss: 0.0047\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0024 - val_loss: 0.0051\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0020 - val_loss: 0.0043\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0023 - val_loss: 0.0042\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0021 - val_loss: 0.0043\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0026 - val_loss: 0.0046\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0022 - val_loss: 0.0035\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0022 - val_loss: 0.0039\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0021 - val_loss: 0.0043\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0021 - val_loss: 0.0034\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0038\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0020 - val_loss: 0.0039\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0021 - val_loss: 0.0034\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0019 - val_loss: 0.0034\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0023 - val_loss: 0.0039\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0031\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0036\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.0034\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0032\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0031\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0037\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.0030\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0035\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0032\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0031\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.0031\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0031\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0035\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0028\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0034\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0020 - val_loss: 0.0031\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0020 - val_loss: 0.0035\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0028\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.0033\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0031\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0031\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019 - val_loss: 0.0030\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0029\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0018 - val_loss: 0.0031\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0030\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.0027\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0032\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0029\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0029\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0028\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0030\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0028\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0028\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0026\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0017 - val_loss: 0.0028\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0027\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0032\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0026\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0027\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0018 - val_loss: 0.0030\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.0025\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0026\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0018 - val_loss: 0.0028\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0017 - val_loss: 0.0029\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0018 - val_loss: 0.0027\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0018 - val_loss: 0.0030\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0018 - val_loss: 0.0026\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - val_loss: 0.0030\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0019 - val_loss: 0.0026\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0015 - val_loss: 0.0026\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0016 - val_loss: 0.0029\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0016 - val_loss: 0.0027\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0019 - val_loss: 0.0029\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0016 - val_loss: 0.0027\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0018 - val_loss: 0.0028\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0018 - val_loss: 0.0026\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0019 - val_loss: 0.0027\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0015 - val_loss: 0.0028\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0018 - val_loss: 0.0025\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0017 - val_loss: 0.0026\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0017 - val_loss: 0.0026\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0025\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0027\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0028\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0026\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0025\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.0023\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0025\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - val_loss: 0.0024\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0025\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.0024\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0022\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0018 - val_loss: 0.0026\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0023\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0016 - val_loss: 0.0024\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0019 - val_loss: 0.0024\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0023\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0024\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0026\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0026\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0028\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0023\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0017 - val_loss: 0.0022\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 0.0022\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0022\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0017 - val_loss: 0.0021\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0016 - val_loss: 0.0024\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.0024\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0014 - val_loss: 0.0024\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0023\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 0.0023\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0016 - val_loss: 0.0022\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0013 - val_loss: 0.0023\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0015 - val_loss: 0.0022\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0015 - val_loss: 0.0022\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0014 - val_loss: 0.0021\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0018 - val_loss: 0.0021\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0016 - val_loss: 0.0024\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0015 - val_loss: 0.0021\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0017 - val_loss: 0.0026\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0015 - val_loss: 0.0020\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0015 - val_loss: 0.0023\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0015 - val_loss: 0.0023\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0016 - val_loss: 0.0021\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0015 - val_loss: 0.0021\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0021\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.0022\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0021\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0022\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0021\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0022\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0021\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0014 - val_loss: 0.0021\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0014 - val_loss: 0.0021\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0022\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0014 - val_loss: 0.0020\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t Selling 0.03424369098998082 units of gold on 2019-04-01 00:00:00\n",
            "\t\t Price: 1293.5, unit: 44.29421429554019\n",
            "2018-05-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.1864 - val_loss: 0.6365\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1652 - val_loss: 0.5447\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1352 - val_loss: 0.4566\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1100 - val_loss: 0.3726\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0830 - val_loss: 0.2935\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0656 - val_loss: 0.2203\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0453 - val_loss: 0.1520\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0300 - val_loss: 0.0931\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0187 - val_loss: 0.0454\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0090 - val_loss: 0.0186\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0065 - val_loss: 0.0099\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0059 - val_loss: 0.0085\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0048 - val_loss: 0.0092\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0096\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0077\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0037 - val_loss: 0.0077\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0030 - val_loss: 0.0079\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0032 - val_loss: 0.0080\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0039 - val_loss: 0.0075\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0035 - val_loss: 0.0071\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0039 - val_loss: 0.0078\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0038 - val_loss: 0.0078\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0035 - val_loss: 0.0071\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0035 - val_loss: 0.0072\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0034 - val_loss: 0.0075\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0035 - val_loss: 0.0070\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0031 - val_loss: 0.0070\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0033 - val_loss: 0.0071\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0038 - val_loss: 0.0071\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0032 - val_loss: 0.0065\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0031 - val_loss: 0.0066\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0032 - val_loss: 0.0071\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0032 - val_loss: 0.0067\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0037 - val_loss: 0.0064\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0067\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0071\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0064\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0062\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0067\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.0064\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0067\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.0065\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0030 - val_loss: 0.0063\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0065\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0068\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0066\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0062\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0063\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0033 - val_loss: 0.0063\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.0064\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0060\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0065\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0066\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0062\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0063\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0063\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0061\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0061\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0064\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0063\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0031 - val_loss: 0.0060\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0062\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0060\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0029 - val_loss: 0.0058\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0029 - val_loss: 0.0063\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0030 - val_loss: 0.0061\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0059\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0060\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0060\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0056\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0029 - val_loss: 0.0054\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0059\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0030 - val_loss: 0.0061\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0058\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0055\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0057\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0058\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0056\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0056\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0028 - val_loss: 0.0056\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0059\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0056\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0029 - val_loss: 0.0053\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0056\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0061\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - val_loss: 0.0056\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0057\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0053\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0053\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0058\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0026 - val_loss: 0.0054\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0026 - val_loss: 0.0051\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0054\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0054\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0053\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0056\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0028 - val_loss: 0.0055\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0027 - val_loss: 0.0050\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0028 - val_loss: 0.0050\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0054\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0060\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0027 - val_loss: 0.0050\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0028 - val_loss: 0.0049\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0029 - val_loss: 0.0052\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0026 - val_loss: 0.0050\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0029 - val_loss: 0.0052\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0027 - val_loss: 0.0052\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0052\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0053\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0028 - val_loss: 0.0049\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0025 - val_loss: 0.0049\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0026 - val_loss: 0.0054\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0026 - val_loss: 0.0053\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0028 - val_loss: 0.0055\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0023 - val_loss: 0.0050\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0025 - val_loss: 0.0047\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0027 - val_loss: 0.0048\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0046\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0048\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0048\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0051\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0050\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0025 - val_loss: 0.0045\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0024 - val_loss: 0.0045\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0048\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0049\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0047\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0045\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0028 - val_loss: 0.0046\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0048\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0043\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0045\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0050\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0026 - val_loss: 0.0050\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0025 - val_loss: 0.0047\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0026 - val_loss: 0.0043\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0043\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0045\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0047\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.0044\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0043\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0048\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025 - val_loss: 0.0047\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0043\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0025 - val_loss: 0.0043\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0044\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0048\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0046\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0044\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0044\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0026 - val_loss: 0.0043\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0040\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0045\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0044\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.0041\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0045\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0042\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0042\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0041\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0044\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0047\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0039\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0039\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0040\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0040\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0041\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0023 - val_loss: 0.0042\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0047\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0041\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0039\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0040\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0041\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.0037\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0026 - val_loss: 0.0040\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0023 - val_loss: 0.0039\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0039\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0042\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0026 - val_loss: 0.0041\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0023 - val_loss: 0.0039\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0024 - val_loss: 0.0040\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0022 - val_loss: 0.0037\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0025 - val_loss: 0.0040\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0021 - val_loss: 0.0041\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0024 - val_loss: 0.0040\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0025 - val_loss: 0.0039\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0038\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0022 - val_loss: 0.0039\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0022 - val_loss: 0.0037\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0023 - val_loss: 0.0039\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0021 - val_loss: 0.0040\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0022 - val_loss: 0.0037\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0025 - val_loss: 0.0037\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0023 - val_loss: 0.0037\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0040\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.0039\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0036\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0038\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0035\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0037\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.0039\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t Selling 0.03621029592083614 units of gold on 2019-05-01 00:00:00\n",
            "\t\t Price: 1283.8, unit: 46.48677790316943\n",
            "2018-06-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 0.1130 - val_loss: 0.5424\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1046 - val_loss: 0.5234\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1009 - val_loss: 0.5048\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1016 - val_loss: 0.4871\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0937 - val_loss: 0.4704\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0821 - val_loss: 0.4557\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0728 - val_loss: 0.4410\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0632 - val_loss: 0.4148\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0581 - val_loss: 0.3794\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0490 - val_loss: 0.3369\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0344 - val_loss: 0.2793\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0291 - val_loss: 0.2139\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0222 - val_loss: 0.1654\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0184 - val_loss: 0.1301\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0125 - val_loss: 0.0984\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0103 - val_loss: 0.0717\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0081 - val_loss: 0.0482\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0058 - val_loss: 0.0290\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0178\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0040 - val_loss: 0.0109\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0104\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0042 - val_loss: 0.0103\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0121\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0129\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0146\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0135\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0139\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0035 - val_loss: 0.0121\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0037 - val_loss: 0.0133\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0034 - val_loss: 0.0127\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0037 - val_loss: 0.0116\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0037 - val_loss: 0.0130\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0035 - val_loss: 0.0113\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0034 - val_loss: 0.0118\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0037 - val_loss: 0.0119\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0034 - val_loss: 0.0104\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0035 - val_loss: 0.0110\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0033 - val_loss: 0.0124\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0035 - val_loss: 0.0111\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0031 - val_loss: 0.0118\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0034 - val_loss: 0.0101\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0032 - val_loss: 0.0100\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0031 - val_loss: 0.0099\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0033 - val_loss: 0.0104\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0031 - val_loss: 0.0102\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0033 - val_loss: 0.0092\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0094\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0032 - val_loss: 0.0084\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0033 - val_loss: 0.0096\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0083\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0096\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0088\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0080\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029 - val_loss: 0.0095\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.0091\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0073\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0094\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.0068\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0074\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0069\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0077\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0069\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0061\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0071\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0074\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0070\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0026 - val_loss: 0.0058\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0025 - val_loss: 0.0064\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0023 - val_loss: 0.0071\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0069\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0065\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0063\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0025 - val_loss: 0.0068\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.0067\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0072\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0083\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0026 - val_loss: 0.0066\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0089\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0052\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0101\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0062\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0099\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0057\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0087\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.0049\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0118\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0045\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0083\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0083\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0076\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0080\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.0075\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0087\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.0068\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0106\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.0079\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0070\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0102\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0052\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0092\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0067\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0019 - val_loss: 0.0119\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0022 - val_loss: 0.0068\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.0066\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0086\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0065\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.0087\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0020 - val_loss: 0.0070\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0021 - val_loss: 0.0059\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0103\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0020 - val_loss: 0.0053\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0021 - val_loss: 0.0114\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0056\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0019 - val_loss: 0.0109\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0021 - val_loss: 0.0043\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0020 - val_loss: 0.0106\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0023 - val_loss: 0.0041\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0023 - val_loss: 0.0103\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0024 - val_loss: 0.0055\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0021 - val_loss: 0.0065\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0020 - val_loss: 0.0073\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0020 - val_loss: 0.0066\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0018 - val_loss: 0.0084\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0020 - val_loss: 0.0061\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0019 - val_loss: 0.0079\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0017 - val_loss: 0.0054\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0081\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0018 - val_loss: 0.0065\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0069\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0019 - val_loss: 0.0085\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.0057\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0098\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0050\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0059\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0058\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0075\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0018 - val_loss: 0.0067\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0071\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0053\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0088\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0056\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0086\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0061\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0068\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0062\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0060\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0078\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0054\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0073\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0067\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0043\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0018 - val_loss: 0.0060\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0045\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0074\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0018 - val_loss: 0.0048\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0069\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0059\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0076\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0020 - val_loss: 0.0060\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0071\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0061\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0018 - val_loss: 0.0053\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0066\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0046\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0098\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0018 - val_loss: 0.0038\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0082\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0016 - val_loss: 0.0042\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0061\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0049\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0072\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0045\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0046\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0018 - val_loss: 0.0059\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0037\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0079\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0036\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0046\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0044\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0061\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0057\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.0041\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0073\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0018 - val_loss: 0.0046\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0063\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0047\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0054\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0015 - val_loss: 0.0046\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0016 - val_loss: 0.0070\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0016 - val_loss: 0.0048\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0061\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0016 - val_loss: 0.0054\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0015 - val_loss: 0.0051\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0016 - val_loss: 0.0047\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0015 - val_loss: 0.0041\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0015 - val_loss: 0.0056\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0017 - val_loss: 0.0059\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0016 - val_loss: 0.0038\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0013 - val_loss: 0.0044\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0016 - val_loss: 0.0049\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "2018-07-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 0.0522 - val_loss: 0.2215\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0426 - val_loss: 0.1881\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0282 - val_loss: 0.1558\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0261 - val_loss: 0.1246\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0195 - val_loss: 0.0949\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0146 - val_loss: 0.0676\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0097 - val_loss: 0.0432\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0076 - val_loss: 0.0249\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0053 - val_loss: 0.0148\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0048 - val_loss: 0.0103\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0042 - val_loss: 0.0074\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0052\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0026 - val_loss: 0.0031\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0040\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0058\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0013 - val_loss: 0.0120\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0013 - val_loss: 0.0119\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0013 - val_loss: 0.0150\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0124\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0012 - val_loss: 0.0104\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0013 - val_loss: 0.0100\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 0.0115\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0095\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 0.0109\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0013 - val_loss: 0.0089\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 0.0105\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0012 - val_loss: 0.0098\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0115\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0111\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0085\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0095\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0011 - val_loss: 0.0087\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0013 - val_loss: 0.0096\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.0084\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0012 - val_loss: 0.0084\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0089\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0010 - val_loss: 0.0084\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0092\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0010 - val_loss: 0.0093\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.7090e-04 - val_loss: 0.0084\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.6887e-04 - val_loss: 0.0091\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.3736e-04 - val_loss: 0.0076\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0011 - val_loss: 0.0078\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0011 - val_loss: 0.0079\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 9.5212e-04 - val_loss: 0.0069\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0011 - val_loss: 0.0073\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.7700e-04 - val_loss: 0.0071\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 9.9811e-04 - val_loss: 0.0079\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0011 - val_loss: 0.0068\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0010 - val_loss: 0.0069\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 9.4987e-04 - val_loss: 0.0059\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.3907e-04 - val_loss: 0.0067\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0010 - val_loss: 0.0066\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0065\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.7424e-04 - val_loss: 0.0055\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0064\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.3218e-04 - val_loss: 0.0064\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0061\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.4210e-04 - val_loss: 0.0056\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.7492e-04 - val_loss: 0.0052\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.2171e-04 - val_loss: 0.0057\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.4100e-04 - val_loss: 0.0053\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0010 - val_loss: 0.0064\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.9453e-04 - val_loss: 0.0052\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0010 - val_loss: 0.0061\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.9754e-04 - val_loss: 0.0051\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.9884e-04 - val_loss: 0.0060\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.1385e-04 - val_loss: 0.0048\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.4329e-04 - val_loss: 0.0053\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.8133e-04 - val_loss: 0.0046\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.1427e-04 - val_loss: 0.0051\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.6660e-04 - val_loss: 0.0057\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.9284e-04 - val_loss: 0.0050\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.7464e-04 - val_loss: 0.0051\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.1393e-04 - val_loss: 0.0050\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.1651e-04 - val_loss: 0.0046\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.1022e-04 - val_loss: 0.0046\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.6079e-04 - val_loss: 0.0046\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.5465e-04 - val_loss: 0.0037\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.9831e-04 - val_loss: 0.0048\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.8181e-04 - val_loss: 0.0036\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.3455e-04 - val_loss: 0.0043\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.8789e-04 - val_loss: 0.0040\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.2465e-04 - val_loss: 0.0043\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.3481e-04 - val_loss: 0.0038\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.4133e-04 - val_loss: 0.0044\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.8060e-04 - val_loss: 0.0040\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.5632e-04 - val_loss: 0.0040\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 7.9506e-04 - val_loss: 0.0035\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.2030e-04 - val_loss: 0.0038\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.6556e-04 - val_loss: 0.0043\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.2023e-04 - val_loss: 0.0029\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.7934e-04 - val_loss: 0.0046\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.2608e-04 - val_loss: 0.0024\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.3433e-04 - val_loss: 0.0044\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.9704e-04 - val_loss: 0.0029\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0040\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.7330e-04 - val_loss: 0.0035\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.7083e-04 - val_loss: 0.0038\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.2803e-04 - val_loss: 0.0029\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.7368e-04 - val_loss: 0.0036\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.5849e-04 - val_loss: 0.0035\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.6293e-04 - val_loss: 0.0037\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 8.3812e-04 - val_loss: 0.0028\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.4967e-04 - val_loss: 0.0038\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.3294e-04 - val_loss: 0.0028\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.1100e-04 - val_loss: 0.0032\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.3817e-04 - val_loss: 0.0031\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.0720e-04 - val_loss: 0.0031\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.0287e-04 - val_loss: 0.0029\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.4478e-04 - val_loss: 0.0030\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.9525e-04 - val_loss: 0.0036\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.4354e-04 - val_loss: 0.0029\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.9008e-04 - val_loss: 0.0032\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.0296e-04 - val_loss: 0.0031\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.6042e-04 - val_loss: 0.0025\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.6727e-04 - val_loss: 0.0031\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.3243e-04 - val_loss: 0.0022\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.5008e-04 - val_loss: 0.0030\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.3605e-04 - val_loss: 0.0029\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.0415e-04 - val_loss: 0.0025\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.5610e-04 - val_loss: 0.0024\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.9380e-04 - val_loss: 0.0029\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.8979e-04 - val_loss: 0.0023\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.7818e-04 - val_loss: 0.0025\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.5823e-04 - val_loss: 0.0031\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.4142e-04 - val_loss: 0.0020\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.1008e-04 - val_loss: 0.0029\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.0740e-04 - val_loss: 0.0026\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.6362e-04 - val_loss: 0.0022\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.0281e-04 - val_loss: 0.0023\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.9861e-04 - val_loss: 0.0024\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.5435e-04 - val_loss: 0.0022\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.1934e-04 - val_loss: 0.0028\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.0336e-04 - val_loss: 0.0022\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.4310e-04 - val_loss: 0.0018\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.5684e-04 - val_loss: 0.0023\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.9065e-04 - val_loss: 0.0035\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.8508e-04 - val_loss: 0.0015\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 7.5521e-04 - val_loss: 0.0023\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.3250e-04 - val_loss: 0.0021\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.7762e-04 - val_loss: 0.0019\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 6.2871e-04 - val_loss: 0.0019\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.9009e-04 - val_loss: 0.0022\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.2816e-04 - val_loss: 0.0020\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.7857e-04 - val_loss: 0.0020\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.1074e-04 - val_loss: 0.0028\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.8926e-04 - val_loss: 0.0020\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.7789e-04 - val_loss: 0.0023\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.8272e-04 - val_loss: 0.0017\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.8927e-04 - val_loss: 0.0017\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.9236e-04 - val_loss: 0.0022\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.1071e-04 - val_loss: 0.0015\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.2672e-04 - val_loss: 0.0019\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.8878e-04 - val_loss: 0.0023\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.0071e-04 - val_loss: 0.0014\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.0705e-04 - val_loss: 0.0017\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.2382e-04 - val_loss: 0.0023\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.3619e-04 - val_loss: 0.0015\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.1956e-04 - val_loss: 0.0014\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.2076e-04 - val_loss: 0.0018\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.4790e-04 - val_loss: 0.0023\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.3940e-04 - val_loss: 0.0013\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.3141e-04 - val_loss: 0.0014\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.9897e-04 - val_loss: 0.0016\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.0995e-04 - val_loss: 0.0017\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.4159e-04 - val_loss: 0.0017\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.9103e-04 - val_loss: 0.0017\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.4561e-04 - val_loss: 0.0015\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.3598e-04 - val_loss: 0.0015\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.3896e-04 - val_loss: 0.0016\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.7157e-04 - val_loss: 0.0019\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.8043e-04 - val_loss: 0.0015\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.5036e-04 - val_loss: 0.0014\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.5376e-04 - val_loss: 0.0013\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.5939e-04 - val_loss: 0.0014\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.4358e-04 - val_loss: 0.0015\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.2730e-04 - val_loss: 0.0014\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.8968e-04 - val_loss: 0.0013\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.3618e-04 - val_loss: 0.0012\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.7699e-04 - val_loss: 0.0018\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.7379e-04 - val_loss: 0.0016\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.0665e-04 - val_loss: 0.0014\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.4716e-04 - val_loss: 0.0012\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.7584e-04 - val_loss: 0.0014\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.4306e-04 - val_loss: 0.0017\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.5110e-04 - val_loss: 0.0013\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.1352e-04 - val_loss: 0.0014\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.4084e-04 - val_loss: 0.0013\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.7642e-04 - val_loss: 0.0015\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.9977e-04 - val_loss: 0.0014\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.1080e-04 - val_loss: 0.0015\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.7461e-04 - val_loss: 0.0012\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.2677e-04 - val_loss: 0.0013\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.8834e-04 - val_loss: 0.0012\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.9866e-04 - val_loss: 0.0012\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.7572e-04 - val_loss: 0.0012\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.5217e-04 - val_loss: 0.0012\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "2018-08-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 0.0965 - val_loss: 0.3347\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0720 - val_loss: 0.2867\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0629 - val_loss: 0.2399\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0482 - val_loss: 0.1949\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0348 - val_loss: 0.1488\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0245 - val_loss: 0.0995\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0173 - val_loss: 0.0501\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0099 - val_loss: 0.0128\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0056 - val_loss: 0.0072\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0043 - val_loss: 0.0112\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038 - val_loss: 0.0089\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.0142\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0267\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0252\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0321\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0336\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0295\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0260\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.0334\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.0258\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0205\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0021 - val_loss: 0.0242\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0234\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0147\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0018 - val_loss: 0.0166\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0154\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0127\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0133\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0132\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0101\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0088\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0089\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0074\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0064\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0067\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0068\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0067\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0052\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0048\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0049\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0014 - val_loss: 0.0053\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0014 - val_loss: 0.0049\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 0.0049\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0014 - val_loss: 0.0045\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 0.0043\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 0.0047\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0013 - val_loss: 0.0044\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0013 - val_loss: 0.0041\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 0.0041\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0014 - val_loss: 0.0038\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 0.0038\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 0.0040\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0014 - val_loss: 0.0041\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0038\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0035\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0013 - val_loss: 0.0037\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0013 - val_loss: 0.0034\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0015 - val_loss: 0.0035\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0013 - val_loss: 0.0035\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0014 - val_loss: 0.0034\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0033\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0013 - val_loss: 0.0033\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0013 - val_loss: 0.0032\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0013 - val_loss: 0.0031\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.0031\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0013 - val_loss: 0.0031\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.0030\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0013 - val_loss: 0.0030\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0013 - val_loss: 0.0031\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0013 - val_loss: 0.0030\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0012 - val_loss: 0.0030\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0013 - val_loss: 0.0029\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0013 - val_loss: 0.0029\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0012 - val_loss: 0.0029\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0011 - val_loss: 0.0028\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0013 - val_loss: 0.0028\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0012 - val_loss: 0.0028\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0012 - val_loss: 0.0028\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0013 - val_loss: 0.0028\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0011 - val_loss: 0.0027\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0010 - val_loss: 0.0028\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0012 - val_loss: 0.0028\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0013 - val_loss: 0.0027\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0011 - val_loss: 0.0027\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0013 - val_loss: 0.0028\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0012 - val_loss: 0.0027\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 0.0027\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0012 - val_loss: 0.0028\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0026\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.0027\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 0.0027\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0011 - val_loss: 0.0027\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0010 - val_loss: 0.0026\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0027\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0027\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0012 - val_loss: 0.0027\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0012 - val_loss: 0.0028\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0026\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0025\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0025\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0025\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0026\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.9083e-04 - val_loss: 0.0025\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0010 - val_loss: 0.0026\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0025\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.6629e-04 - val_loss: 0.0025\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.5910e-04 - val_loss: 0.0025\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0025\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0024\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0010 - val_loss: 0.0025\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0025\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0024\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0011 - val_loss: 0.0025\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.0803e-04 - val_loss: 0.0023\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.8726e-04 - val_loss: 0.0024\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.4745e-04 - val_loss: 0.0023\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0023\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0024\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0023\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0011 - val_loss: 0.0025\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.6069e-04 - val_loss: 0.0023\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0024\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.7397e-04 - val_loss: 0.0023\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0023\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.4177e-04 - val_loss: 0.0023\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0023\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.3045e-04 - val_loss: 0.0025\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.3717e-04 - val_loss: 0.0023\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.7549e-04 - val_loss: 0.0024\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.3925e-04 - val_loss: 0.0022\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0023\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.6646e-04 - val_loss: 0.0022\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.5533e-04 - val_loss: 0.0022\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.6375e-04 - val_loss: 0.0022\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0023\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.7306e-04 - val_loss: 0.0023\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.5149e-04 - val_loss: 0.0023\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.4187e-04 - val_loss: 0.0023\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.2449e-04 - val_loss: 0.0022\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 8.2038e-04 - val_loss: 0.0025\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.5382e-04 - val_loss: 0.0021\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.2816e-04 - val_loss: 0.0025\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0010 - val_loss: 0.0021\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.1025e-04 - val_loss: 0.0023\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0021\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.3557e-04 - val_loss: 0.0021\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.2053e-04 - val_loss: 0.0025\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.9318e-04 - val_loss: 0.0023\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.1516e-04 - val_loss: 0.0022\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.3114e-04 - val_loss: 0.0023\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.2487e-04 - val_loss: 0.0021\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0010 - val_loss: 0.0022\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.1742e-04 - val_loss: 0.0022\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.2317e-04 - val_loss: 0.0021\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.2292e-04 - val_loss: 0.0023\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0010 - val_loss: 0.0021\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.5831e-04 - val_loss: 0.0023\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.6101e-04 - val_loss: 0.0022\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.6826e-04 - val_loss: 0.0022\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.0537e-04 - val_loss: 0.0023\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.9886e-04 - val_loss: 0.0023\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.5896e-04 - val_loss: 0.0021\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.0161e-04 - val_loss: 0.0023\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.3961e-04 - val_loss: 0.0023\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.1146e-04 - val_loss: 0.0023\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.0879e-04 - val_loss: 0.0022\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.9765e-04 - val_loss: 0.0022\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 9.4986e-04 - val_loss: 0.0023\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.8428e-04 - val_loss: 0.0022\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.6981e-04 - val_loss: 0.0022\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.4401e-04 - val_loss: 0.0021\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.3353e-04 - val_loss: 0.0024\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.3706e-04 - val_loss: 0.0021\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.3339e-04 - val_loss: 0.0026\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.8034e-04 - val_loss: 0.0021\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.5918e-04 - val_loss: 0.0024\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.3976e-04 - val_loss: 0.0021\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 9.0252e-04 - val_loss: 0.0023\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.5368e-04 - val_loss: 0.0022\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.3151e-04 - val_loss: 0.0022\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.9248e-04 - val_loss: 0.0022\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.6613e-04 - val_loss: 0.0021\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.0215e-04 - val_loss: 0.0024\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.9023e-04 - val_loss: 0.0023\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.6324e-04 - val_loss: 0.0019\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.5429e-04 - val_loss: 0.0024\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.9872e-04 - val_loss: 0.0020\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.9204e-04 - val_loss: 0.0022\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 8.5782e-04 - val_loss: 0.0022\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.7521e-04 - val_loss: 0.0022\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.7367e-04 - val_loss: 0.0023\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.9946e-04 - val_loss: 0.0023\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.9211e-04 - val_loss: 0.0023\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.2129e-04 - val_loss: 0.0022\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.4779e-04 - val_loss: 0.0024\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.8234e-04 - val_loss: 0.0022\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.8612e-04 - val_loss: 0.0024\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "2018-09-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 0.1299 - val_loss: 0.6725\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1072 - val_loss: 0.5871\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0805 - val_loss: 0.5205\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0710 - val_loss: 0.4667\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0618 - val_loss: 0.4192\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0484 - val_loss: 0.3777\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0393 - val_loss: 0.3398\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0336 - val_loss: 0.3040\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0266 - val_loss: 0.2702\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0187 - val_loss: 0.2325\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0143 - val_loss: 0.1936\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0122 - val_loss: 0.1599\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0113 - val_loss: 0.1367\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0104 - val_loss: 0.1213\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0092 - val_loss: 0.1082\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0083 - val_loss: 0.1009\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0069 - val_loss: 0.0901\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0057 - val_loss: 0.0795\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0051 - val_loss: 0.0645\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0042 - val_loss: 0.0541\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0034 - val_loss: 0.0462\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0026 - val_loss: 0.0362\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0019 - val_loss: 0.0265\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0014 - val_loss: 0.0214\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0013 - val_loss: 0.0175\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0013 - val_loss: 0.0178\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0190\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 0.0203\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0211\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.8920e-04 - val_loss: 0.0223\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0230\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0228\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0231\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0010 - val_loss: 0.0228\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0243\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0253\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0238\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.9998e-04 - val_loss: 0.0234\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.6419e-04 - val_loss: 0.0231\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0224\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0231\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0010 - val_loss: 0.0239\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0240\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0010 - val_loss: 0.0225\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.8187e-04 - val_loss: 0.0232\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0238\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.9223e-04 - val_loss: 0.0220\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.0894e-04 - val_loss: 0.0227\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.5170e-04 - val_loss: 0.0235\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.8129e-04 - val_loss: 0.0227\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.0036e-04 - val_loss: 0.0232\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.7309e-04 - val_loss: 0.0241\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.5326e-04 - val_loss: 0.0227\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.2006e-04 - val_loss: 0.0222\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0227\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.1815e-04 - val_loss: 0.0228\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.8089e-04 - val_loss: 0.0223\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.6308e-04 - val_loss: 0.0223\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.6109e-04 - val_loss: 0.0224\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.4252e-04 - val_loss: 0.0226\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.7213e-04 - val_loss: 0.0224\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.5176e-04 - val_loss: 0.0221\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 8.5617e-04 - val_loss: 0.0235\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.5009e-04 - val_loss: 0.0243\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.8362e-04 - val_loss: 0.0228\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.5328e-04 - val_loss: 0.0217\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.9610e-04 - val_loss: 0.0209\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.6572e-04 - val_loss: 0.0213\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.8644e-04 - val_loss: 0.0235\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.9895e-04 - val_loss: 0.0227\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.5291e-04 - val_loss: 0.0224\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.9694e-04 - val_loss: 0.0215\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.6941e-04 - val_loss: 0.0206\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.5062e-04 - val_loss: 0.0218\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.8056e-04 - val_loss: 0.0214\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.3406e-04 - val_loss: 0.0226\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.6708e-04 - val_loss: 0.0232\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.0800e-04 - val_loss: 0.0204\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.4060e-04 - val_loss: 0.0203\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.8915e-04 - val_loss: 0.0207\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.7016e-04 - val_loss: 0.0218\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.8908e-04 - val_loss: 0.0210\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.0673e-04 - val_loss: 0.0199\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.0790e-04 - val_loss: 0.0192\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.2511e-04 - val_loss: 0.0208\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.7586e-04 - val_loss: 0.0207\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.1168e-04 - val_loss: 0.0200\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.9865e-04 - val_loss: 0.0204\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.2646e-04 - val_loss: 0.0203\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.3839e-04 - val_loss: 0.0200\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.2152e-04 - val_loss: 0.0191\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.5879e-04 - val_loss: 0.0202\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.7821e-04 - val_loss: 0.0216\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.6265e-04 - val_loss: 0.0206\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.8006e-04 - val_loss: 0.0198\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.2640e-04 - val_loss: 0.0196\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.8137e-04 - val_loss: 0.0195\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 7.6397e-04 - val_loss: 0.0206\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.7530e-04 - val_loss: 0.0214\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 8.7065e-04 - val_loss: 0.0212\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.3938e-04 - val_loss: 0.0200\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.0794e-04 - val_loss: 0.0195\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.4242e-04 - val_loss: 0.0187\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.4598e-04 - val_loss: 0.0188\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.8545e-04 - val_loss: 0.0186\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 8.4112e-04 - val_loss: 0.0198\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.8699e-04 - val_loss: 0.0181\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.4963e-04 - val_loss: 0.0178\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.8850e-04 - val_loss: 0.0212\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.5607e-04 - val_loss: 0.0207\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.7108e-04 - val_loss: 0.0194\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.1817e-04 - val_loss: 0.0206\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.8634e-04 - val_loss: 0.0194\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.5314e-04 - val_loss: 0.0189\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.4777e-04 - val_loss: 0.0191\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.2658e-04 - val_loss: 0.0180\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.1475e-04 - val_loss: 0.0171\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.4559e-04 - val_loss: 0.0186\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.9154e-04 - val_loss: 0.0186\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.3281e-04 - val_loss: 0.0182\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.3738e-04 - val_loss: 0.0183\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.8501e-04 - val_loss: 0.0180\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.7016e-04 - val_loss: 0.0184\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.8575e-04 - val_loss: 0.0183\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.1064e-04 - val_loss: 0.0172\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.4833e-04 - val_loss: 0.0163\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.7761e-04 - val_loss: 0.0184\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.9518e-04 - val_loss: 0.0187\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.0729e-04 - val_loss: 0.0173\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.5452e-04 - val_loss: 0.0160\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.8631e-04 - val_loss: 0.0159\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.0950e-04 - val_loss: 0.0168\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.7363e-04 - val_loss: 0.0165\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.1956e-04 - val_loss: 0.0166\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.0382e-04 - val_loss: 0.0175\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.6679e-04 - val_loss: 0.0173\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.2034e-04 - val_loss: 0.0172\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.1536e-04 - val_loss: 0.0170\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.7839e-04 - val_loss: 0.0167\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.6348e-04 - val_loss: 0.0169\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.7787e-04 - val_loss: 0.0167\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.7603e-04 - val_loss: 0.0167\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.2513e-04 - val_loss: 0.0163\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.5920e-04 - val_loss: 0.0167\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.6843e-04 - val_loss: 0.0166\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.4272e-04 - val_loss: 0.0172\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.3411e-04 - val_loss: 0.0174\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.6635e-04 - val_loss: 0.0155\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.7165e-04 - val_loss: 0.0153\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.6545e-04 - val_loss: 0.0158\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.1039e-04 - val_loss: 0.0148\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.0115e-04 - val_loss: 0.0157\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.0387e-04 - val_loss: 0.0172\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.8548e-04 - val_loss: 0.0161\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.0310e-04 - val_loss: 0.0153\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.9299e-04 - val_loss: 0.0148\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.4087e-04 - val_loss: 0.0151\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.7225e-04 - val_loss: 0.0158\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.3012e-04 - val_loss: 0.0153\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.5674e-04 - val_loss: 0.0166\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.6319e-04 - val_loss: 0.0153\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.9627e-04 - val_loss: 0.0155\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.5335e-04 - val_loss: 0.0152\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.0652e-04 - val_loss: 0.0143\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.6988e-04 - val_loss: 0.0141\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.7423e-04 - val_loss: 0.0135\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.4737e-04 - val_loss: 0.0139\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.8957e-04 - val_loss: 0.0145\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.4110e-04 - val_loss: 0.0145\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.1862e-04 - val_loss: 0.0142\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.1860e-04 - val_loss: 0.0143\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.0324e-04 - val_loss: 0.0135\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.6301e-04 - val_loss: 0.0143\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.1965e-04 - val_loss: 0.0150\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 6.2768e-04 - val_loss: 0.0143\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.2190e-04 - val_loss: 0.0135\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.6524e-04 - val_loss: 0.0135\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 6.5599e-04 - val_loss: 0.0131\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 7.0896e-04 - val_loss: 0.0132\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.8379e-04 - val_loss: 0.0131\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.2347e-04 - val_loss: 0.0132\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.0508e-04 - val_loss: 0.0125\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.7145e-04 - val_loss: 0.0123\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.7777e-04 - val_loss: 0.0133\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.8618e-04 - val_loss: 0.0129\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.6291e-04 - val_loss: 0.0121\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.1051e-04 - val_loss: 0.0134\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.7385e-04 - val_loss: 0.0123\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.7161e-04 - val_loss: 0.0118\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.7775e-04 - val_loss: 0.0122\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.2916e-04 - val_loss: 0.0122\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.6203e-04 - val_loss: 0.0120\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.2350e-04 - val_loss: 0.0125\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.8213e-04 - val_loss: 0.0119\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.2581e-04 - val_loss: 0.0117\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.8412e-04 - val_loss: 0.0116\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.2248e-04 - val_loss: 0.0118\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.9242e-04 - val_loss: 0.0114\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.0995e-04 - val_loss: 0.0115\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.8809e-04 - val_loss: 0.0111\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "2018-10-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 0.0642 - val_loss: 0.4238\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0435 - val_loss: 0.3293\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0310 - val_loss: 0.2486\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0213 - val_loss: 0.1773\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0121 - val_loss: 0.1178\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0062 - val_loss: 0.0712\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0037 - val_loss: 0.0397\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.0232\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0029 - val_loss: 0.0184\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0026 - val_loss: 0.0192\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0213\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0231\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0224\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.0191\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0020 - val_loss: 0.0152\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0123\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0097\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0087\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0075\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0073\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0016 - val_loss: 0.0073\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0014 - val_loss: 0.0078\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0014 - val_loss: 0.0088\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0013 - val_loss: 0.0108\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0012 - val_loss: 0.0133\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0012 - val_loss: 0.0173\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0012 - val_loss: 0.0208\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0012 - val_loss: 0.0281\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0012 - val_loss: 0.0334\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0011 - val_loss: 0.0394\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0010 - val_loss: 0.0488\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0010 - val_loss: 0.0536\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0012 - val_loss: 0.0564\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0010 - val_loss: 0.0627\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.6235e-04 - val_loss: 0.0745\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.6572e-04 - val_loss: 0.0827\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0010 - val_loss: 0.0764\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.8261e-04 - val_loss: 0.0803\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.2553e-04 - val_loss: 0.1001\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.4095e-04 - val_loss: 0.1129\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.5042e-04 - val_loss: 0.1043\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.0166e-04 - val_loss: 0.1059\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.6276e-04 - val_loss: 0.1001\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0010 - val_loss: 0.1049\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.0568e-04 - val_loss: 0.1230\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.5983e-04 - val_loss: 0.1202\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.6063e-04 - val_loss: 0.1087\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.5000e-04 - val_loss: 0.1174\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.7128e-04 - val_loss: 0.1327\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.2020e-04 - val_loss: 0.1295\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.9349e-04 - val_loss: 0.1111\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.2776e-04 - val_loss: 0.1162\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.8265e-04 - val_loss: 0.1041\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.8871e-04 - val_loss: 0.1170\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.6217e-04 - val_loss: 0.1191\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.1090e-04 - val_loss: 0.1074\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.4922e-04 - val_loss: 0.1122\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.1340e-04 - val_loss: 0.0958\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.0112e-04 - val_loss: 0.1051\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.5411e-04 - val_loss: 0.1191\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.5223e-04 - val_loss: 0.1072\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.2542e-04 - val_loss: 0.0977\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.2967e-04 - val_loss: 0.1027\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.4168e-04 - val_loss: 0.1061\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.6026e-04 - val_loss: 0.1032\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.4883e-04 - val_loss: 0.0974\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.2149e-04 - val_loss: 0.1060\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.8882e-04 - val_loss: 0.0953\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.7135e-04 - val_loss: 0.0992\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.3988e-04 - val_loss: 0.1003\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.0116e-04 - val_loss: 0.0919\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.3132e-04 - val_loss: 0.0910\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.7262e-04 - val_loss: 0.0942\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.8148e-04 - val_loss: 0.0971\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.5643e-04 - val_loss: 0.0966\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.1187e-04 - val_loss: 0.1009\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.0660e-04 - val_loss: 0.0945\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.8250e-04 - val_loss: 0.0838\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.5135e-04 - val_loss: 0.0861\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.4496e-04 - val_loss: 0.0946\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.8835e-04 - val_loss: 0.0918\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.6348e-04 - val_loss: 0.0907\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.8161e-04 - val_loss: 0.0925\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.2722e-04 - val_loss: 0.0898\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.1079e-04 - val_loss: 0.1004\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.3576e-04 - val_loss: 0.0916\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.7043e-04 - val_loss: 0.0810\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.2058e-04 - val_loss: 0.0807\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.0162e-04 - val_loss: 0.0938\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.0044e-04 - val_loss: 0.0856\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.2315e-04 - val_loss: 0.0786\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.5486e-04 - val_loss: 0.0892\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.4038e-04 - val_loss: 0.0935\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.7317e-04 - val_loss: 0.0854\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.6431e-04 - val_loss: 0.0681\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.7134e-04 - val_loss: 0.0835\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.5153e-04 - val_loss: 0.0845\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.0378e-04 - val_loss: 0.0710\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.3625e-04 - val_loss: 0.0772\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 7.1695e-04 - val_loss: 0.0837\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.3640e-04 - val_loss: 0.0795\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.0251e-04 - val_loss: 0.0711\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.4517e-04 - val_loss: 0.0748\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.0337e-04 - val_loss: 0.0721\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.7727e-04 - val_loss: 0.0772\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 7.2324e-04 - val_loss: 0.0604\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.8381e-04 - val_loss: 0.0703\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.7207e-04 - val_loss: 0.0716\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.9164e-04 - val_loss: 0.0685\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.0089e-04 - val_loss: 0.0615\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9431e-04 - val_loss: 0.0609\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.0071e-04 - val_loss: 0.0630\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.5360e-04 - val_loss: 0.0665\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 6.8618e-04 - val_loss: 0.0662\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 6.3343e-04 - val_loss: 0.0638\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.3870e-04 - val_loss: 0.0592\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.6324e-04 - val_loss: 0.0577\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.1251e-04 - val_loss: 0.0620\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.9717e-04 - val_loss: 0.0597\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.8384e-04 - val_loss: 0.0501\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.8020e-04 - val_loss: 0.0616\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.5347e-04 - val_loss: 0.0618\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.8509e-04 - val_loss: 0.0525\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.3425e-04 - val_loss: 0.0496\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.6457e-04 - val_loss: 0.0552\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.6922e-04 - val_loss: 0.0572\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.6829e-04 - val_loss: 0.0464\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 7.0376e-04 - val_loss: 0.0527\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.1906e-04 - val_loss: 0.0558\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.8722e-04 - val_loss: 0.0476\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.5133e-04 - val_loss: 0.0501\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.9034e-04 - val_loss: 0.0459\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.3287e-04 - val_loss: 0.0513\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.0845e-04 - val_loss: 0.0493\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.8893e-04 - val_loss: 0.0450\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.3597e-04 - val_loss: 0.0510\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.2713e-04 - val_loss: 0.0475\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.1959e-04 - val_loss: 0.0496\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.5571e-04 - val_loss: 0.0451\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.3233e-04 - val_loss: 0.0495\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.0340e-04 - val_loss: 0.0421\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.5764e-04 - val_loss: 0.0463\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.6795e-04 - val_loss: 0.0451\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.6362e-04 - val_loss: 0.0415\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.5180e-04 - val_loss: 0.0445\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.4874e-04 - val_loss: 0.0430\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.1160e-04 - val_loss: 0.0439\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.9969e-04 - val_loss: 0.0406\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.4583e-04 - val_loss: 0.0359\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.3479e-04 - val_loss: 0.0402\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.4005e-04 - val_loss: 0.0367\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.2633e-04 - val_loss: 0.0393\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.4154e-04 - val_loss: 0.0319\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.2176e-04 - val_loss: 0.0383\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.6992e-04 - val_loss: 0.0375\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.3330e-04 - val_loss: 0.0326\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.6913e-04 - val_loss: 0.0379\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.8317e-04 - val_loss: 0.0332\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.5078e-04 - val_loss: 0.0319\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.3284e-04 - val_loss: 0.0378\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.1011e-04 - val_loss: 0.0323\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.2644e-04 - val_loss: 0.0321\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.9176e-04 - val_loss: 0.0322\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.9387e-04 - val_loss: 0.0328\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.4482e-04 - val_loss: 0.0323\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.0054e-04 - val_loss: 0.0308\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.8253e-04 - val_loss: 0.0316\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.7745e-04 - val_loss: 0.0290\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.7029e-04 - val_loss: 0.0292\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.7029e-04 - val_loss: 0.0334\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.9827e-04 - val_loss: 0.0274\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.5728e-04 - val_loss: 0.0297\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.4964e-04 - val_loss: 0.0326\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.0962e-04 - val_loss: 0.0251\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.5369e-04 - val_loss: 0.0289\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.7366e-04 - val_loss: 0.0299\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 5.5605e-04 - val_loss: 0.0255\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.9728e-04 - val_loss: 0.0262\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 6.2106e-04 - val_loss: 0.0278\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.4659e-04 - val_loss: 0.0286\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.8687e-04 - val_loss: 0.0240\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 6.4222e-04 - val_loss: 0.0260\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.3368e-04 - val_loss: 0.0264\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.9722e-04 - val_loss: 0.0243\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.4997e-04 - val_loss: 0.0258\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 5.6707e-04 - val_loss: 0.0266\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.0132e-04 - val_loss: 0.0235\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.6940e-04 - val_loss: 0.0241\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 5.7526e-04 - val_loss: 0.0265\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.6593e-04 - val_loss: 0.0245\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.2814e-04 - val_loss: 0.0257\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 5.6345e-04 - val_loss: 0.0237\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 5.3758e-04 - val_loss: 0.0228\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.5630e-04 - val_loss: 0.0249\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.9474e-04 - val_loss: 0.0222\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.9325e-04 - val_loss: 0.0244\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.8072e-04 - val_loss: 0.0237\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.5960e-04 - val_loss: 0.0206\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.7490e-04 - val_loss: 0.0210\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.7522e-04 - val_loss: 0.0226\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "2018-11-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 0.0827 - val_loss: 0.7220\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0709 - val_loss: 0.6584\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0636 - val_loss: 0.5977\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0516 - val_loss: 0.5400\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0447 - val_loss: 0.4842\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0365 - val_loss: 0.4303\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0306 - val_loss: 0.3790\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0210 - val_loss: 0.3315\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0143 - val_loss: 0.2853\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0109 - val_loss: 0.2366\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0069 - val_loss: 0.1814\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0048 - val_loss: 0.1381\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0045 - val_loss: 0.1092\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0038 - val_loss: 0.0928\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0848\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0035 - val_loss: 0.0769\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0029 - val_loss: 0.0702\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0634\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0575\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0504\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0441\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0378\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 0.0320\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0014 - val_loss: 0.0269\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0012 - val_loss: 0.0234\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0011 - val_loss: 0.0207\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0012 - val_loss: 0.0189\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0012 - val_loss: 0.0183\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0013 - val_loss: 0.0186\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0012 - val_loss: 0.0188\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0011 - val_loss: 0.0187\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.9884e-04 - val_loss: 0.0190\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0011 - val_loss: 0.0195\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0012 - val_loss: 0.0204\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0011 - val_loss: 0.0198\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0010 - val_loss: 0.0202\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0010 - val_loss: 0.0204\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 9.8178e-04 - val_loss: 0.0203\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0010 - val_loss: 0.0193\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0011 - val_loss: 0.0191\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0010 - val_loss: 0.0199\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0011 - val_loss: 0.0194\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0203\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0010 - val_loss: 0.0198\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0188\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 0.0188\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.9456e-04 - val_loss: 0.0186\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0191\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0198\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.8510e-04 - val_loss: 0.0197\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0196\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0011 - val_loss: 0.0192\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0192\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0193\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.7366e-04 - val_loss: 0.0184\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.7464e-04 - val_loss: 0.0189\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.0129e-04 - val_loss: 0.0202\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.7854e-04 - val_loss: 0.0188\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0010 - val_loss: 0.0192\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.8844e-04 - val_loss: 0.0193\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.8696e-04 - val_loss: 0.0201\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.3306e-04 - val_loss: 0.0202\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0194\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0011 - val_loss: 0.0197\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.8244e-04 - val_loss: 0.0185\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0185\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0187\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0191\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.1946e-04 - val_loss: 0.0186\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0200\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.1590e-04 - val_loss: 0.0202\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.3874e-04 - val_loss: 0.0175\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.8228e-04 - val_loss: 0.0180\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.7339e-04 - val_loss: 0.0199\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0010 - val_loss: 0.0201\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.5296e-04 - val_loss: 0.0192\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.7745e-04 - val_loss: 0.0195\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0010 - val_loss: 0.0195\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.2945e-04 - val_loss: 0.0186\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.2269e-04 - val_loss: 0.0170\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.6293e-04 - val_loss: 0.0168\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.9004e-04 - val_loss: 0.0171\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.0410e-04 - val_loss: 0.0190\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.9327e-04 - val_loss: 0.0183\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.9716e-04 - val_loss: 0.0178\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.4939e-04 - val_loss: 0.0188\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.4573e-04 - val_loss: 0.0192\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.0784e-04 - val_loss: 0.0184\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.9727e-04 - val_loss: 0.0186\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.1699e-04 - val_loss: 0.0191\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.7881e-04 - val_loss: 0.0183\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.3525e-04 - val_loss: 0.0191\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.5193e-04 - val_loss: 0.0198\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.4152e-04 - val_loss: 0.0180\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.1120e-04 - val_loss: 0.0180\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0185\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.8132e-04 - val_loss: 0.0181\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.4234e-04 - val_loss: 0.0173\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.7195e-04 - val_loss: 0.0173\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.2191e-04 - val_loss: 0.0162\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.9168e-04 - val_loss: 0.0176\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.3273e-04 - val_loss: 0.0171\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.0950e-04 - val_loss: 0.0161\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.4367e-04 - val_loss: 0.0156\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.7383e-04 - val_loss: 0.0161\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.5527e-04 - val_loss: 0.0163\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.4752e-04 - val_loss: 0.0159\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.7764e-04 - val_loss: 0.0161\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.7625e-04 - val_loss: 0.0157\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 8.6220e-04 - val_loss: 0.0151\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 7.7674e-04 - val_loss: 0.0151\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.8687e-04 - val_loss: 0.0145\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.7369e-04 - val_loss: 0.0154\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.5709e-04 - val_loss: 0.0162\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.6097e-04 - val_loss: 0.0156\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 8.7282e-04 - val_loss: 0.0162\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.1831e-04 - val_loss: 0.0158\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.0641e-04 - val_loss: 0.0152\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.1180e-04 - val_loss: 0.0151\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.2544e-04 - val_loss: 0.0154\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.0391e-04 - val_loss: 0.0155\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.8701e-04 - val_loss: 0.0158\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.9644e-04 - val_loss: 0.0161\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.1751e-04 - val_loss: 0.0157\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.6884e-04 - val_loss: 0.0149\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.8730e-04 - val_loss: 0.0150\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.3189e-04 - val_loss: 0.0140\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.6514e-04 - val_loss: 0.0150\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.2463e-04 - val_loss: 0.0148\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.1705e-04 - val_loss: 0.0137\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.2438e-04 - val_loss: 0.0146\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.0476e-04 - val_loss: 0.0143\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.7233e-04 - val_loss: 0.0134\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.2534e-04 - val_loss: 0.0144\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.3081e-04 - val_loss: 0.0133\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.9135e-04 - val_loss: 0.0131\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.7504e-04 - val_loss: 0.0145\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.7190e-04 - val_loss: 0.0136\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.4309e-04 - val_loss: 0.0134\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.1762e-04 - val_loss: 0.0135\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.2619e-04 - val_loss: 0.0142\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.5640e-04 - val_loss: 0.0134\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.5904e-04 - val_loss: 0.0135\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.4569e-04 - val_loss: 0.0136\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.1251e-04 - val_loss: 0.0130\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.3361e-04 - val_loss: 0.0132\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.7387e-04 - val_loss: 0.0122\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.1354e-04 - val_loss: 0.0127\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.7455e-04 - val_loss: 0.0131\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.4568e-04 - val_loss: 0.0134\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.7186e-04 - val_loss: 0.0134\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.7683e-04 - val_loss: 0.0124\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.0103e-04 - val_loss: 0.0131\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.2620e-04 - val_loss: 0.0142\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.1009e-04 - val_loss: 0.0134\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.0475e-04 - val_loss: 0.0137\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.5842e-04 - val_loss: 0.0130\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.7104e-04 - val_loss: 0.0124\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.7543e-04 - val_loss: 0.0114\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.2542e-04 - val_loss: 0.0127\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.1478e-04 - val_loss: 0.0125\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.5331e-04 - val_loss: 0.0123\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.4984e-04 - val_loss: 0.0128\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.9732e-04 - val_loss: 0.0119\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.3211e-04 - val_loss: 0.0133\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.8601e-04 - val_loss: 0.0130\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.0571e-04 - val_loss: 0.0121\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.6944e-04 - val_loss: 0.0132\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.0069e-04 - val_loss: 0.0119\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.1032e-04 - val_loss: 0.0122\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.9770e-04 - val_loss: 0.0122\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.1323e-04 - val_loss: 0.0120\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.4140e-04 - val_loss: 0.0120\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.4822e-04 - val_loss: 0.0119\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.0286e-04 - val_loss: 0.0131\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.4384e-04 - val_loss: 0.0130\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.6977e-04 - val_loss: 0.0122\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.0913e-04 - val_loss: 0.0127\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.0678e-04 - val_loss: 0.0113\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.4498e-04 - val_loss: 0.0114\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.5201e-04 - val_loss: 0.0114\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.1484e-04 - val_loss: 0.0108\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.5993e-04 - val_loss: 0.0110\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.7355e-04 - val_loss: 0.0102\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.3227e-04 - val_loss: 0.0102\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.6381e-04 - val_loss: 0.0099\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 5.9687e-04 - val_loss: 0.0101\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 6.1063e-04 - val_loss: 0.0102\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.8480e-04 - val_loss: 0.0102\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 7.4772e-04 - val_loss: 0.0099\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.7547e-04 - val_loss: 0.0101\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 5.8291e-04 - val_loss: 0.0099\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.6897e-04 - val_loss: 0.0092\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.1606e-04 - val_loss: 0.0097\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.1213e-04 - val_loss: 0.0096\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 6.7326e-04 - val_loss: 0.0094\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.5584e-04 - val_loss: 0.0100\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.1768e-04 - val_loss: 0.0092\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.7276e-04 - val_loss: 0.0089\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 5.3734e-04 - val_loss: 0.0092\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "2018-12-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0900 - val_loss: 0.7144\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0768 - val_loss: 0.6738\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0695 - val_loss: 0.6380\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0648 - val_loss: 0.6061\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0567 - val_loss: 0.5760\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0558 - val_loss: 0.5470\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0503 - val_loss: 0.5198\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0455 - val_loss: 0.4933\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0405 - val_loss: 0.4672\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0356 - val_loss: 0.4411\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0334 - val_loss: 0.4138\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0301 - val_loss: 0.3853\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0251 - val_loss: 0.3516\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0185 - val_loss: 0.3059\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0133 - val_loss: 0.2550\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0108 - val_loss: 0.2006\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0083 - val_loss: 0.1526\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0077 - val_loss: 0.1230\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0085 - val_loss: 0.1096\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0078 - val_loss: 0.1149\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0078 - val_loss: 0.1149\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0072 - val_loss: 0.1066\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0070 - val_loss: 0.0937\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0065 - val_loss: 0.0805\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0062 - val_loss: 0.0717\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0063 - val_loss: 0.0617\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0055 - val_loss: 0.0536\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0051 - val_loss: 0.0405\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0051 - val_loss: 0.0277\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0054 - val_loss: 0.0188\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0055 - val_loss: 0.0119\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0047 - val_loss: 0.0112\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0045 - val_loss: 0.0086\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0048 - val_loss: 0.0087\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0043 - val_loss: 0.0126\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0046 - val_loss: 0.0183\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0039 - val_loss: 0.0275\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0042 - val_loss: 0.0440\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0035 - val_loss: 0.0577\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0037 - val_loss: 0.0815\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0035 - val_loss: 0.1112\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0035 - val_loss: 0.1445\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0031 - val_loss: 0.1758\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0033 - val_loss: 0.2467\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0033 - val_loss: 0.3022\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0031 - val_loss: 0.3110\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0028 - val_loss: 0.3930\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.4866\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0030 - val_loss: 0.4734\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0030 - val_loss: 0.4349\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0030 - val_loss: 0.5110\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0028 - val_loss: 0.5096\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0030 - val_loss: 0.6035\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0024 - val_loss: 0.7016\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0028 - val_loss: 0.7307\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0028 - val_loss: 0.7758\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.7494\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - val_loss: 0.6765\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.7604\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.8526\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0030 - val_loss: 0.8296\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.8234\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.8492\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0025 - val_loss: 0.9607\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0027 - val_loss: 1.0321\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.7822\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.7912\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.9819\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 1.0945\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025 - val_loss: 0.9234\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0029 - val_loss: 0.8240\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.8011\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.7996\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.8811\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0027 - val_loss: 0.8507\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0026 - val_loss: 0.7176\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025 - val_loss: 0.8173\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.8970\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.9270\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.9460\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.8499\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0026 - val_loss: 0.7388\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.7226\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0024 - val_loss: 0.7631\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.8997\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.8302\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.7500\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.7834\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - val_loss: 0.8670\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025 - val_loss: 0.8859\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.7986\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.7268\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.8429\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0023 - val_loss: 0.8957\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0024 - val_loss: 0.8453\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.7504\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.7765\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0022 - val_loss: 0.6005\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.7181\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.7931\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0027 - val_loss: 0.7426\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.7187\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.7659\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.8281\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.7238\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.6971\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0027 - val_loss: 0.7091\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.6821\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.7793\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.7551\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.6326\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.4793\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0025 - val_loss: 0.5904\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.7204\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0020 - val_loss: 0.7072\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.5167\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0026 - val_loss: 0.5816\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0024 - val_loss: 0.6533\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0022 - val_loss: 0.6538\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0019 - val_loss: 0.5895\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0024 - val_loss: 0.6046\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0025 - val_loss: 0.5934\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0023 - val_loss: 0.5802\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0020 - val_loss: 0.6085\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0022 - val_loss: 0.7095\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0019 - val_loss: 0.6096\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0023 - val_loss: 0.6128\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.5939\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0023 - val_loss: 0.5948\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0026 - val_loss: 0.6262\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0023 - val_loss: 0.5756\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0021 - val_loss: 0.6089\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0022 - val_loss: 0.6184\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0020 - val_loss: 0.5920\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0025 - val_loss: 0.5720\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0025 - val_loss: 0.5538\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0025 - val_loss: 0.5848\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.5828\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.6289\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.5740\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0020 - val_loss: 0.5748\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.5824\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.5484\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.5590\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.5995\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0026 - val_loss: 0.5362\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.5106\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.5590\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0026 - val_loss: 0.6187\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.6391\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.5156\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025 - val_loss: 0.5130\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0024 - val_loss: 0.5537\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0026 - val_loss: 0.5394\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0023 - val_loss: 0.5131\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - val_loss: 0.5589\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0021 - val_loss: 0.5778\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.5555\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.5048\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.4720\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.4852\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.4749\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.4850\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.4357\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.4627\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0019 - val_loss: 0.4008\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0023 - val_loss: 0.4199\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0025 - val_loss: 0.4693\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.4285\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.3991\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0022 - val_loss: 0.4125\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.3854\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0021 - val_loss: 0.3907\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.4081\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.4241\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.3896\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.3788\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0019 - val_loss: 0.3800\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.4036\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.4056\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.4166\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.4002\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.3790\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.4399\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.4023\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0021 - val_loss: 0.3822\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0021 - val_loss: 0.4127\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.4101\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0024 - val_loss: 0.4051\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0018 - val_loss: 0.3785\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.3982\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0021 - val_loss: 0.4037\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0019 - val_loss: 0.3985\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.3609\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.3593\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0024 - val_loss: 0.3839\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0019 - val_loss: 0.4113\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0017 - val_loss: 0.3766\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0018 - val_loss: 0.3288\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0021 - val_loss: 0.3462\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1460.15, Last: 1892844.3354248044\n",
            "\t\t Buying 0.03341291881613281 units of gold on 2019-12-01 00:00:00\n",
            "\t\t Price: 1460.15, unit: 48.78787340937632\n",
            "2019-01-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0657 - val_loss: 0.4425\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0549 - val_loss: 0.3714\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0477 - val_loss: 0.2979\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0344 - val_loss: 0.2222\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0225 - val_loss: 0.1440\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0195 - val_loss: 0.0650\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0108 - val_loss: 0.0123\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0072 - val_loss: 0.0107\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0060 - val_loss: 0.0465\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0057 - val_loss: 0.0499\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0047 - val_loss: 0.0517\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0041 - val_loss: 0.1273\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.1960\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0033 - val_loss: 0.2320\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.2642\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.2830\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.2566\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.2174\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.1995\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0032 - val_loss: 0.2250\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.1789\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0031 - val_loss: 0.1443\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.1520\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0030 - val_loss: 0.1519\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0030 - val_loss: 0.1369\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.1277\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031 - val_loss: 0.1479\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.1138\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0030 - val_loss: 0.1015\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.1035\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0028 - val_loss: 0.0944\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0027 - val_loss: 0.0780\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0875\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0901\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0733\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025 - val_loss: 0.0719\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0648\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0548\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0028 - val_loss: 0.0529\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024 - val_loss: 0.0592\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0027 - val_loss: 0.0502\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0025 - val_loss: 0.0451\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0023 - val_loss: 0.0491\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0024 - val_loss: 0.0482\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0026 - val_loss: 0.0352\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0028 - val_loss: 0.0293\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0025 - val_loss: 0.0354\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0020 - val_loss: 0.0367\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0024 - val_loss: 0.0385\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0023 - val_loss: 0.0276\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0021 - val_loss: 0.0304\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0023 - val_loss: 0.0297\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0024 - val_loss: 0.0289\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0023 - val_loss: 0.0230\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0021 - val_loss: 0.0252\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0204\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0022 - val_loss: 0.0230\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0022 - val_loss: 0.0213\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0019 - val_loss: 0.0214\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0019 - val_loss: 0.0244\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0022 - val_loss: 0.0195\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.0180\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0139\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0020 - val_loss: 0.0157\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0130\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.0130\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0121\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0118\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0090\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0100\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - val_loss: 0.0140\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0062\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0024 - val_loss: 0.0085\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0020 - val_loss: 0.0081\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0065\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0107\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0097\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0056\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0060\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0021 - val_loss: 0.0057\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0053\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0037\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0059\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0056\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0044\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0020 - val_loss: 0.0057\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0041\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0034\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0030\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0029\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0029\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0041\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0025\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0028\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0019\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0018 - val_loss: 0.0024\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0030\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0020\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0021\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.0017\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0019\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0018 - val_loss: 0.0017\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0019 - val_loss: 0.0018\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0019\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0017\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0021\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0016 - val_loss: 0.0019\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0016 - val_loss: 0.0019\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0020 - val_loss: 0.0019\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0016 - val_loss: 0.0018\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0016 - val_loss: 0.0019\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0020\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0016 - val_loss: 0.0019\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0015 - val_loss: 0.0021\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0015 - val_loss: 0.0019\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0016 - val_loss: 0.0018\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0018\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0015 - val_loss: 0.0020\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0019\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0013 - val_loss: 0.0019\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0018\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0015 - val_loss: 0.0018\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0013 - val_loss: 0.0018\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.0018\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0018\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0016\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0017 - val_loss: 0.0019\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1514.75, Last: 1530.5355687022209\n",
            "\t\t Buying 0.030582000859681675 units of gold on 2020-01-01 00:00:00\n",
            "\t\t Price: 1514.75, unit: 46.324085802202816\n",
            "2019-02-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 0.1100 - val_loss: 0.3560\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0968 - val_loss: 0.2931\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0919 - val_loss: 0.2202\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0558 - val_loss: 0.1364\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0393 - val_loss: 0.0487\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0193 - val_loss: 0.0058\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0095 - val_loss: 0.0394\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0081 - val_loss: 0.0180\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0046 - val_loss: 0.0076\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0048 - val_loss: 0.0093\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0037 - val_loss: 0.0193\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0052 - val_loss: 0.0242\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0128\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0087\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0095\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0040 - val_loss: 0.0118\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0095\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0031 - val_loss: 0.0090\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0107\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.0091\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0033 - val_loss: 0.0080\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0091\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034 - val_loss: 0.0078\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0034 - val_loss: 0.0070\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0057\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0059\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0029 - val_loss: 0.0059\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0071\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0063\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0052\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.0046\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0027 - val_loss: 0.0049\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0041\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0049\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.0038\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0042\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0048\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0038\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0035\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0032\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0032\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0029\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0037\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0028\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0027\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0020 - val_loss: 0.0024\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0021 - val_loss: 0.0020\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0017 - val_loss: 0.0026\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0019 - val_loss: 0.0024\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0018 - val_loss: 0.0016\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0019 - val_loss: 0.0025\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0021 - val_loss: 0.0017\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0017 - val_loss: 0.0021\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0018 - val_loss: 0.0025\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0019 - val_loss: 0.0016\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0017 - val_loss: 0.0019\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0016\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0017\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0014 - val_loss: 0.0016\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.6032e-04 - val_loss: 0.0013\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 9.3428e-04 - val_loss: 0.0013\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.7872e-04 - val_loss: 0.0013\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.4606e-04 - val_loss: 0.0012\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.4352e-04 - val_loss: 0.0011\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.6961e-04 - val_loss: 0.0011\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.2356e-04 - val_loss: 0.0011\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.6703e-04 - val_loss: 0.0011\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.6338e-04 - val_loss: 0.0011\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.6707e-04 - val_loss: 0.0011\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.8183e-04 - val_loss: 0.0011\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.0989e-04 - val_loss: 0.0011\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.7344e-04 - val_loss: 0.0011\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.5975e-04 - val_loss: 0.0011\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.4101e-04 - val_loss: 0.0011\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 9.5714e-04 - val_loss: 0.0011\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t Selling 0.027764625343511914 units of gold on 2020-02-01 00:00:00\n",
            "\t\t Price: 1584.2, unit: 43.984719469191575\n",
            "2019-03-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.1600 - val_loss: 0.4463\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1382 - val_loss: 0.4280\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1390 - val_loss: 0.4108\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1332 - val_loss: 0.3945\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1204 - val_loss: 0.3787\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1220 - val_loss: 0.3631\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1124 - val_loss: 0.3477\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1072 - val_loss: 0.3324\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1073 - val_loss: 0.3175\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1022 - val_loss: 0.3024\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0908 - val_loss: 0.2876\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0907 - val_loss: 0.2721\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0811 - val_loss: 0.2565\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0753 - val_loss: 0.2404\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0676 - val_loss: 0.2239\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0599 - val_loss: 0.2062\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0593 - val_loss: 0.1882\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0555 - val_loss: 0.1689\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0443 - val_loss: 0.1485\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0421 - val_loss: 0.1265\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0336 - val_loss: 0.1042\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0248 - val_loss: 0.0807\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0191 - val_loss: 0.0576\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0127 - val_loss: 0.0356\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0076 - val_loss: 0.0176\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0041 - val_loss: 0.0060\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0016 - val_loss: 0.0019\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 9.5495e-04 - val_loss: 0.0011\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0013 - val_loss: 0.0011\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0012 - val_loss: 0.0010\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0011 - val_loss: 0.0010\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0010 - val_loss: 0.0010\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 9.3023e-04 - val_loss: 0.0010\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.4700e-04 - val_loss: 0.0011\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0010 - val_loss: 0.0010\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0010 - val_loss: 0.0010\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.3465e-04 - val_loss: 0.0011\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0011 - val_loss: 0.0010\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 9.4157e-04 - val_loss: 0.0011\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0010\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0010\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0010\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.3498e-04 - val_loss: 0.0010\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.4152e-04 - val_loss: 0.0011\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.6503e-04 - val_loss: 9.9494e-04\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.6069e-04 - val_loss: 0.0010\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0010\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.8126e-04 - val_loss: 0.0010\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.9178e-04 - val_loss: 9.8843e-04\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.9824e-04 - val_loss: 0.0010\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0012 - val_loss: 9.9322e-04\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.3183e-04 - val_loss: 9.9306e-04\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 9.8956e-04\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.9820e-04 - val_loss: 9.7544e-04\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0010\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.5275e-04 - val_loss: 9.7202e-04\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.5437e-04 - val_loss: 9.7013e-04\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 9.9236e-04\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 9.8266e-04\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0010\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0010\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0010 - val_loss: 9.6643e-04\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.7228e-04 - val_loss: 9.7943e-04\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 0.0010\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.8952e-04 - val_loss: 9.9345e-04\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.6441e-04 - val_loss: 0.0010\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 9.5715e-04\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.1440e-04 - val_loss: 9.5587e-04\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0010 - val_loss: 9.5359e-04\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.9705e-04 - val_loss: 0.0010\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.8141e-04 - val_loss: 9.5845e-04\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.0580e-04 - val_loss: 9.7872e-04\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 9.6302e-04\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.2168e-04 - val_loss: 9.9841e-04\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.4501e-04 - val_loss: 9.5842e-04\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.0010\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.6538e-04 - val_loss: 9.7121e-04\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 9.5735e-04\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.9046e-04 - val_loss: 0.0010\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.0556e-04 - val_loss: 9.3814e-04\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.6276e-04 - val_loss: 9.7913e-04\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.8824e-04 - val_loss: 9.9056e-04\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.5712e-04 - val_loss: 9.5611e-04\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.5422e-04 - val_loss: 9.3677e-04\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.2081e-04 - val_loss: 9.9013e-04\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.4727e-04 - val_loss: 9.4708e-04\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 9.8517e-04 - val_loss: 9.4331e-04\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0010 - val_loss: 0.0010\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 8.9161e-04 - val_loss: 0.0010\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.8921e-04 - val_loss: 9.4467e-04\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 9.6831e-04 - val_loss: 9.2018e-04\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.7848e-04 - val_loss: 9.7572e-04\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 9.7463e-04 - val_loss: 9.8032e-04\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 9.5190e-04 - val_loss: 9.7913e-04\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.8836e-04 - val_loss: 9.7617e-04\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0010 - val_loss: 9.2833e-04\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0011 - val_loss: 9.7670e-04\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 8.4458e-04 - val_loss: 9.8992e-04\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 9.4177e-04 - val_loss: 9.4450e-04\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.3006e-04 - val_loss: 9.8174e-04\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 9.0698e-04 - val_loss: 9.0994e-04\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0011 - val_loss: 9.7501e-04\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.3620e-04 - val_loss: 9.3147e-04\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0010\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.7579e-04 - val_loss: 9.9577e-04\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 9.0351e-04\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.8817e-04 - val_loss: 9.7561e-04\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.9655e-04 - val_loss: 9.0246e-04\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.7624e-04 - val_loss: 9.3588e-04\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.8796e-04 - val_loss: 9.8357e-04\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.0000e-04 - val_loss: 9.2044e-04\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.8747e-04 - val_loss: 9.9426e-04\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.6851e-04 - val_loss: 9.9110e-04\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.8920e-04 - val_loss: 9.0149e-04\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.8404e-04 - val_loss: 0.0010\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.1330e-04 - val_loss: 8.9507e-04\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.7938e-04 - val_loss: 9.0796e-04\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.0062e-04 - val_loss: 9.3452e-04\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.7328e-04 - val_loss: 0.0010\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.8693e-04 - val_loss: 9.0147e-04\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.0100e-04 - val_loss: 9.0388e-04\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.9013e-04 - val_loss: 9.1632e-04\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.2214e-04 - val_loss: 9.7058e-04\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.8201e-04 - val_loss: 0.0010\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.6313e-04 - val_loss: 9.4185e-04\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.3473e-04 - val_loss: 9.0118e-04\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.8879e-04 - val_loss: 9.1959e-04\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.9205e-04 - val_loss: 0.0011\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.5779e-04 - val_loss: 9.2068e-04\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.1340e-04 - val_loss: 9.5350e-04\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.2426e-04 - val_loss: 0.0010\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.4795e-04 - val_loss: 9.0815e-04\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.9759e-04 - val_loss: 9.6082e-04\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.8258e-04 - val_loss: 9.0218e-04\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.8276e-04 - val_loss: 9.8177e-04\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.8886e-04 - val_loss: 8.9721e-04\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.5934e-04 - val_loss: 0.0011\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.0801e-04 - val_loss: 8.9836e-04\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.8000e-04 - val_loss: 0.0010\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.4331e-04 - val_loss: 9.0552e-04\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.2586e-04 - val_loss: 9.2249e-04\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.4468e-04 - val_loss: 0.0010\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.4494e-04 - val_loss: 9.4968e-04\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.8540e-04 - val_loss: 0.0011\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.6571e-04 - val_loss: 0.0010\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.3167e-04 - val_loss: 9.3894e-04\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.8506e-04 - val_loss: 9.2392e-04\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.4713e-04 - val_loss: 0.0011\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.1926e-04 - val_loss: 9.1283e-04\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.7809e-04 - val_loss: 0.0010\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.3184e-04 - val_loss: 9.1962e-04\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.7230e-04 - val_loss: 9.4846e-04\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.2717e-04 - val_loss: 0.0010\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.5421e-04 - val_loss: 9.4445e-04\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.4326e-04 - val_loss: 9.0989e-04\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.1194e-04 - val_loss: 0.0011\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.7232e-04 - val_loss: 8.7854e-04\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.9690e-04 - val_loss: 9.6466e-04\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.9814e-04 - val_loss: 0.0010\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 8.3643e-04 - val_loss: 9.0162e-04\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.2696e-04 - val_loss: 0.0010\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 8.7280e-04 - val_loss: 9.9628e-04\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 9.4408e-04 - val_loss: 8.8835e-04\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.3580e-04 - val_loss: 0.0011\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.2790e-04 - val_loss: 9.2818e-04\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 9.4057e-04 - val_loss: 0.0011\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 8.4765e-04 - val_loss: 9.6567e-04\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.3383e-04 - val_loss: 9.1911e-04\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.8357e-04 - val_loss: 9.5537e-04\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 7.8895e-04 - val_loss: 8.5135e-04\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 8.3095e-04 - val_loss: 9.2838e-04\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.1853e-04 - val_loss: 0.0011\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 8.2750e-04 - val_loss: 9.7692e-04\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.0782e-04 - val_loss: 9.6733e-04\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.0036e-04 - val_loss: 9.4682e-04\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.6408e-04 - val_loss: 9.6325e-04\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.3948e-04 - val_loss: 9.0813e-04\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.8922e-04 - val_loss: 8.9173e-04\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.9927e-04 - val_loss: 0.0010\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.5285e-04 - val_loss: 9.9454e-04\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.9038e-04 - val_loss: 9.5798e-04\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.8441e-04 - val_loss: 9.3798e-04\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.2595e-04 - val_loss: 9.5528e-04\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.2217e-04 - val_loss: 0.0011\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.1045e-04 - val_loss: 8.8124e-04\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.0571e-04 - val_loss: 0.0010\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.5264e-04 - val_loss: 0.0011\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.6691e-04 - val_loss: 9.4664e-04\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.6986e-04 - val_loss: 9.3057e-04\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Train predicted data:  (367, 1)\n",
            "Test predicted data:  (367, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t Selling 0.028674698315319165 units of gold on 2020-03-01 00:00:00\n",
            "\t\t Price: 1609.85, unit: 46.16196308291656\n",
            "2019-04-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 0.1965 - val_loss: 0.4809\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1515 - val_loss: 0.3961\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1345 - val_loss: 0.3164\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0986 - val_loss: 0.2428\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0664 - val_loss: 0.1747\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0473 - val_loss: 0.1136\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0254 - val_loss: 0.0644\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0108 - val_loss: 0.0297\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0040 - val_loss: 0.0113\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0013 - val_loss: 0.0059\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0050\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0014 - val_loss: 0.0052\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0012 - val_loss: 0.0057\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0014 - val_loss: 0.0054\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0011 - val_loss: 0.0056\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0011 - val_loss: 0.0054\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0012 - val_loss: 0.0051\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0012 - val_loss: 0.0054\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0012 - val_loss: 0.0056\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0012 - val_loss: 0.0053\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0011 - val_loss: 0.0053\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0013 - val_loss: 0.0054\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0011 - val_loss: 0.0052\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0012 - val_loss: 0.0055\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.7621e-04 - val_loss: 0.0053\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0012 - val_loss: 0.0054\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0012 - val_loss: 0.0055\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0011 - val_loss: 0.0053\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0011 - val_loss: 0.0053\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 9.9732e-04 - val_loss: 0.0052\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0012 - val_loss: 0.0052\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0011 - val_loss: 0.0054\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.6168e-04 - val_loss: 0.0054\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0053\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0054\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0053\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0052\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0053\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0013 - val_loss: 0.0051\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0051\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.4667e-04 - val_loss: 0.0052\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.2822e-04 - val_loss: 0.0051\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0052\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0010 - val_loss: 0.0051\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.6405e-04 - val_loss: 0.0053\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.5575e-04 - val_loss: 0.0050\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.9764e-04 - val_loss: 0.0051\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.7995e-04 - val_loss: 0.0052\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0050\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.5067e-04 - val_loss: 0.0053\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0052\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.8181e-04 - val_loss: 0.0051\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0050\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0012 - val_loss: 0.0051\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0010 - val_loss: 0.0051\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.8644e-04 - val_loss: 0.0050\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.9889e-04 - val_loss: 0.0052\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.6449e-04 - val_loss: 0.0050\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0050\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.8825e-04 - val_loss: 0.0051\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.0416e-04 - val_loss: 0.0050\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.7005e-04 - val_loss: 0.0048\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0049\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.2040e-04 - val_loss: 0.0050\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.3114e-04 - val_loss: 0.0048\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.7081e-04 - val_loss: 0.0051\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.5330e-04 - val_loss: 0.0048\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.6014e-04 - val_loss: 0.0051\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.1092e-04 - val_loss: 0.0049\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.1867e-04 - val_loss: 0.0047\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0050\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.4839e-04 - val_loss: 0.0049\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.3610e-04 - val_loss: 0.0048\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0010 - val_loss: 0.0050\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.7685e-04 - val_loss: 0.0047\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.3032e-04 - val_loss: 0.0051\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.1442e-04 - val_loss: 0.0047\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.8958e-04 - val_loss: 0.0046\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.7847e-04 - val_loss: 0.0047\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.6892e-04 - val_loss: 0.0047\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.1773e-04 - val_loss: 0.0047\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.0049\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0049\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.8565e-04 - val_loss: 0.0045\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.1939e-04 - val_loss: 0.0049\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.9145e-04 - val_loss: 0.0045\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.9195e-04 - val_loss: 0.0046\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.6963e-04 - val_loss: 0.0046\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.3985e-04 - val_loss: 0.0046\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.9377e-04 - val_loss: 0.0045\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.4540e-04 - val_loss: 0.0046\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 8.4246e-04 - val_loss: 0.0048\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.2864e-04 - val_loss: 0.0044\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 8.3189e-04 - val_loss: 0.0046\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.3531e-04 - val_loss: 0.0046\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.4001e-04 - val_loss: 0.0045\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.4097e-04 - val_loss: 0.0047\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 9.3647e-04 - val_loss: 0.0047\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.1390e-04 - val_loss: 0.0043\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.7703e-04 - val_loss: 0.0047\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 8.2900e-04 - val_loss: 0.0044\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 9.3045e-04 - val_loss: 0.0045\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 8.6752e-04 - val_loss: 0.0044\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.6885e-04 - val_loss: 0.0045\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.0662e-04 - val_loss: 0.0045\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.9649e-04 - val_loss: 0.0046\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.6391e-04 - val_loss: 0.0043\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.6410e-04 - val_loss: 0.0045\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.6251e-04 - val_loss: 0.0045\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.4550e-04 - val_loss: 0.0043\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.0734e-04 - val_loss: 0.0044\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.0952e-04 - val_loss: 0.0043\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.8256e-04 - val_loss: 0.0043\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.5100e-04 - val_loss: 0.0042\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.5995e-04 - val_loss: 0.0044\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.4879e-04 - val_loss: 0.0040\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.9288e-04 - val_loss: 0.0046\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.8310e-04 - val_loss: 0.0042\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.8237e-04 - val_loss: 0.0042\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.0772e-04 - val_loss: 0.0043\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.7362e-04 - val_loss: 0.0040\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.1940e-04 - val_loss: 0.0044\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.0842e-04 - val_loss: 0.0043\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.6650e-04 - val_loss: 0.0043\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.1018e-04 - val_loss: 0.0041\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.2289e-04 - val_loss: 0.0040\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 7.2755e-04 - val_loss: 0.0039\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.1266e-04 - val_loss: 0.0041\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.5027e-04 - val_loss: 0.0040\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.5708e-04 - val_loss: 0.0043\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.5489e-04 - val_loss: 0.0039\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.1884e-04 - val_loss: 0.0041\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.9487e-04 - val_loss: 0.0042\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.3660e-04 - val_loss: 0.0039\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.0438e-04 - val_loss: 0.0038\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.9060e-04 - val_loss: 0.0041\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 7.8934e-04 - val_loss: 0.0038\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.7694e-04 - val_loss: 0.0039\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.4871e-04 - val_loss: 0.0039\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.1235e-04 - val_loss: 0.0037\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.6545e-04 - val_loss: 0.0038\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.7903e-04 - val_loss: 0.0038\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.0338e-04 - val_loss: 0.0038\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.6881e-04 - val_loss: 0.0042\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.2881e-04 - val_loss: 0.0038\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.2774e-04 - val_loss: 0.0039\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.8754e-04 - val_loss: 0.0043\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.6361e-04 - val_loss: 0.0037\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.0309e-04 - val_loss: 0.0037\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.2553e-04 - val_loss: 0.0038\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.5831e-04 - val_loss: 0.0036\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.0079e-04 - val_loss: 0.0041\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.1551e-04 - val_loss: 0.0035\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.9532e-04 - val_loss: 0.0038\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 7.5855e-04 - val_loss: 0.0037\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.2281e-04 - val_loss: 0.0035\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 6.8614e-04 - val_loss: 0.0036\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 7.8414e-04 - val_loss: 0.0038\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.2833e-04 - val_loss: 0.0039\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.5865e-04 - val_loss: 0.0037\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 8.5664e-04 - val_loss: 0.0037\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 8.8167e-04 - val_loss: 0.0036\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 7.2818e-04 - val_loss: 0.0035\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.9087e-04 - val_loss: 0.0037\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 8.6784e-04 - val_loss: 0.0035\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 7.1608e-04 - val_loss: 0.0037\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.8275e-04 - val_loss: 0.0034\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 7.7169e-04 - val_loss: 0.0035\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.7220e-04 - val_loss: 0.0036\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.8028e-04 - val_loss: 0.0034\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 7.3760e-04 - val_loss: 0.0037\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.6372e-04 - val_loss: 0.0036\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.9281e-04 - val_loss: 0.0034\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.9833e-04 - val_loss: 0.0035\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.2535e-04 - val_loss: 0.0039\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.9004e-04 - val_loss: 0.0035\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.7867e-04 - val_loss: 0.0034\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.3320e-04 - val_loss: 0.0034\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.1840e-04 - val_loss: 0.0033\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.2017e-04 - val_loss: 0.0036\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.8330e-04 - val_loss: 0.0039\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.8779e-04 - val_loss: 0.0033\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.9651e-04 - val_loss: 0.0035\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.6184e-04 - val_loss: 0.0033\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.9169e-04 - val_loss: 0.0034\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.6558e-04 - val_loss: 0.0035\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.1234e-04 - val_loss: 0.0033\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.1568e-04 - val_loss: 0.0033\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.2758e-04 - val_loss: 0.0034\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.5326e-04 - val_loss: 0.0032\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.5922e-04 - val_loss: 0.0034\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.1532e-04 - val_loss: 0.0034\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.9525e-04 - val_loss: 0.0033\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.1099e-04 - val_loss: 0.0035\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.9457e-04 - val_loss: 0.0034\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.6669e-04 - val_loss: 0.0034\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.6712e-04 - val_loss: 0.0031\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.9703e-04 - val_loss: 0.0033\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.9482e-04 - val_loss: 0.0032\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.1072e-04 - val_loss: 0.0032\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (367, 1)\n",
            "Test predicted data:  (367, 1)\n",
            "Output of predicted next days:  30\n",
            "2019-05-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 0.1586 - val_loss: 0.4654\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1420 - val_loss: 0.4280\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1236 - val_loss: 0.3941\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1125 - val_loss: 0.3610\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0980 - val_loss: 0.3138\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0725 - val_loss: 0.2535\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0503 - val_loss: 0.1889\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0328 - val_loss: 0.1242\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0170 - val_loss: 0.0686\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0094 - val_loss: 0.0351\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0081 - val_loss: 0.0253\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0068 - val_loss: 0.0234\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0054 - val_loss: 0.0209\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0047 - val_loss: 0.0170\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0038 - val_loss: 0.0133\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0030 - val_loss: 0.0116\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0024 - val_loss: 0.0104\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0093\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0018 - val_loss: 0.0083\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0084\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0082\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0085\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0017 - val_loss: 0.0085\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0094\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015 - val_loss: 0.0100\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0014 - val_loss: 0.0108\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0012 - val_loss: 0.0119\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0012 - val_loss: 0.0128\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.0135\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0138\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0141\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0148\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0151\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0011 - val_loss: 0.0149\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 0.0145\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0155\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0145\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.6786e-04 - val_loss: 0.0153\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.9751e-04 - val_loss: 0.0151\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0148\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0146\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0139\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.2657e-04 - val_loss: 0.0145\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0139\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0143\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0143\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0145\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0142\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0010 - val_loss: 0.0138\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0135\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0011 - val_loss: 0.0131\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0141\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0011 - val_loss: 0.0134\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0010 - val_loss: 0.0134\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0136\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0011 - val_loss: 0.0137\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0133\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0133\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0132\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0128\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.7820e-04 - val_loss: 0.0136\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.0118\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0010 - val_loss: 0.0134\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0138\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0013 - val_loss: 0.0138\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0011 - val_loss: 0.0134\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0010 - val_loss: 0.0128\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0011 - val_loss: 0.0139\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0010 - val_loss: 0.0133\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.5560e-04 - val_loss: 0.0135\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0011 - val_loss: 0.0134\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0011 - val_loss: 0.0133\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.7324e-04 - val_loss: 0.0127\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0010 - val_loss: 0.0123\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0011 - val_loss: 0.0128\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0011 - val_loss: 0.0122\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0011 - val_loss: 0.0126\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 9.9177e-04 - val_loss: 0.0125\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.4474e-04 - val_loss: 0.0123\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 9.6423e-04 - val_loss: 0.0124\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 9.2838e-04 - val_loss: 0.0118\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0011 - val_loss: 0.0125\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 9.6648e-04 - val_loss: 0.0127\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0010 - val_loss: 0.0121\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 9.8640e-04 - val_loss: 0.0127\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0011 - val_loss: 0.0126\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0010 - val_loss: 0.0110\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0011 - val_loss: 0.0121\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0012 - val_loss: 0.0119\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0112\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0119\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0121\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0010 - val_loss: 0.0113\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.4010e-04 - val_loss: 0.0116\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.7807e-04 - val_loss: 0.0111\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.0117\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.4332e-04 - val_loss: 0.0118\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0010 - val_loss: 0.0105\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0010 - val_loss: 0.0123\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.8848e-04 - val_loss: 0.0110\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0117\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0118\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0011 - val_loss: 0.0117\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.9200e-04 - val_loss: 0.0111\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.9628e-04 - val_loss: 0.0109\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0117\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.1278e-04 - val_loss: 0.0107\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.2725e-04 - val_loss: 0.0111\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.2683e-04 - val_loss: 0.0110\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.8691e-04 - val_loss: 0.0111\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0011 - val_loss: 0.0110\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.6927e-04 - val_loss: 0.0105\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.2483e-04 - val_loss: 0.0104\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.3287e-04 - val_loss: 0.0109\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.3024e-04 - val_loss: 0.0099\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.7028e-04 - val_loss: 0.0108\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.5201e-04 - val_loss: 0.0099\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.7697e-04 - val_loss: 0.0107\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0011 - val_loss: 0.0100\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.8354e-04 - val_loss: 0.0107\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.6705e-04 - val_loss: 0.0097\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.8595e-04 - val_loss: 0.0103\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.0095\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0010 - val_loss: 0.0099\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.7599e-04 - val_loss: 0.0096\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.9514e-04 - val_loss: 0.0091\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.3404e-04 - val_loss: 0.0100\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.5005e-04 - val_loss: 0.0094\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.4339e-04 - val_loss: 0.0097\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.2089e-04 - val_loss: 0.0095\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0106\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0102\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.1235e-04 - val_loss: 0.0095\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 0.0098\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.2312e-04 - val_loss: 0.0092\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.4927e-04 - val_loss: 0.0098\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.9062e-04 - val_loss: 0.0097\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 7.6519e-04 - val_loss: 0.0095\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 8.4797e-04 - val_loss: 0.0096\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.6989e-04 - val_loss: 0.0090\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.2696e-04 - val_loss: 0.0090\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 9.7456e-04 - val_loss: 0.0086\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 9.3749e-04 - val_loss: 0.0102\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 8.6823e-04 - val_loss: 0.0087\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0010 - val_loss: 0.0098\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0011 - val_loss: 0.0093\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.9352e-04 - val_loss: 0.0099\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 7.9104e-04 - val_loss: 0.0088\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 8.0942e-04 - val_loss: 0.0093\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.9100e-04 - val_loss: 0.0091\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.1783e-04 - val_loss: 0.0088\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 9.0810e-04 - val_loss: 0.0087\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 8.1411e-04 - val_loss: 0.0089\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 8.3483e-04 - val_loss: 0.0084\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 8.0931e-04 - val_loss: 0.0085\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.0953e-04 - val_loss: 0.0088\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.5553e-04 - val_loss: 0.0086\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 7.9992e-04 - val_loss: 0.0085\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.7564e-04 - val_loss: 0.0085\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.5971e-04 - val_loss: 0.0086\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.9591e-04 - val_loss: 0.0086\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.7474e-04 - val_loss: 0.0083\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.3719e-04 - val_loss: 0.0080\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.2331e-04 - val_loss: 0.0079\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.7181e-04 - val_loss: 0.0084\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.6965e-04 - val_loss: 0.0081\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.7618e-04 - val_loss: 0.0080\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.0184e-04 - val_loss: 0.0077\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.8189e-04 - val_loss: 0.0079\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.7818e-04 - val_loss: 0.0080\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.9710e-04 - val_loss: 0.0081\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.9636e-04 - val_loss: 0.0081\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.7179e-04 - val_loss: 0.0078\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.1942e-04 - val_loss: 0.0082\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.2398e-04 - val_loss: 0.0086\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.5045e-04 - val_loss: 0.0080\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.3395e-04 - val_loss: 0.0091\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.8943e-04 - val_loss: 0.0080\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.2354e-04 - val_loss: 0.0080\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.6142e-04 - val_loss: 0.0092\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.5117e-04 - val_loss: 0.0080\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.9235e-04 - val_loss: 0.0090\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.1432e-04 - val_loss: 0.0088\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.0888e-04 - val_loss: 0.0087\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.6717e-04 - val_loss: 0.0085\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.7991e-04 - val_loss: 0.0089\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.1423e-04 - val_loss: 0.0080\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.5174e-04 - val_loss: 0.0088\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.8214e-04 - val_loss: 0.0087\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.7577e-04 - val_loss: 0.0093\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.3617e-04 - val_loss: 0.0093\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.4696e-04 - val_loss: 0.0092\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.8195e-04 - val_loss: 0.0094\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.7594e-04 - val_loss: 0.0083\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.0219e-04 - val_loss: 0.0086\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.1170e-04 - val_loss: 0.0085\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.4846e-04 - val_loss: 0.0085\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.4395e-04 - val_loss: 0.0086\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.8645e-04 - val_loss: 0.0091\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.3427e-04 - val_loss: 0.0084\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (367, 1)\n",
            "Test predicted data:  (367, 1)\n",
            "Output of predicted next days:  30\n",
            "2019-06-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - loss: 0.1294 - val_loss: 0.4379\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1042 - val_loss: 0.3589\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0828 - val_loss: 0.2892\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0588 - val_loss: 0.2302\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0409 - val_loss: 0.1745\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0261 - val_loss: 0.1231\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0154 - val_loss: 0.0786\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0073 - val_loss: 0.0436\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0029 - val_loss: 0.0223\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.0137\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0126\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0150\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.0191\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0023 - val_loss: 0.0213\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0021 - val_loss: 0.0216\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0201\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0022 - val_loss: 0.0183\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0169\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0163\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.0157\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.0152\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0021 - val_loss: 0.0153\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.0151\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.0145\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0018 - val_loss: 0.0147\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021 - val_loss: 0.0135\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019 - val_loss: 0.0129\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0124\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.0114\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.0118\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - val_loss: 0.0115\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0018 - val_loss: 0.0106\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0103\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0095\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0097\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0015 - val_loss: 0.0093\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0088\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0087\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0087\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0085\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0082\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - val_loss: 0.0081\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 0.0080\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0078\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0078\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0078\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0079\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015 - val_loss: 0.0079\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0014 - val_loss: 0.0082\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0015 - val_loss: 0.0083\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0015 - val_loss: 0.0088\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0015 - val_loss: 0.0089\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0014 - val_loss: 0.0088\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.0095\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0105\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0012 - val_loss: 0.0109\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0015 - val_loss: 0.0105\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0014 - val_loss: 0.0114\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0014 - val_loss: 0.0127\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0012 - val_loss: 0.0118\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0014 - val_loss: 0.0134\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0013 - val_loss: 0.0134\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0013 - val_loss: 0.0144\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0013 - val_loss: 0.0147\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0012 - val_loss: 0.0170\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0012 - val_loss: 0.0149\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0014 - val_loss: 0.0160\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0012 - val_loss: 0.0182\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0013 - val_loss: 0.0181\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0013 - val_loss: 0.0188\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0012 - val_loss: 0.0180\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0012 - val_loss: 0.0192\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0208\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0013 - val_loss: 0.0214\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0201\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0014 - val_loss: 0.0216\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0012 - val_loss: 0.0209\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0012 - val_loss: 0.0238\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0219\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0011 - val_loss: 0.0231\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0013 - val_loss: 0.0252\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 0.0213\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0217\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0013 - val_loss: 0.0239\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0013 - val_loss: 0.0213\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0012 - val_loss: 0.0223\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0255\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0012 - val_loss: 0.0215\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0225\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0241\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0203\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0012 - val_loss: 0.0223\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0221\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0227\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0252\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0198\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.0238\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 0.0239\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0012 - val_loss: 0.0217\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0221\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.3333e-04 - val_loss: 0.0233\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0243\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0224\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0251\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0230\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0240\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0243\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0237\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0258\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.9902e-04 - val_loss: 0.0228\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0247\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.0242\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0011 - val_loss: 0.0240\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0241\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0222\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0011 - val_loss: 0.0258\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0215\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0248\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0238\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0210\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.8179e-04 - val_loss: 0.0237\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0011 - val_loss: 0.0209\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0011 - val_loss: 0.0228\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.8480e-04 - val_loss: 0.0223\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0010 - val_loss: 0.0217\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0010 - val_loss: 0.0218\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 9.1070e-04 - val_loss: 0.0221\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0010 - val_loss: 0.0231\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0011 - val_loss: 0.0213\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.6159e-04 - val_loss: 0.0247\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.2460e-04 - val_loss: 0.0197\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.7184e-04 - val_loss: 0.0225\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0010 - val_loss: 0.0206\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 9.2521e-04 - val_loss: 0.0218\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 9.7092e-04 - val_loss: 0.0208\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.9676e-04 - val_loss: 0.0196\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.9041e-04 - val_loss: 0.0213\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 9.1584e-04 - val_loss: 0.0204\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.6471e-04 - val_loss: 0.0199\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.7006e-04 - val_loss: 0.0203\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.4036e-04 - val_loss: 0.0188\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.8042e-04 - val_loss: 0.0192\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.0960e-04 - val_loss: 0.0195\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.4196e-04 - val_loss: 0.0190\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0172\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.6986e-04 - val_loss: 0.0187\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.5077e-04 - val_loss: 0.0170\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0201\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.4541e-04 - val_loss: 0.0159\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0180\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.7455e-04 - val_loss: 0.0177\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.7591e-04 - val_loss: 0.0151\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.5987e-04 - val_loss: 0.0182\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.3198e-04 - val_loss: 0.0155\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.4288e-04 - val_loss: 0.0167\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.5866e-04 - val_loss: 0.0172\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.7427e-04 - val_loss: 0.0162\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 0.0151\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.9847e-04 - val_loss: 0.0165\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.3183e-04 - val_loss: 0.0161\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.0888e-04 - val_loss: 0.0156\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.3518e-04 - val_loss: 0.0162\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.6013e-04 - val_loss: 0.0148\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.1887e-04 - val_loss: 0.0162\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.7149e-04 - val_loss: 0.0155\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.3977e-04 - val_loss: 0.0138\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0010 - val_loss: 0.0156\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.0069e-04 - val_loss: 0.0152\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.7010e-04 - val_loss: 0.0147\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.9972e-04 - val_loss: 0.0136\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.3423e-04 - val_loss: 0.0125\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.9456e-04 - val_loss: 0.0151\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.8208e-04 - val_loss: 0.0120\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.2038e-04 - val_loss: 0.0153\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.0936e-04 - val_loss: 0.0118\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.0928e-04 - val_loss: 0.0144\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.3232e-04 - val_loss: 0.0132\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.3795e-04 - val_loss: 0.0124\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.7408e-04 - val_loss: 0.0136\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.4438e-04 - val_loss: 0.0128\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.4468e-04 - val_loss: 0.0130\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.1468e-04 - val_loss: 0.0125\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.1125e-04 - val_loss: 0.0113\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.6540e-04 - val_loss: 0.0118\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.3397e-04 - val_loss: 0.0125\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.0588e-04 - val_loss: 0.0108\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.8348e-04 - val_loss: 0.0105\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.7620e-04 - val_loss: 0.0104\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.6825e-04 - val_loss: 0.0097\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0081\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.5159e-04 - val_loss: 0.0105\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.1015e-04 - val_loss: 0.0079\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 9.4012e-04 - val_loss: 0.0111\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.2132e-04 - val_loss: 0.0090\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.8547e-04 - val_loss: 0.0111\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.8841e-04 - val_loss: 0.0093\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.7187e-04 - val_loss: 0.0094\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.3953e-04 - val_loss: 0.0100\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.9770e-04 - val_loss: 0.0084\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.5801e-04 - val_loss: 0.0092\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "Train predicted data:  (367, 1)\n",
            "Test predicted data:  (367, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1730.6, Last: 13368616.385449218\n",
            "\t\t Buying 0.027994325815047343 units of gold on 2020-06-01 00:00:00\n",
            "\t\t Price: 1730.6, unit: 48.44698025552093\n",
            "2019-07-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 0.1222 - val_loss: 0.7443\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0984 - val_loss: 0.6717\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0858 - val_loss: 0.6056\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0693 - val_loss: 0.5435\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0582 - val_loss: 0.4861\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0518 - val_loss: 0.4311\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0407 - val_loss: 0.3788\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0309 - val_loss: 0.3289\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0276 - val_loss: 0.2787\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0230 - val_loss: 0.2295\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0158 - val_loss: 0.1827\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0127 - val_loss: 0.1399\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0085 - val_loss: 0.1012\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0061 - val_loss: 0.0676\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0047 - val_loss: 0.0447\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0318\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0044 - val_loss: 0.0295\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0042 - val_loss: 0.0296\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0045 - val_loss: 0.0312\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0043 - val_loss: 0.0302\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0042 - val_loss: 0.0299\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0043 - val_loss: 0.0291\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0269\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038 - val_loss: 0.0233\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0210\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0041 - val_loss: 0.0193\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0039 - val_loss: 0.0183\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.0176\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0039 - val_loss: 0.0164\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0039 - val_loss: 0.0147\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0146\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0036 - val_loss: 0.0149\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0152\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0031 - val_loss: 0.0157\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.0170\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0197\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0233\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0029 - val_loss: 0.0276\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0030 - val_loss: 0.0306\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0030 - val_loss: 0.0337\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0420\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0028 - val_loss: 0.0440\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0028 - val_loss: 0.0606\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0028 - val_loss: 0.0676\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0027 - val_loss: 0.0766\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0025 - val_loss: 0.0740\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0025 - val_loss: 0.0942\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0028 - val_loss: 0.1003\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0027 - val_loss: 0.1203\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0025 - val_loss: 0.1375\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0026 - val_loss: 0.1580\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0027 - val_loss: 0.1656\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0023 - val_loss: 0.1918\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0025 - val_loss: 0.2238\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0021 - val_loss: 0.2359\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0023 - val_loss: 0.2241\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0025 - val_loss: 0.2434\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0023 - val_loss: 0.2665\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.3234\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.3104\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.3367\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0024 - val_loss: 0.3753\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.4078\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.3560\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.4100\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.4459\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0025 - val_loss: 0.4644\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.4484\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0020 - val_loss: 0.4271\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.5156\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.5014\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.5184\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.4362\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.4908\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.5390\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0022 - val_loss: 0.5204\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.5058\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.4974\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.5058\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.5056\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0020 - val_loss: 0.5123\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.5373\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0023 - val_loss: 0.5440\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0021 - val_loss: 0.5121\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.5321\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.5322\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0023 - val_loss: 0.5630\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.5345\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.4923\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.5019\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.5360\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.5371\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0021 - val_loss: 0.5358\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.4969\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.4300\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0021 - val_loss: 0.5055\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.4894\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0021 - val_loss: 0.5092\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.4837\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.4618\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.5416\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.4838\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.4062\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.4523\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0022 - val_loss: 0.5051\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.4349\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.4061\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0019 - val_loss: 0.4584\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0022 - val_loss: 0.4809\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.4165\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0021 - val_loss: 0.3850\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0019 - val_loss: 0.4219\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0022 - val_loss: 0.4199\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0022 - val_loss: 0.4019\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0023 - val_loss: 0.4071\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0020 - val_loss: 0.4126\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0021 - val_loss: 0.3773\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0019 - val_loss: 0.3847\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0020 - val_loss: 0.4082\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0018 - val_loss: 0.3924\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0019 - val_loss: 0.3867\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0019 - val_loss: 0.3816\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0022 - val_loss: 0.3931\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0022 - val_loss: 0.3870\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0019 - val_loss: 0.3823\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0019 - val_loss: 0.3660\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0019 - val_loss: 0.3562\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0021 - val_loss: 0.3718\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0020 - val_loss: 0.3487\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.3603\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.3701\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.3392\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0019 - val_loss: 0.3146\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.3730\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.3280\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.3578\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.3154\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0019 - val_loss: 0.3305\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.3133\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0019 - val_loss: 0.3231\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0018 - val_loss: 0.3201\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.3070\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.2926\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0020 - val_loss: 0.3246\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0018 - val_loss: 0.3083\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0018 - val_loss: 0.3018\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019 - val_loss: 0.3021\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.2991\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.2938\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.2776\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.2721\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0019 - val_loss: 0.2901\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0018 - val_loss: 0.3037\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019 - val_loss: 0.2640\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.2566\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020 - val_loss: 0.2695\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.2757\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0018 - val_loss: 0.2589\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.2235\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.2423\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.2644\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.2546\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.2598\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.2417\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019 - val_loss: 0.2443\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.2440\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.2576\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0018 - val_loss: 0.2364\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.2488\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.2340\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.2445\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0019 - val_loss: 0.2159\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0018 - val_loss: 0.2154\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.2398\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.2287\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.2033\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.1937\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.2263\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0016 - val_loss: 0.2024\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0018 - val_loss: 0.1915\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0015 - val_loss: 0.2087\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0019 - val_loss: 0.2281\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0017 - val_loss: 0.2048\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0018 - val_loss: 0.1790\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0017 - val_loss: 0.1965\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0017 - val_loss: 0.1996\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0019 - val_loss: 0.1851\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0019 - val_loss: 0.1847\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0016 - val_loss: 0.1806\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0018 - val_loss: 0.1978\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0017 - val_loss: 0.1897\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0016 - val_loss: 0.1816\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0015 - val_loss: 0.1717\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0016 - val_loss: 0.1790\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0017 - val_loss: 0.1819\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0016 - val_loss: 0.1698\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0017 - val_loss: 0.1690\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0014 - val_loss: 0.1679\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0015 - val_loss: 0.1684\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.1760\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (367, 1)\n",
            "Test predicted data:  (367, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1771.05, Last: 8765681.82724609\n",
            "\t\t Buying 0.02597352291161578 units of gold on 2020-07-01 00:00:00\n",
            "\t\t Price: 1771.05, unit: 46.00040775261712\n",
            "2019-08-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0811 - val_loss: 0.4571\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0502 - val_loss: 0.3748\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0384 - val_loss: 0.3097\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0318 - val_loss: 0.2584\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0208 - val_loss: 0.2173\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0131 - val_loss: 0.1815\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0113 - val_loss: 0.1506\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0080 - val_loss: 0.1251\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0060 - val_loss: 0.1045\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0057 - val_loss: 0.0894\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0043 - val_loss: 0.0807\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0046 - val_loss: 0.0747\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0045 - val_loss: 0.0717\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0045 - val_loss: 0.0705\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0041 - val_loss: 0.0686\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0038 - val_loss: 0.0680\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0037 - val_loss: 0.0649\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0041 - val_loss: 0.0624\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036 - val_loss: 0.0594\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0039 - val_loss: 0.0562\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0035 - val_loss: 0.0530\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0030 - val_loss: 0.0508\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.0482\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0027 - val_loss: 0.0433\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0035 - val_loss: 0.0396\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0027 - val_loss: 0.0367\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0028 - val_loss: 0.0352\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0029 - val_loss: 0.0325\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0027 - val_loss: 0.0303\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0024 - val_loss: 0.0273\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0025 - val_loss: 0.0231\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0024 - val_loss: 0.0209\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0023 - val_loss: 0.0193\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0019 - val_loss: 0.0167\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0023 - val_loss: 0.0140\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0020 - val_loss: 0.0121\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0018 - val_loss: 0.0100\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0018 - val_loss: 0.0077\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0016 - val_loss: 0.0064\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0018 - val_loss: 0.0051\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0018 - val_loss: 0.0041\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0015 - val_loss: 0.0032\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0014 - val_loss: 0.0028\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0027\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0013 - val_loss: 0.0032\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 0.0039\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.0053\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0072\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0103\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.0146\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0010 - val_loss: 0.0160\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.7072e-04 - val_loss: 0.0206\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.9849e-04 - val_loss: 0.0243\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.2635e-04 - val_loss: 0.0257\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.8839e-04 - val_loss: 0.0314\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.6004e-04 - val_loss: 0.0386\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.8278e-04 - val_loss: 0.0433\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.3877e-04 - val_loss: 0.0432\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.4869e-04 - val_loss: 0.0498\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.0002e-04 - val_loss: 0.0548\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.5864e-04 - val_loss: 0.0504\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.6967e-04 - val_loss: 0.0580\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.2583e-04 - val_loss: 0.0622\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.0629e-04 - val_loss: 0.0584\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.6407e-04 - val_loss: 0.0634\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.9001e-04 - val_loss: 0.0704\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.8077e-04 - val_loss: 0.0703\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.4320e-04 - val_loss: 0.0636\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.1878e-04 - val_loss: 0.0686\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.9542e-04 - val_loss: 0.0793\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.3004e-04 - val_loss: 0.0736\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.7276e-04 - val_loss: 0.0630\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.0200e-04 - val_loss: 0.0663\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.9014e-04 - val_loss: 0.0710\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.9411e-04 - val_loss: 0.0734\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.9356e-04 - val_loss: 0.0709\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.2144e-04 - val_loss: 0.0590\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.9240e-04 - val_loss: 0.0658\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.6752e-04 - val_loss: 0.0701\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.5826e-04 - val_loss: 0.0630\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.1601e-04 - val_loss: 0.0637\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.2908e-04 - val_loss: 0.0571\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.8037e-04 - val_loss: 0.0635\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.3125e-04 - val_loss: 0.0629\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.7080e-04 - val_loss: 0.0638\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.5389e-04 - val_loss: 0.0601\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.3833e-04 - val_loss: 0.0650\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.6367e-04 - val_loss: 0.0647\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.4127e-04 - val_loss: 0.0594\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.1328e-04 - val_loss: 0.0572\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.2369e-04 - val_loss: 0.0628\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.8441e-04 - val_loss: 0.0665\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.0848e-04 - val_loss: 0.0595\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.7202e-04 - val_loss: 0.0560\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.1234e-04 - val_loss: 0.0612\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 7.6791e-04 - val_loss: 0.0679\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.2587e-04 - val_loss: 0.0469\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 7.6829e-04 - val_loss: 0.0488\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 8.2791e-04 - val_loss: 0.0632\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.6297e-04 - val_loss: 0.0544\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.7877e-04 - val_loss: 0.0512\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.4208e-04 - val_loss: 0.0532\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.9252e-04 - val_loss: 0.0488\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.8963e-04 - val_loss: 0.0522\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.6080e-04 - val_loss: 0.0528\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.2757e-04 - val_loss: 0.0485\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.6582e-04 - val_loss: 0.0486\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.3583e-04 - val_loss: 0.0464\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.1519e-04 - val_loss: 0.0507\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.4484e-04 - val_loss: 0.0436\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.6035e-04 - val_loss: 0.0448\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.0585e-04 - val_loss: 0.0487\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.4589e-04 - val_loss: 0.0472\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 7.7542e-04 - val_loss: 0.0411\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.4595e-04 - val_loss: 0.0508\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.3067e-04 - val_loss: 0.0459\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 7.6998e-04 - val_loss: 0.0432\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.3224e-04 - val_loss: 0.0395\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 7.7499e-04 - val_loss: 0.0410\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.3386e-04 - val_loss: 0.0456\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.8386e-04 - val_loss: 0.0415\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.5088e-04 - val_loss: 0.0362\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.9815e-04 - val_loss: 0.0417\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.1637e-04 - val_loss: 0.0440\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.6886e-04 - val_loss: 0.0436\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.6008e-04 - val_loss: 0.0404\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.6410e-04 - val_loss: 0.0416\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.9644e-04 - val_loss: 0.0409\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.6131e-04 - val_loss: 0.0355\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.2400e-04 - val_loss: 0.0383\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.6303e-04 - val_loss: 0.0376\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.4340e-04 - val_loss: 0.0346\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.2631e-04 - val_loss: 0.0347\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.3503e-04 - val_loss: 0.0389\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.6567e-04 - val_loss: 0.0328\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.3053e-04 - val_loss: 0.0329\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.1201e-04 - val_loss: 0.0374\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.4198e-04 - val_loss: 0.0308\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.4668e-04 - val_loss: 0.0334\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.7561e-04 - val_loss: 0.0350\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.1915e-04 - val_loss: 0.0357\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.2000e-04 - val_loss: 0.0301\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.5846e-04 - val_loss: 0.0311\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.0027e-04 - val_loss: 0.0302\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.6974e-04 - val_loss: 0.0289\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.6840e-04 - val_loss: 0.0309\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.0429e-04 - val_loss: 0.0327\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.8618e-04 - val_loss: 0.0269\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.5883e-04 - val_loss: 0.0295\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.6649e-04 - val_loss: 0.0252\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.6229e-04 - val_loss: 0.0267\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.3057e-04 - val_loss: 0.0240\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.5549e-04 - val_loss: 0.0249\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.7623e-04 - val_loss: 0.0254\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.2899e-04 - val_loss: 0.0263\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.5909e-04 - val_loss: 0.0255\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.1813e-04 - val_loss: 0.0236\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.9663e-04 - val_loss: 0.0244\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.1250e-04 - val_loss: 0.0230\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.7756e-04 - val_loss: 0.0254\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.7911e-04 - val_loss: 0.0236\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.5638e-04 - val_loss: 0.0214\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.8439e-04 - val_loss: 0.0231\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.1885e-04 - val_loss: 0.0223\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 6.7446e-04 - val_loss: 0.0240\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.9255e-04 - val_loss: 0.0203\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.5734e-04 - val_loss: 0.0209\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.3036e-04 - val_loss: 0.0226\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 6.7658e-04 - val_loss: 0.0181\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.1994e-04 - val_loss: 0.0198\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.3773e-04 - val_loss: 0.0216\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 8.0253e-04 - val_loss: 0.0183\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.6028e-04 - val_loss: 0.0200\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.0359e-04 - val_loss: 0.0187\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 7.5244e-04 - val_loss: 0.0195\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 7.3665e-04 - val_loss: 0.0168\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.9222e-04 - val_loss: 0.0184\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.5173e-04 - val_loss: 0.0170\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.9058e-04 - val_loss: 0.0200\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.0340e-04 - val_loss: 0.0199\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 6.9369e-04 - val_loss: 0.0158\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.6165e-04 - val_loss: 0.0156\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 6.8619e-04 - val_loss: 0.0161\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.9138e-04 - val_loss: 0.0147\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.7764e-04 - val_loss: 0.0126\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.5705e-04 - val_loss: 0.0152\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.0621e-04 - val_loss: 0.0153\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.7045e-04 - val_loss: 0.0123\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.9694e-04 - val_loss: 0.0123\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.2741e-04 - val_loss: 0.0152\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.1937e-04 - val_loss: 0.0148\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.7313e-04 - val_loss: 0.0138\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 7.7430e-04 - val_loss: 0.0135\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.7633e-04 - val_loss: 0.0151\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.0090e-04 - val_loss: 0.0143\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.5344e-04 - val_loss: 0.0116\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.4525e-04 - val_loss: 0.0114\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.4752e-04 - val_loss: 0.0131\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.9534e-04 - val_loss: 0.0120\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 7.0539e-04 - val_loss: 0.0098\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (367, 1)\n",
            "Test predicted data:  (367, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t Selling 0.0222288091816937 units of gold on 2020-08-01 00:00:00\n",
            "\t\t Price: 1964.9, unit: 43.677387161109955\n",
            "2019-09-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 0.0290 - val_loss: 0.3738\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0228 - val_loss: 0.3523\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0228 - val_loss: 0.3332\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0188 - val_loss: 0.3164\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0161 - val_loss: 0.3008\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0143 - val_loss: 0.2862\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0140 - val_loss: 0.2720\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0150 - val_loss: 0.2582\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0101 - val_loss: 0.2452\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0109 - val_loss: 0.2318\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0095 - val_loss: 0.2191\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0082 - val_loss: 0.2059\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0085 - val_loss: 0.1925\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0084 - val_loss: 0.1802\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0079 - val_loss: 0.1694\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0073 - val_loss: 0.1575\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0075 - val_loss: 0.1445\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0068 - val_loss: 0.1329\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0068 - val_loss: 0.1213\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0063 - val_loss: 0.1083\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0054 - val_loss: 0.0951\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0051 - val_loss: 0.0798\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0042 - val_loss: 0.0640\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - val_loss: 0.0442\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0028 - val_loss: 0.0232\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0023 - val_loss: 0.0123\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0016 - val_loss: 0.0109\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0018 - val_loss: 0.0117\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0018 - val_loss: 0.0113\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0018 - val_loss: 0.0102\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0017 - val_loss: 0.0100\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0021 - val_loss: 0.0094\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0021 - val_loss: 0.0093\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0090\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0091\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0089\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.0089\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0020 - val_loss: 0.0087\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0086\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0082\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0081\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0018 - val_loss: 0.0081\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0079\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0080\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0081\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0083\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.0078\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.0079\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.0080\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0013 - val_loss: 0.0076\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0014 - val_loss: 0.0076\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.0075\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0014 - val_loss: 0.0072\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0013 - val_loss: 0.0073\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0013 - val_loss: 0.0076\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.0077\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0015 - val_loss: 0.0076\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0082\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0080\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0079\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0077\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0013 - val_loss: 0.0074\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0014 - val_loss: 0.0071\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015 - val_loss: 0.0067\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0014 - val_loss: 0.0065\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0065\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0013 - val_loss: 0.0062\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0061\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.3840e-04 - val_loss: 0.0060\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0015 - val_loss: 0.0058\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0012 - val_loss: 0.0058\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0057\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0056\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.0059\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0053\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0055\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0047\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0012 - val_loss: 0.0043\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.0047\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0010 - val_loss: 0.0039\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0012 - val_loss: 0.0037\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0012 - val_loss: 0.0033\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0011 - val_loss: 0.0031\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0010 - val_loss: 0.0032\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0011 - val_loss: 0.0031\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.0125e-04 - val_loss: 0.0033\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.5822e-04 - val_loss: 0.0035\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.1715e-04 - val_loss: 0.0029\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 9.0622e-04 - val_loss: 0.0031\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 9.7689e-04 - val_loss: 0.0034\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 9.1317e-04 - val_loss: 0.0052\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.0796e-04 - val_loss: 0.0047\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0011 - val_loss: 0.0067\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.0226e-04 - val_loss: 0.0068\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0010 - val_loss: 0.0050\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.9127e-04 - val_loss: 0.0086\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.0418e-04 - val_loss: 0.0063\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.5138e-04 - val_loss: 0.0058\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0011 - val_loss: 0.0102\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.9055e-04 - val_loss: 0.0085\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.2033e-04 - val_loss: 0.0052\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.4004e-04 - val_loss: 0.0033\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.3854e-04 - val_loss: 0.0040\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.5872e-04 - val_loss: 0.0067\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.0793e-04 - val_loss: 0.0077\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0087\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0132\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.3403e-04 - val_loss: 0.0195\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.7796e-04 - val_loss: 0.0266\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.6384e-04 - val_loss: 0.0431\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.1851e-04 - val_loss: 0.0544\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.5410e-04 - val_loss: 0.0852\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.7362e-04 - val_loss: 0.1313\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.7967e-04 - val_loss: 0.1765\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.5720e-04 - val_loss: 0.2462\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.7203e-04 - val_loss: 0.3124\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.7700e-04 - val_loss: 0.4001\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.5979e-04 - val_loss: 0.4272\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.4475e-04 - val_loss: 0.4948\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.4238\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.5169e-04 - val_loss: 0.4202\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.2966e-04 - val_loss: 0.3854\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.8035e-04 - val_loss: 0.4123\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.8345e-04 - val_loss: 0.5772\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.0634e-04 - val_loss: 0.6258\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.3360e-04 - val_loss: 0.8255\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.8833e-04 - val_loss: 0.9079\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.5094e-04 - val_loss: 1.0186\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.7465e-04 - val_loss: 1.1701\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.6786e-04 - val_loss: 1.3277\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.6405e-04 - val_loss: 1.5168\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.5174e-04 - val_loss: 1.7508\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.1460e-04 - val_loss: 1.9465\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.1164e-04 - val_loss: 2.0971\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.2464e-04 - val_loss: 2.1002\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.3885e-04 - val_loss: 2.4083\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.1054e-04 - val_loss: 2.7246\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.8618e-04 - val_loss: 3.0102\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.4096e-04 - val_loss: 3.3898\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.4105e-04 - val_loss: 3.9153\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.6133e-04 - val_loss: 3.5193\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.3848e-04 - val_loss: 4.1374\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.2833e-04 - val_loss: 3.7231\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.7732e-04 - val_loss: 4.2391\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 5.9389e-04 - val_loss: 4.0662\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.1750e-04 - val_loss: 4.2068\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.0628e-04 - val_loss: 4.2961\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.8169e-04 - val_loss: 4.5666\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9349e-04 - val_loss: 4.5787\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.8957e-04 - val_loss: 4.0932\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.9906e-04 - val_loss: 4.3454\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.6019e-04 - val_loss: 3.8895\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.7039e-04 - val_loss: 3.8503\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 5.6510e-04 - val_loss: 3.9569\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.3968e-04 - val_loss: 3.6519\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 6.6524e-04 - val_loss: 4.3865\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 7.2405e-04 - val_loss: 3.4662\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.8808e-04 - val_loss: 3.6128\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.3021e-04 - val_loss: 3.5706\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.2191e-04 - val_loss: 2.7889\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 7.1835e-04 - val_loss: 3.2197\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 6.6355e-04 - val_loss: 2.7617\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.9774e-04 - val_loss: 3.3069\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 6.9833e-04 - val_loss: 2.7554\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.2452e-04 - val_loss: 3.1167\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 6.8689e-04 - val_loss: 2.9666\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.4403e-04 - val_loss: 3.2742\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.9733e-04 - val_loss: 3.2393\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.6115e-04 - val_loss: 3.3990\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.2747e-04 - val_loss: 3.5767\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.9507e-04 - val_loss: 3.4234\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.3743e-04 - val_loss: 3.6715\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.1158e-04 - val_loss: 3.5156\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.0935e-04 - val_loss: 3.4043\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.8886e-04 - val_loss: 3.4349\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.9747e-04 - val_loss: 2.8178\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.4254e-04 - val_loss: 2.9311\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.3455e-04 - val_loss: 2.7501\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.8246e-04 - val_loss: 2.7172\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.8067e-04 - val_loss: 2.9002\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.3265e-04 - val_loss: 2.6490\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.9968e-04 - val_loss: 2.6764\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.5586e-04 - val_loss: 2.7763\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.3616e-04 - val_loss: 2.8276\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7394e-04 - val_loss: 3.1225\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.2484e-04 - val_loss: 2.7334\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.5540e-04 - val_loss: 2.8862\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.9198e-04 - val_loss: 2.4205\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.2249e-04 - val_loss: 2.4208\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.1916e-04 - val_loss: 2.5079\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.3816e-04 - val_loss: 2.5106\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.0223e-04 - val_loss: 2.7445\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.6891e-04 - val_loss: 2.7225\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.6399e-04 - val_loss: 2.8365\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6.2064e-04 - val_loss: 2.7503\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.8089e-04 - val_loss: 2.4880\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.3345e-04 - val_loss: 2.5339\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.0528e-04 - val_loss: 2.5274\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.1313e-04 - val_loss: 2.5003\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (367, 1)\n",
            "Test predicted data:  (367, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1972.35, Last: 11933319.229101567\n",
            "\t\t Buying 0.02324101595841757 units of gold on 2020-09-01 00:00:00\n",
            "\t\t Price: 1972.35, unit: 45.839417825584896\n",
            "2019-10-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.0409 - val_loss: 0.3923\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0382 - val_loss: 0.3534\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0293 - val_loss: 0.3133\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0227 - val_loss: 0.2714\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0192 - val_loss: 0.2279\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0157 - val_loss: 0.1798\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0133 - val_loss: 0.1459\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0135 - val_loss: 0.1313\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0124 - val_loss: 0.1205\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0113 - val_loss: 0.1099\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0107 - val_loss: 0.0963\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0098 - val_loss: 0.0805\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0094 - val_loss: 0.0622\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0085 - val_loss: 0.0432\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0078 - val_loss: 0.0311\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0072 - val_loss: 0.0207\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0064 - val_loss: 0.0123\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0058 - val_loss: 0.0091\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0048 - val_loss: 0.0130\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0266\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0039 - val_loss: 0.0446\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0039 - val_loss: 0.0769\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0042 - val_loss: 0.1159\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0034 - val_loss: 0.1544\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.1543\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - val_loss: 0.1988\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0035 - val_loss: 0.2341\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0032 - val_loss: 0.1823\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.1652\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.1802\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0036 - val_loss: 0.1625\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0033 - val_loss: 0.1644\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0032 - val_loss: 0.1579\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0031 - val_loss: 0.1342\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.1338\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.1213\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0033 - val_loss: 0.1357\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0035 - val_loss: 0.1150\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0027 - val_loss: 0.1305\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.1213\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.1045\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0968\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.1106\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0030 - val_loss: 0.0932\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0977\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0867\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0027 - val_loss: 0.0769\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0029 - val_loss: 0.0734\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0031 - val_loss: 0.0748\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0026 - val_loss: 0.0746\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0027 - val_loss: 0.0607\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - val_loss: 0.0658\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0027 - val_loss: 0.0584\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0591\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0609\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0030 - val_loss: 0.0494\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0434\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0473\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0478\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - val_loss: 0.0414\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0026 - val_loss: 0.0391\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0406\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0376\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025 - val_loss: 0.0323\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0028 - val_loss: 0.0338\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0026 - val_loss: 0.0384\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0029 - val_loss: 0.0308\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0030 - val_loss: 0.0219\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0024 - val_loss: 0.0257\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0023 - val_loss: 0.0239\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0024 - val_loss: 0.0198\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0027 - val_loss: 0.0126\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0026 - val_loss: 0.0163\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0025 - val_loss: 0.0123\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0029 - val_loss: 0.0148\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0024 - val_loss: 0.0124\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0026 - val_loss: 0.0107\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0020 - val_loss: 0.0073\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0023 - val_loss: 0.0071\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0020 - val_loss: 0.0054\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0024 - val_loss: 0.0040\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0020 - val_loss: 0.0041\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0021 - val_loss: 0.0027\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0019 - val_loss: 0.0017\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0029\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0023 - val_loss: 0.0051\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0020 - val_loss: 0.0072\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.0125\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0018 - val_loss: 0.0178\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0307\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0350\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0016 - val_loss: 0.0324\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.0567\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0361\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0014 - val_loss: 0.0549\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0509\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.0616\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0655\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.0678\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0013 - val_loss: 0.0669\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0014 - val_loss: 0.0820\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0672\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0017 - val_loss: 0.0790\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0015 - val_loss: 0.0840\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0820\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.0889\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0014 - val_loss: 0.0769\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0844\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0910\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0013 - val_loss: 0.0778\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0995\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0716\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0819\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0012 - val_loss: 0.0775\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0013 - val_loss: 0.0764\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0012 - val_loss: 0.0899\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.0823\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 0.0854\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0921\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0014 - val_loss: 0.0851\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0011 - val_loss: 0.0773\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0809\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0012 - val_loss: 0.0809\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.0715\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0871\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.8041e-04 - val_loss: 0.0749\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0817\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0756\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0012 - val_loss: 0.0833\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.0760\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.6174e-04 - val_loss: 0.0787\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0743\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.6862e-04 - val_loss: 0.0794\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0010 - val_loss: 0.0798\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0011 - val_loss: 0.0714\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0012 - val_loss: 0.0771\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0011 - val_loss: 0.0617\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0011 - val_loss: 0.0767\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0011 - val_loss: 0.0667\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0010 - val_loss: 0.0660\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0011 - val_loss: 0.0691\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0011 - val_loss: 0.0587\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 9.8128e-04 - val_loss: 0.0628\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.6315e-04 - val_loss: 0.0582\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 7.8799e-04 - val_loss: 0.0577\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.2280e-04 - val_loss: 0.0571\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 9.3509e-04 - val_loss: 0.0525\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.7710e-04 - val_loss: 0.0593\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 8.6654e-04 - val_loss: 0.0542\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 9.2191e-04 - val_loss: 0.0544\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 8.6838e-04 - val_loss: 0.0507\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 7.9801e-04 - val_loss: 0.0529\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.6174e-04 - val_loss: 0.0490\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 9.1307e-04 - val_loss: 0.0508\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 9.0659e-04 - val_loss: 0.0422\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.1245e-04 - val_loss: 0.0521\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.6295e-04 - val_loss: 0.0419\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.6173e-04 - val_loss: 0.0409\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0445\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0011 - val_loss: 0.0335\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.9363e-04 - val_loss: 0.0370\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.9065e-04 - val_loss: 0.0319\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.2096e-04 - val_loss: 0.0309\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0010 - val_loss: 0.0330\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.1939e-04 - val_loss: 0.0283\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0010 - val_loss: 0.0294\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.2277e-04 - val_loss: 0.0290\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.2243e-04 - val_loss: 0.0295\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.6505e-04 - val_loss: 0.0246\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.7224e-04 - val_loss: 0.0240\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.8027e-04 - val_loss: 0.0253\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.8171e-04 - val_loss: 0.0231\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.1676e-04 - val_loss: 0.0256\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.0721e-04 - val_loss: 0.0196\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.1230e-04 - val_loss: 0.0221\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.3466e-04 - val_loss: 0.0199\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.5924e-04 - val_loss: 0.0186\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.8316e-04 - val_loss: 0.0194\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.7087e-04 - val_loss: 0.0181\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.3298e-04 - val_loss: 0.0139\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.9454e-04 - val_loss: 0.0147\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.3568e-04 - val_loss: 0.0132\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.5472e-04 - val_loss: 0.0128\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.1837e-04 - val_loss: 0.0118\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.3435e-04 - val_loss: 0.0122\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.2085e-04 - val_loss: 0.0117\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.8870e-04 - val_loss: 0.0105\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.7417e-04 - val_loss: 0.0115\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.0646e-04 - val_loss: 0.0113\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.1488e-04 - val_loss: 0.0079\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.5851e-04 - val_loss: 0.0088\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.4000e-04 - val_loss: 0.0070\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.0078e-04 - val_loss: 0.0053\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.1797e-04 - val_loss: 0.0053\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.2812e-04 - val_loss: 0.0041\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.4485e-04 - val_loss: 0.0045\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.6860e-04 - val_loss: 0.0050\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.9833e-04 - val_loss: 0.0048\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.5359e-04 - val_loss: 0.0051\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.2822e-04 - val_loss: 0.0047\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "Train predicted data:  (367, 1)\n",
            "Test predicted data:  (367, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t Selling 0.022883557952362178 units of gold on 2020-10-01 00:00:00\n",
            "\t\t Price: 1902.0, unit: 43.52452722539286\n",
            "2019-11-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0467 - val_loss: 0.2798\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0386 - val_loss: 0.2409\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0313 - val_loss: 0.2020\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0216 - val_loss: 0.1562\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0156 - val_loss: 0.1132\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0114 - val_loss: 0.0757\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0076 - val_loss: 0.0469\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0065 - val_loss: 0.0276\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0062 - val_loss: 0.0182\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0062 - val_loss: 0.0163\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0060 - val_loss: 0.0166\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0053 - val_loss: 0.0150\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0051 - val_loss: 0.0121\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0046 - val_loss: 0.0086\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0043 - val_loss: 0.0057\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0039 - val_loss: 0.0051\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0038 - val_loss: 0.0064\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0098\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0136\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0037 - val_loss: 0.0187\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0033 - val_loss: 0.0240\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0029 - val_loss: 0.0305\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0389\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - val_loss: 0.0365\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0303\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - val_loss: 0.0224\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0024 - val_loss: 0.0176\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0173\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0152\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0124\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0134\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0118\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0109\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.0089\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.0080\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0016 - val_loss: 0.0090\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0021 - val_loss: 0.0082\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0070\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0070\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0069\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.0065\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0015 - val_loss: 0.0054\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0022 - val_loss: 0.0054\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0019 - val_loss: 0.0058\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0019 - val_loss: 0.0048\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0017 - val_loss: 0.0050\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0015 - val_loss: 0.0052\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0015 - val_loss: 0.0049\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0019 - val_loss: 0.0047\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019 - val_loss: 0.0046\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0021 - val_loss: 0.0044\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - val_loss: 0.0043\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0021 - val_loss: 0.0041\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0015 - val_loss: 0.0042\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0016 - val_loss: 0.0042\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0021 - val_loss: 0.0041\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0016 - val_loss: 0.0040\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0018 - val_loss: 0.0037\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0017 - val_loss: 0.0038\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0015 - val_loss: 0.0038\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0017 - val_loss: 0.0037\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0018 - val_loss: 0.0034\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0037\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018 - val_loss: 0.0037\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0034\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.0033\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0033\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0032\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0034\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0034\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0014 - val_loss: 0.0032\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 0.0032\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0032\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0031\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015 - val_loss: 0.0030\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0029\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0029\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0028\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0028\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0028\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0028\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0027\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0027\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0013 - val_loss: 0.0027\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0027\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0026\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.0025\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0026\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0027\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0025\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0013 - val_loss: 0.0024\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0023\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0027\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0012 - val_loss: 0.0025\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0025\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0023\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0023\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0014 - val_loss: 0.0024\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0013 - val_loss: 0.0023\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.0024\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0012 - val_loss: 0.0024\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0023\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0022\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 9.9431e-04 - val_loss: 0.0021\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0012 - val_loss: 0.0023\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0012 - val_loss: 0.0021\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 8.4851e-04 - val_loss: 0.0024\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 9.2488e-04 - val_loss: 0.0023\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.2379e-04 - val_loss: 0.0025\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0010 - val_loss: 0.0021\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.9199e-04 - val_loss: 0.0022\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.9328e-04 - val_loss: 0.0020\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0010 - val_loss: 0.0020\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 9.3028e-04 - val_loss: 0.0021\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 9.8458e-04 - val_loss: 0.0021\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0012 - val_loss: 0.0023\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0011 - val_loss: 0.0020\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0011 - val_loss: 0.0022\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.9249e-04 - val_loss: 0.0019\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0015 - val_loss: 0.0021\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0011 - val_loss: 0.0019\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.3129e-04 - val_loss: 0.0020\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0012 - val_loss: 0.0020\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.8505e-04 - val_loss: 0.0023\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0020\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.6663e-04 - val_loss: 0.0020\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.9844e-04 - val_loss: 0.0017\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.2335e-04 - val_loss: 0.0020\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0011 - val_loss: 0.0021\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.2455e-04 - val_loss: 0.0019\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.0543e-04 - val_loss: 0.0017\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.7532e-04 - val_loss: 0.0016\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.6649e-04 - val_loss: 0.0015\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.2204e-04 - val_loss: 0.0017\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.5037e-04 - val_loss: 0.0015\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.5799e-04 - val_loss: 0.0015\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.8904e-04 - val_loss: 0.0014\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.1949e-04 - val_loss: 0.0016\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010 - val_loss: 0.0016\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.6437e-04 - val_loss: 0.0015\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.4314e-04 - val_loss: 0.0015\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.8334e-04 - val_loss: 0.0013\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 7.8707e-04 - val_loss: 0.0017\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.2222e-04 - val_loss: 0.0017\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.6690e-04 - val_loss: 0.0014\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.4589e-04 - val_loss: 0.0016\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.5795e-04 - val_loss: 0.0015\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.0022e-04 - val_loss: 0.0014\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.1962e-04 - val_loss: 0.0012\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.6774e-04 - val_loss: 0.0019\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.9804e-04 - val_loss: 0.0012\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.9975e-04 - val_loss: 0.0017\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.2376e-04 - val_loss: 0.0013\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.1908e-04 - val_loss: 0.0014\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.3227e-04 - val_loss: 0.0015\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.5179e-04 - val_loss: 0.0012\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.1728e-04 - val_loss: 0.0020\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0024\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.0580e-04 - val_loss: 0.0013\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.9302e-04 - val_loss: 0.0015\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.0845e-04 - val_loss: 0.0012\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.8745e-04 - val_loss: 0.0012\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.6700e-04 - val_loss: 0.0014\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 7.5920e-04 - val_loss: 0.0012\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.8359e-04 - val_loss: 0.0013\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.1914e-04 - val_loss: 0.0015\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.3878e-04 - val_loss: 0.0012\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.8032e-04 - val_loss: 0.0018\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 5.4371e-04 - val_loss: 0.0015\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 9.2839e-04 - val_loss: 0.0014\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 7.2442e-04 - val_loss: 0.0012\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.9488e-04 - val_loss: 0.0016\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 8.8065e-04 - val_loss: 0.0012\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.3761e-04 - val_loss: 0.0014\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 5.8940e-04 - val_loss: 0.0012\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.7170e-04 - val_loss: 0.0015\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 7.6878e-04 - val_loss: 0.0012\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 8.0223e-04 - val_loss: 0.0012\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9.3691e-04 - val_loss: 0.0013\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.1423e-04 - val_loss: 0.0014\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.2419e-04 - val_loss: 0.0012\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 5.9086e-04 - val_loss: 0.0012\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.3699e-04 - val_loss: 0.0012\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 7.2605e-04 - val_loss: 0.0016\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.6021e-04 - val_loss: 0.0012\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.8196e-04 - val_loss: 0.0013\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.6800e-04 - val_loss: 0.0016\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.4512e-04 - val_loss: 0.0012\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Train predicted data:  (367, 1)\n",
            "Test predicted data:  (367, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1881.85, Last: 1884.4466808617117\n",
            "\t\t Buying 0.024273449702712654 units of gold on 2020-11-01 00:00:00\n",
            "\t\t Price: 1881.85, unit: 45.67899132304981\n",
            "2019-12-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.0653 - val_loss: 0.3278\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0578 - val_loss: 0.2899\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0465 - val_loss: 0.2502\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0352 - val_loss: 0.2084\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0285 - val_loss: 0.1625\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0170 - val_loss: 0.1112\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0102 - val_loss: 0.0645\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0062 - val_loss: 0.0299\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0037 - val_loss: 0.0151\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0099\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - val_loss: 0.0068\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025 - val_loss: 0.0044\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0039\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0037\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0018 - val_loss: 0.0037\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0037\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0036\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0037\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0035\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0023 - val_loss: 0.0035\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.0034\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0036\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - val_loss: 0.0034\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0033\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0035\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0035\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0033\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0033\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0020 - val_loss: 0.0033\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0021 - val_loss: 0.0032\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0021 - val_loss: 0.0033\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0021 - val_loss: 0.0033\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0020 - val_loss: 0.0031\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0019 - val_loss: 0.0033\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0017 - val_loss: 0.0032\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0020 - val_loss: 0.0031\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0022 - val_loss: 0.0032\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0020 - val_loss: 0.0031\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0019 - val_loss: 0.0031\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0018 - val_loss: 0.0031\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0019 - val_loss: 0.0030\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0016 - val_loss: 0.0030\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0016 - val_loss: 0.0029\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0016 - val_loss: 0.0030\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0019 - val_loss: 0.0028\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0020 - val_loss: 0.0031\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0017 - val_loss: 0.0029\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0017 - val_loss: 0.0028\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0018 - val_loss: 0.0031\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0014 - val_loss: 0.0029\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.0027\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.0030\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0028\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.0027\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0029\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0027\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - val_loss: 0.0028\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0026\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.0026\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0014 - val_loss: 0.0026\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0015 - val_loss: 0.0027\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0025\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0024\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0026\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - val_loss: 0.0027\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0025\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0026\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.0024\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.0024\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0026\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0023\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0014 - val_loss: 0.0025\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0027\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0014 - val_loss: 0.0024\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.0023\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0023\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0023\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - val_loss: 0.0022\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.0023\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0021\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - val_loss: 0.0021\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0025\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0014 - val_loss: 0.0020\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0014 - val_loss: 0.0021\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0012 - val_loss: 0.0021\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0014 - val_loss: 0.0021\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0012 - val_loss: 0.0021\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0011 - val_loss: 0.0021\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0012 - val_loss: 0.0022\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 9.9372e-04 - val_loss: 0.0020\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0012 - val_loss: 0.0020\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0014 - val_loss: 0.0020\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0014 - val_loss: 0.0020\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0012 - val_loss: 0.0021\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0013 - val_loss: 0.0019\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0010 - val_loss: 0.0020\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0011 - val_loss: 0.0021\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0013 - val_loss: 0.0019\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 9.8327e-04 - val_loss: 0.0023\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0012 - val_loss: 0.0019\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.6073e-04 - val_loss: 0.0018\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0013 - val_loss: 0.0019\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0018\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0010 - val_loss: 0.0019\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0014 - val_loss: 0.0020\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 0.0021\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0016 - val_loss: 0.0018\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0012 - val_loss: 0.0018\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.5413e-04 - val_loss: 0.0018\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0019\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0012 - val_loss: 0.0018\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0010 - val_loss: 0.0019\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.5672e-04 - val_loss: 0.0017\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.9379e-04 - val_loss: 0.0019\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 0.0017\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.0179e-04 - val_loss: 0.0017\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.0018\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 9.9917e-04 - val_loss: 0.0017\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0012 - val_loss: 0.0018\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 0.0017\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - val_loss: 0.0017\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.7592e-04 - val_loss: 0.0017\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.8510e-04 - val_loss: 0.0018\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0012 - val_loss: 0.0019\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.1282e-04 - val_loss: 0.0016\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.5778e-04 - val_loss: 0.0020\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.6574e-04 - val_loss: 0.0017\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.4258e-04 - val_loss: 0.0019\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.7782e-04 - val_loss: 0.0018\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.9456e-04 - val_loss: 0.0016\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 0.0018\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0011 - val_loss: 0.0017\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.3854e-04 - val_loss: 0.0018\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0021\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0013 - val_loss: 0.0019\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010 - val_loss: 0.0017\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0010 - val_loss: 0.0019\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.1830e-04 - val_loss: 0.0021\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.4163e-04 - val_loss: 0.0018\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.7917e-04 - val_loss: 0.0022\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.6749e-04 - val_loss: 0.0018\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.8132e-04 - val_loss: 0.0019\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0011 - val_loss: 0.0023\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.1633e-04 - val_loss: 0.0023\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 8.3569e-04 - val_loss: 0.0022\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 9.8153e-04 - val_loss: 0.0026\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.3471e-04 - val_loss: 0.0028\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 9.4817e-04 - val_loss: 0.0027\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 8.6437e-04 - val_loss: 0.0034\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0023\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.7357e-04 - val_loss: 0.0034\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.8810e-04 - val_loss: 0.0037\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.4350e-04 - val_loss: 0.0035\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.2017e-04 - val_loss: 0.0033\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.5963e-04 - val_loss: 0.0068\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0010 - val_loss: 0.0031\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 8.6320e-04 - val_loss: 0.0055\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 9.7899e-04 - val_loss: 0.0045\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0010 - val_loss: 0.0048\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 7.0902e-04 - val_loss: 0.0041\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.0314e-04 - val_loss: 0.0055\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.5738e-04 - val_loss: 0.0049\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.1569e-04 - val_loss: 0.0048\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.4656e-04 - val_loss: 0.0083\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - val_loss: 0.0027\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.0088\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0012 - val_loss: 0.0024\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0010 - val_loss: 0.0059\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (367, 1)\n",
            "Test predicted data:  (367, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1810.75, Last: 2577.2020554184915\n",
            "\t\t Buying 0.02395261756798884 units of gold on 2020-12-01 00:00:00\n",
            "\t\t Price: 1810.75, unit: 43.37220226123579\n",
            "2020-01-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.2127 - val_loss: 0.5817\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1899 - val_loss: 0.5006\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1425 - val_loss: 0.4314\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1328 - val_loss: 0.3690\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1069 - val_loss: 0.3109\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0829 - val_loss: 0.2543\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0719 - val_loss: 0.2017\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0498 - val_loss: 0.1569\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0431 - val_loss: 0.1164\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0308 - val_loss: 0.0760\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0183 - val_loss: 0.0400\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0127 - val_loss: 0.0141\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0084 - val_loss: 0.0037\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0080 - val_loss: 0.0021\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0058 - val_loss: 0.0023\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0059 - val_loss: 0.0023\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0055 - val_loss: 0.0024\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0044 - val_loss: 0.0061\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0033 - val_loss: 0.0075\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0035 - val_loss: 0.0084\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0029 - val_loss: 0.0170\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0027 - val_loss: 0.0188\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0024 - val_loss: 0.0230\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0024 - val_loss: 0.0212\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0028 - val_loss: 0.0267\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0027 - val_loss: 0.0216\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0025 - val_loss: 0.0222\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0026 - val_loss: 0.0212\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0026 - val_loss: 0.0198\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0025 - val_loss: 0.0216\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0024 - val_loss: 0.0199\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0027 - val_loss: 0.0202\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0025 - val_loss: 0.0203\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0028 - val_loss: 0.0193\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0027 - val_loss: 0.0206\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0024 - val_loss: 0.0209\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0027 - val_loss: 0.0192\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0022 - val_loss: 0.0189\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0025 - val_loss: 0.0170\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0023 - val_loss: 0.0186\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0028 - val_loss: 0.0178\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0025 - val_loss: 0.0179\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0030 - val_loss: 0.0180\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0192\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0140\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0027 - val_loss: 0.0192\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0172\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0029 - val_loss: 0.0178\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0152\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0181\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0155\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0163\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0152\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0152\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0024 - val_loss: 0.0170\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0153\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0027 - val_loss: 0.0142\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0026 - val_loss: 0.0163\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0149\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0025 - val_loss: 0.0186\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0023 - val_loss: 0.0144\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0143\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0025 - val_loss: 0.0189\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.0146\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0136\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025 - val_loss: 0.0174\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0153\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0145\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.0164\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.0133\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0021 - val_loss: 0.0155\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0023 - val_loss: 0.0131\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0023 - val_loss: 0.0141\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0155\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0026 - val_loss: 0.0125\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.0150\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0131\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0021 - val_loss: 0.0135\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025 - val_loss: 0.0124\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.0140\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0021 - val_loss: 0.0127\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.0136\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0022 - val_loss: 0.0128\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0125\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0134\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0118\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0111\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0027 - val_loss: 0.0132\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0023 - val_loss: 0.0105\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0024 - val_loss: 0.0138\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0021 - val_loss: 0.0119\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0022 - val_loss: 0.0108\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0023 - val_loss: 0.0118\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0018 - val_loss: 0.0130\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0026 - val_loss: 0.0104\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0024 - val_loss: 0.0144\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0023 - val_loss: 0.0087\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0022 - val_loss: 0.0125\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0024 - val_loss: 0.0100\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0020 - val_loss: 0.0111\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0024 - val_loss: 0.0116\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0023 - val_loss: 0.0129\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0018 - val_loss: 0.0098\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0020 - val_loss: 0.0126\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0022 - val_loss: 0.0096\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0023 - val_loss: 0.0118\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0022 - val_loss: 0.0104\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0021 - val_loss: 0.0113\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0019 - val_loss: 0.0106\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0021 - val_loss: 0.0108\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.0108\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0088\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0020 - val_loss: 0.0095\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0025 - val_loss: 0.0117\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0022 - val_loss: 0.0107\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.0080\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.0103\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0019 - val_loss: 0.0081\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0021 - val_loss: 0.0104\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0089\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0100\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025 - val_loss: 0.0095\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.0088\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0017 - val_loss: 0.0091\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0018 - val_loss: 0.0078\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0098\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0072\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0022 - val_loss: 0.0086\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0019 - val_loss: 0.0077\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.0068\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0089\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0018 - val_loss: 0.0065\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0023 - val_loss: 0.0094\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0021 - val_loss: 0.0058\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0092\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0057\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.0070\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.0067\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019 - val_loss: 0.0068\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0018 - val_loss: 0.0071\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0020 - val_loss: 0.0073\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0046\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0019 - val_loss: 0.0060\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0046\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0051\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0061\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0018 - val_loss: 0.0036\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.0061\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0018 - val_loss: 0.0033\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0037\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0017 - val_loss: 0.0052\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0018 - val_loss: 0.0036\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0019 - val_loss: 0.0040\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0020 - val_loss: 0.0030\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0035\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 0.0026\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0018 - val_loss: 0.0067\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0022 - val_loss: 0.0016\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0016 - val_loss: 0.0020\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0016 - val_loss: 0.0019\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0017 - val_loss: 0.0019\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0016 - val_loss: 0.0021\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0015 - val_loss: 0.0023\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0014 - val_loss: 0.0027\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0018 - val_loss: 0.0016\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0040\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0016\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.0024\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015 - val_loss: 0.0045\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015 - val_loss: 0.0012\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - val_loss: 0.0073\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - val_loss: 0.0012\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.0045\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012 - val_loss: 0.0025\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.0053\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0012\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0045\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0033\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.0018\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "Train predicted data:  (367, 1)\n",
            "Test predicted data:  (367, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1887.6, Last: 3527.3075235605243\n",
            "\t\t Buying 0.02181707249790389 units of gold on 2021-01-01 00:00:00\n",
            "\t\t Price: 1887.6, unit: 41.181906047043384\n",
            "2020-02-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.3112 - val_loss: 0.4725\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2643 - val_loss: 0.4201\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2449 - val_loss: 0.3706\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1979 - val_loss: 0.3241\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1776 - val_loss: 0.2789\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1464 - val_loss: 0.2344\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1258 - val_loss: 0.1897\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0999 - val_loss: 0.1438\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0742 - val_loss: 0.0988\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0510 - val_loss: 0.0572\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0278 - val_loss: 0.0222\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0126 - val_loss: 0.0037\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0080 - val_loss: 0.0026\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0055 - val_loss: 0.0024\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0055 - val_loss: 0.0033\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0040 - val_loss: 0.0024\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0057 - val_loss: 0.0024\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0047 - val_loss: 0.0025\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0046 - val_loss: 0.0024\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0051 - val_loss: 0.0023\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0047 - val_loss: 0.0023\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0050 - val_loss: 0.0024\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0042 - val_loss: 0.0023\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0048 - val_loss: 0.0022\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0044 - val_loss: 0.0023\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0040 - val_loss: 0.0022\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0047 - val_loss: 0.0021\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0046 - val_loss: 0.0021\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0036 - val_loss: 0.0021\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - val_loss: 0.0021\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0038 - val_loss: 0.0022\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0044 - val_loss: 0.0021\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0044 - val_loss: 0.0022\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0039 - val_loss: 0.0021\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0020\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0040 - val_loss: 0.0020\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0022\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0021\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0037 - val_loss: 0.0021\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0020\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0020\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0038 - val_loss: 0.0019\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0039 - val_loss: 0.0020\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0041 - val_loss: 0.0019\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0035 - val_loss: 0.0021\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0018\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0041 - val_loss: 0.0019\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0020\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0031 - val_loss: 0.0019\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038 - val_loss: 0.0019\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.0019\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.0019\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0020\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0018\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0033 - val_loss: 0.0018\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036 - val_loss: 0.0018\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0034 - val_loss: 0.0021\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0032 - val_loss: 0.0019\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0019\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0018\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0030 - val_loss: 0.0017\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0018\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036 - val_loss: 0.0017\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0028 - val_loss: 0.0020\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0016\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0034 - val_loss: 0.0019\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0019\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0034 - val_loss: 0.0017\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0021\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0028 - val_loss: 0.0017\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0017\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0023\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0016\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0031 - val_loss: 0.0019\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0029 - val_loss: 0.0017\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0029 - val_loss: 0.0018\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0030 - val_loss: 0.0018\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0028 - val_loss: 0.0020\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0029 - val_loss: 0.0015\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0032 - val_loss: 0.0019\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0027 - val_loss: 0.0016\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0029 - val_loss: 0.0022\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0029 - val_loss: 0.0015\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0025 - val_loss: 0.0018\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0032 - val_loss: 0.0018\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0031 - val_loss: 0.0018\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0032 - val_loss: 0.0016\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0027 - val_loss: 0.0020\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0028 - val_loss: 0.0017\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0026 - val_loss: 0.0018\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0029 - val_loss: 0.0016\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0028 - val_loss: 0.0016\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0028 - val_loss: 0.0016\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0025 - val_loss: 0.0019\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0025 - val_loss: 0.0016\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0031 - val_loss: 0.0016\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0026 - val_loss: 0.0015\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0025 - val_loss: 0.0018\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0026 - val_loss: 0.0017\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0027 - val_loss: 0.0017\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0024 - val_loss: 0.0014\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0018\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0015\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.0017\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.0018\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0024 - val_loss: 0.0015\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - val_loss: 0.0019\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0014\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025 - val_loss: 0.0014\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0027 - val_loss: 0.0021\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0021 - val_loss: 0.0014\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0023\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0015\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0025 - val_loss: 0.0016\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.0015\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.0014\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.0017\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0018\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.0013\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0026 - val_loss: 0.0028\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0013\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - val_loss: 0.0025\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0014\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0025 - val_loss: 0.0017\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0023 - val_loss: 0.0018\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0021 - val_loss: 0.0019\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0017\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0019\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0021 - val_loss: 0.0015\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0025 - val_loss: 0.0030\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0025 - val_loss: 0.0014\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0022\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0015\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0024 - val_loss: 0.0021\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0019\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 0.0016\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0017 - val_loss: 0.0019\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0022 - val_loss: 0.0021\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0021 - val_loss: 0.0025\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.0014\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0021 - val_loss: 0.0026\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0020 - val_loss: 0.0012\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0019 - val_loss: 0.0021\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0020 - val_loss: 0.0024\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0022 - val_loss: 0.0012\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0023 - val_loss: 0.0027\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0016 - val_loss: 0.0012\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0020 - val_loss: 0.0025\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0017 - val_loss: 0.0022\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0019 - val_loss: 0.0012\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0018 - val_loss: 0.0026\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0012\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0022\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 0.0019\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0019\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0012\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0017 - val_loss: 0.0026\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - val_loss: 0.0021\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0016 - val_loss: 0.0011\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0041\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0011\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0013 - val_loss: 0.0018\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0013 - val_loss: 0.0026\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - val_loss: 0.0018\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0014 - val_loss: 0.0020\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0016 - val_loss: 0.0028\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0013 - val_loss: 0.0026\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (367, 1)\n",
            "Test predicted data:  (367, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1862.9500000000003, Last: 1938.1572310686113\n",
            "\t\t Buying 0.020989409158414178 units of gold on 2021-02-01 00:00:00\n",
            "\t\t Price: 1862.95, unit: 39.10221979166769\n",
            "2020-03-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.2851 - val_loss: 0.3391\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2872 - val_loss: 0.2836\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2175 - val_loss: 0.2295\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1810 - val_loss: 0.1764\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1309 - val_loss: 0.1260\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0900 - val_loss: 0.0801\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0631 - val_loss: 0.0409\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0320 - val_loss: 0.0125\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0116 - val_loss: 0.0023\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0066 - val_loss: 0.0042\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0080 - val_loss: 0.0028\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0064 - val_loss: 0.0024\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0064 - val_loss: 0.0024\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0058 - val_loss: 0.0021\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0052 - val_loss: 0.0024\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0059 - val_loss: 0.0022\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045 - val_loss: 0.0021\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0043 - val_loss: 0.0023\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0044 - val_loss: 0.0021\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0039 - val_loss: 0.0021\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0043 - val_loss: 0.0021\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036 - val_loss: 0.0022\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0044 - val_loss: 0.0021\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0039 - val_loss: 0.0020\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0048 - val_loss: 0.0021\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0045 - val_loss: 0.0021\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0021\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0042 - val_loss: 0.0020\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0039 - val_loss: 0.0022\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0042 - val_loss: 0.0021\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0036 - val_loss: 0.0021\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0035 - val_loss: 0.0020\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0045 - val_loss: 0.0020\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0036 - val_loss: 0.0021\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0035 - val_loss: 0.0021\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0039 - val_loss: 0.0019\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0020\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - val_loss: 0.0023\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0033 - val_loss: 0.0020\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038 - val_loss: 0.0019\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0020\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0021\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0037 - val_loss: 0.0020\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - val_loss: 0.0019\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0019\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0035 - val_loss: 0.0020\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0033 - val_loss: 0.0020\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0020\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0030 - val_loss: 0.0020\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0033 - val_loss: 0.0020\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0030 - val_loss: 0.0019\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0020\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0020\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0031 - val_loss: 0.0019\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0034 - val_loss: 0.0018\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0032 - val_loss: 0.0022\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0036 - val_loss: 0.0019\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0031 - val_loss: 0.0018\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0036 - val_loss: 0.0021\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0032 - val_loss: 0.0020\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0030 - val_loss: 0.0018\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0031 - val_loss: 0.0019\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0035 - val_loss: 0.0020\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0034 - val_loss: 0.0019\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.0018\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0032 - val_loss: 0.0019\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0032 - val_loss: 0.0019\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0031 - val_loss: 0.0018\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0027 - val_loss: 0.0019\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0028 - val_loss: 0.0018\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0028 - val_loss: 0.0019\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0030 - val_loss: 0.0019\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0027 - val_loss: 0.0019\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0034 - val_loss: 0.0018\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0029 - val_loss: 0.0019\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0018\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - val_loss: 0.0018\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0032 - val_loss: 0.0019\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0031 - val_loss: 0.0019\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0018\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0030 - val_loss: 0.0021\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0018\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0026 - val_loss: 0.0018\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0024 - val_loss: 0.0019\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0030 - val_loss: 0.0019\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0019\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0028 - val_loss: 0.0018\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0019\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0026 - val_loss: 0.0020\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0026 - val_loss: 0.0018\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0018\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0030 - val_loss: 0.0019\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0029 - val_loss: 0.0018\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0028 - val_loss: 0.0019\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0018\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0030 - val_loss: 0.0019\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.0017\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0027 - val_loss: 0.0018\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0017\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0029 - val_loss: 0.0018\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0018\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0027 - val_loss: 0.0018\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0017\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0019\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0017\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0027 - val_loss: 0.0018\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0018\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0025 - val_loss: 0.0017\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0032 - val_loss: 0.0018\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0018\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0018\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.0017\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0027 - val_loss: 0.0017\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0018\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0029 - val_loss: 0.0017\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0018\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0027 - val_loss: 0.0018\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - val_loss: 0.0017\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.0018\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0023 - val_loss: 0.0017\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0017\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.0019\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0023 - val_loss: 0.0017\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0024 - val_loss: 0.0018\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0026 - val_loss: 0.0017\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0020 - val_loss: 0.0017\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0027 - val_loss: 0.0019\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0027 - val_loss: 0.0018\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0026 - val_loss: 0.0018\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0022 - val_loss: 0.0018\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0023 - val_loss: 0.0018\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0030 - val_loss: 0.0017\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0025 - val_loss: 0.0019\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0029 - val_loss: 0.0016\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0024 - val_loss: 0.0018\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0024 - val_loss: 0.0017\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0026 - val_loss: 0.0017\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0024 - val_loss: 0.0017\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0028 - val_loss: 0.0018\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0025 - val_loss: 0.0016\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0024 - val_loss: 0.0016\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0028 - val_loss: 0.0018\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0016\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025 - val_loss: 0.0019\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0017\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0024 - val_loss: 0.0018\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.0017\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0024 - val_loss: 0.0016\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0017\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0016\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0017\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0023 - val_loss: 0.0016\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - val_loss: 0.0016\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0018\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0017\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0024 - val_loss: 0.0016\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0017\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0024 - val_loss: 0.0017\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0026 - val_loss: 0.0016\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.0017\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025 - val_loss: 0.0017\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0022 - val_loss: 0.0016\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0020 - val_loss: 0.0017\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.0016\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025 - val_loss: 0.0016\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0017\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0016\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0016\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.0016\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025 - val_loss: 0.0016\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0021 - val_loss: 0.0017\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0016\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0022 - val_loss: 0.0016\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0016\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0021 - val_loss: 0.0017\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0023 - val_loss: 0.0016\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0021 - val_loss: 0.0015\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.0015\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0018 - val_loss: 0.0016\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0024 - val_loss: 0.0015\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0019 - val_loss: 0.0016\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0018 - val_loss: 0.0016\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0022 - val_loss: 0.0015\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1734.15, Last: 1773.333223819733\n",
            "\t\t Buying 0.021409657579902815 units of gold on 2021-03-01 00:00:00\n",
            "\t\t Price: 1734.15, unit: 37.12755769218847\n",
            "2020-04-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 0.1159 - val_loss: 0.0725\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0903 - val_loss: 0.0476\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0585 - val_loss: 0.0256\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0309 - val_loss: 0.0098\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0117 - val_loss: 0.0029\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0039\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0044 - val_loss: 0.0048\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045 - val_loss: 0.0036\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.0028\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0033 - val_loss: 0.0027\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0027\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0027\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0029\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0029 - val_loss: 0.0028\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0032 - val_loss: 0.0027\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0030 - val_loss: 0.0027\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0027\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0029 - val_loss: 0.0027\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0028 - val_loss: 0.0027\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0031 - val_loss: 0.0028\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036 - val_loss: 0.0027\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0026\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0026\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0027\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0027\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0034 - val_loss: 0.0027\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.0026\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0026\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - val_loss: 0.0026\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0028 - val_loss: 0.0026\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0028 - val_loss: 0.0027\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.0026\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0026\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0033 - val_loss: 0.0026\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0034 - val_loss: 0.0026\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0030 - val_loss: 0.0026\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0030 - val_loss: 0.0026\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0029 - val_loss: 0.0026\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0027 - val_loss: 0.0025\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0030 - val_loss: 0.0025\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0031 - val_loss: 0.0025\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0030 - val_loss: 0.0026\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0031 - val_loss: 0.0025\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0032 - val_loss: 0.0025\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0029 - val_loss: 0.0025\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0030 - val_loss: 0.0026\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0025\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0027 - val_loss: 0.0025\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0033 - val_loss: 0.0025\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0027 - val_loss: 0.0025\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0030 - val_loss: 0.0025\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0030 - val_loss: 0.0026\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0028 - val_loss: 0.0025\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025 - val_loss: 0.0025\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0024\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0029 - val_loss: 0.0025\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0029 - val_loss: 0.0026\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0026 - val_loss: 0.0025\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0025\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0027 - val_loss: 0.0025\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0033 - val_loss: 0.0025\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0025\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0024\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0024 - val_loss: 0.0025\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0025\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0024\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0029 - val_loss: 0.0025\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0026 - val_loss: 0.0024\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0024\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0024\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0025\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - val_loss: 0.0025\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0031 - val_loss: 0.0025\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0025 - val_loss: 0.0024\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0027 - val_loss: 0.0024\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - val_loss: 0.0024\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0023 - val_loss: 0.0024\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0032 - val_loss: 0.0023\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.0024\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0023\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.0024\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0027 - val_loss: 0.0024\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025 - val_loss: 0.0024\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - val_loss: 0.0023\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0027 - val_loss: 0.0024\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0025 - val_loss: 0.0022\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0028 - val_loss: 0.0022\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0024 - val_loss: 0.0022\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0027 - val_loss: 0.0022\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0031 - val_loss: 0.0023\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0023 - val_loss: 0.0022\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0025 - val_loss: 0.0022\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0024 - val_loss: 0.0023\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0023 - val_loss: 0.0022\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0028 - val_loss: 0.0022\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0024 - val_loss: 0.0023\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0025 - val_loss: 0.0022\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0029 - val_loss: 0.0022\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0024 - val_loss: 0.0022\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0023 - val_loss: 0.0022\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0024 - val_loss: 0.0021\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0030 - val_loss: 0.0022\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0024 - val_loss: 0.0022\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0026 - val_loss: 0.0021\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0022\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0021\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0021\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0022 - val_loss: 0.0021\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0021\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - val_loss: 0.0021\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025 - val_loss: 0.0021\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0023 - val_loss: 0.0022\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0024 - val_loss: 0.0021\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0023 - val_loss: 0.0021\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0021\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.0021\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0024 - val_loss: 0.0021\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0021\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0020\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019 - val_loss: 0.0021\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0021\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0024 - val_loss: 0.0020\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0023 - val_loss: 0.0020\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.0021\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0021\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0023 - val_loss: 0.0020\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - val_loss: 0.0020\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.0020\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.0020\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0020 - val_loss: 0.0021\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - val_loss: 0.0019\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.0020\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0023 - val_loss: 0.0019\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.0020\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0024 - val_loss: 0.0022\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0019\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.0020\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0020\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - val_loss: 0.0019\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0020\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021 - val_loss: 0.0019\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0020 - val_loss: 0.0019\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0018 - val_loss: 0.0020\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0020\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0021 - val_loss: 0.0020\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0022 - val_loss: 0.0020\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0021 - val_loss: 0.0019\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0019 - val_loss: 0.0019\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0020 - val_loss: 0.0019\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0016 - val_loss: 0.0020\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0022 - val_loss: 0.0019\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025 - val_loss: 0.0019\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0022 - val_loss: 0.0019\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0018 - val_loss: 0.0020\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0023 - val_loss: 0.0018\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1726.05, Last: 1751.6234602332115\n",
            "\t\t Buying 0.02042386722790936 units of gold on 2021-04-01 00:00:00\n",
            "\t\t Price: 1726.05, unit: 35.25261602873295\n",
            "2020-05-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.1463 - val_loss: 0.0427\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1034 - val_loss: 0.0269\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0689 - val_loss: 0.0146\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0392 - val_loss: 0.0072\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0165 - val_loss: 0.0073\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0088 - val_loss: 0.0120\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0098 - val_loss: 0.0115\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0100 - val_loss: 0.0087\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0083 - val_loss: 0.0073\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0081 - val_loss: 0.0071\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0083 - val_loss: 0.0073\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0080 - val_loss: 0.0075\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0078 - val_loss: 0.0074\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0072 - val_loss: 0.0070\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0072 - val_loss: 0.0069\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0082 - val_loss: 0.0067\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0076 - val_loss: 0.0068\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0071 - val_loss: 0.0064\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0068 - val_loss: 0.0059\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0076 - val_loss: 0.0060\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0078 - val_loss: 0.0060\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0070 - val_loss: 0.0056\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0074 - val_loss: 0.0059\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0080 - val_loss: 0.0056\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0069 - val_loss: 0.0054\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0074 - val_loss: 0.0052\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0061 - val_loss: 0.0051\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0072 - val_loss: 0.0050\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0077 - val_loss: 0.0051\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0075 - val_loss: 0.0052\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0064 - val_loss: 0.0048\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0075 - val_loss: 0.0049\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0072 - val_loss: 0.0047\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0070 - val_loss: 0.0047\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0060 - val_loss: 0.0047\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0065 - val_loss: 0.0049\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0060 - val_loss: 0.0046\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0063 - val_loss: 0.0045\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0076 - val_loss: 0.0045\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0074 - val_loss: 0.0046\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0068 - val_loss: 0.0046\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0064 - val_loss: 0.0043\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0067 - val_loss: 0.0045\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0058 - val_loss: 0.0043\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0063 - val_loss: 0.0044\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0076 - val_loss: 0.0043\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0060 - val_loss: 0.0043\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0065 - val_loss: 0.0045\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0053 - val_loss: 0.0042\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0064 - val_loss: 0.0042\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0070 - val_loss: 0.0041\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0055 - val_loss: 0.0041\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0059 - val_loss: 0.0042\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0060 - val_loss: 0.0041\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0062 - val_loss: 0.0041\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0067 - val_loss: 0.0044\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0054 - val_loss: 0.0043\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0062 - val_loss: 0.0041\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0058 - val_loss: 0.0039\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0060 - val_loss: 0.0040\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0059 - val_loss: 0.0043\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0063 - val_loss: 0.0041\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0064 - val_loss: 0.0039\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0062 - val_loss: 0.0039\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0057 - val_loss: 0.0041\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0055 - val_loss: 0.0040\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0056 - val_loss: 0.0039\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0064 - val_loss: 0.0039\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0051 - val_loss: 0.0039\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0053 - val_loss: 0.0038\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0064 - val_loss: 0.0039\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0052 - val_loss: 0.0037\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0060 - val_loss: 0.0040\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0060 - val_loss: 0.0038\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0065 - val_loss: 0.0037\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0057 - val_loss: 0.0037\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0057 - val_loss: 0.0038\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0055 - val_loss: 0.0039\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0051 - val_loss: 0.0037\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0065 - val_loss: 0.0035\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0059 - val_loss: 0.0036\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0054 - val_loss: 0.0040\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0057 - val_loss: 0.0036\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0049 - val_loss: 0.0037\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0051 - val_loss: 0.0035\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0054 - val_loss: 0.0036\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0055 - val_loss: 0.0037\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0047 - val_loss: 0.0037\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0052 - val_loss: 0.0035\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0053 - val_loss: 0.0034\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0051 - val_loss: 0.0036\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0050 - val_loss: 0.0037\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0050 - val_loss: 0.0035\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0053 - val_loss: 0.0033\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0045 - val_loss: 0.0035\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0051 - val_loss: 0.0036\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0049 - val_loss: 0.0034\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0046 - val_loss: 0.0036\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0043 - val_loss: 0.0033\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0044 - val_loss: 0.0032\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0047 - val_loss: 0.0034\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0054 - val_loss: 0.0036\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0049 - val_loss: 0.0035\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0051 - val_loss: 0.0033\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0051 - val_loss: 0.0033\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0048 - val_loss: 0.0034\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0050 - val_loss: 0.0032\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0043 - val_loss: 0.0034\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0053 - val_loss: 0.0035\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0043 - val_loss: 0.0032\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0050 - val_loss: 0.0030\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0049 - val_loss: 0.0032\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0041 - val_loss: 0.0035\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0044 - val_loss: 0.0035\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0048 - val_loss: 0.0030\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0047 - val_loss: 0.0031\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0046 - val_loss: 0.0033\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0045 - val_loss: 0.0033\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0042 - val_loss: 0.0031\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0042 - val_loss: 0.0030\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038 - val_loss: 0.0031\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0040 - val_loss: 0.0032\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - val_loss: 0.0031\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0040 - val_loss: 0.0031\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0042 - val_loss: 0.0031\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0047 - val_loss: 0.0031\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - val_loss: 0.0031\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0042 - val_loss: 0.0029\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0044 - val_loss: 0.0030\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0038 - val_loss: 0.0032\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0046 - val_loss: 0.0032\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0045 - val_loss: 0.0031\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045 - val_loss: 0.0029\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0041 - val_loss: 0.0030\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0047 - val_loss: 0.0031\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045 - val_loss: 0.0029\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0045 - val_loss: 0.0029\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - val_loss: 0.0030\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0039 - val_loss: 0.0029\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0032\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0040 - val_loss: 0.0031\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0037 - val_loss: 0.0029\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0043 - val_loss: 0.0029\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0036 - val_loss: 0.0029\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0045 - val_loss: 0.0029\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038 - val_loss: 0.0028\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0042 - val_loss: 0.0029\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0043 - val_loss: 0.0029\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0037 - val_loss: 0.0029\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0034 - val_loss: 0.0028\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038 - val_loss: 0.0028\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0040 - val_loss: 0.0029\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0037 - val_loss: 0.0028\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0040 - val_loss: 0.0031\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0043 - val_loss: 0.0029\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038 - val_loss: 0.0028\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0036 - val_loss: 0.0028\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0036 - val_loss: 0.0029\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0038 - val_loss: 0.0028\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0045 - val_loss: 0.0029\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0037 - val_loss: 0.0027\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0038 - val_loss: 0.0026\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0040 - val_loss: 0.0027\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0041 - val_loss: 0.0028\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0042 - val_loss: 0.0028\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0038 - val_loss: 0.0027\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0035 - val_loss: 0.0027\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0030 - val_loss: 0.0028\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0038 - val_loss: 0.0027\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0038 - val_loss: 0.0027\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0040 - val_loss: 0.0029\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0040 - val_loss: 0.0027\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0038 - val_loss: 0.0026\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0033 - val_loss: 0.0028\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0035 - val_loss: 0.0028\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.0029\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0036 - val_loss: 0.0026\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0038 - val_loss: 0.0025\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0040 - val_loss: 0.0028\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0042 - val_loss: 0.0028\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0025\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0036 - val_loss: 0.0025\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - val_loss: 0.0027\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0035 - val_loss: 0.0028\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0039 - val_loss: 0.0028\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0039 - val_loss: 0.0024\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035 - val_loss: 0.0027\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0034 - val_loss: 0.0027\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - val_loss: 0.0025\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0033 - val_loss: 0.0026\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0036 - val_loss: 0.0026\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.0027\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0026\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0038 - val_loss: 0.0024\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0028 - val_loss: 0.0026\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0037 - val_loss: 0.0026\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0029 - val_loss: 0.0026\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0035 - val_loss: 0.0023\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0035 - val_loss: 0.0025\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0037 - val_loss: 0.0027\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1767.6499999999999, Last: 1844.5380141466858\n",
            "\t\t Buying 0.018936078363523286 units of gold on 2021-05-01 00:00:00\n",
            "\t\t Price: 1767.65, unit: 33.47235891928194\n",
            "2020-06-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - loss: 0.0939 - val_loss: 0.0193\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0643 - val_loss: 0.0093\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0306 - val_loss: 0.0049\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0130 - val_loss: 0.0068\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0060 - val_loss: 0.0111\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0066 - val_loss: 0.0110\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0065 - val_loss: 0.0082\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0050 - val_loss: 0.0067\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0048 - val_loss: 0.0064\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0051 - val_loss: 0.0067\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0047 - val_loss: 0.0071\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0046 - val_loss: 0.0072\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0044 - val_loss: 0.0067\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0052 - val_loss: 0.0063\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0047 - val_loss: 0.0064\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0044 - val_loss: 0.0062\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0046 - val_loss: 0.0062\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0046 - val_loss: 0.0059\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0046 - val_loss: 0.0055\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0045 - val_loss: 0.0053\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045 - val_loss: 0.0054\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0042 - val_loss: 0.0052\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0047 - val_loss: 0.0052\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0043 - val_loss: 0.0051\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0053 - val_loss: 0.0049\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0046\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0045 - val_loss: 0.0045\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0042 - val_loss: 0.0043\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0053 - val_loss: 0.0040\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0043 - val_loss: 0.0037\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045 - val_loss: 0.0039\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0044 - val_loss: 0.0040\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0048 - val_loss: 0.0039\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - val_loss: 0.0038\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0045 - val_loss: 0.0038\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0040 - val_loss: 0.0036\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0040 - val_loss: 0.0038\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0046 - val_loss: 0.0040\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0044 - val_loss: 0.0038\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0039 - val_loss: 0.0038\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0040 - val_loss: 0.0038\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0044 - val_loss: 0.0038\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0047 - val_loss: 0.0037\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0044 - val_loss: 0.0038\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0041 - val_loss: 0.0038\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0043 - val_loss: 0.0037\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0037 - val_loss: 0.0036\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0039 - val_loss: 0.0038\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0040 - val_loss: 0.0039\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0047 - val_loss: 0.0037\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0043 - val_loss: 0.0036\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0035 - val_loss: 0.0037\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0040 - val_loss: 0.0036\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0042 - val_loss: 0.0036\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0044 - val_loss: 0.0036\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0037\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - val_loss: 0.0037\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0042 - val_loss: 0.0036\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0039 - val_loss: 0.0035\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0039 - val_loss: 0.0036\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0038\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0048 - val_loss: 0.0035\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0036 - val_loss: 0.0036\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0039 - val_loss: 0.0036\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0039 - val_loss: 0.0035\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0041 - val_loss: 0.0036\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0036 - val_loss: 0.0036\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0037 - val_loss: 0.0034\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0038 - val_loss: 0.0036\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0038 - val_loss: 0.0035\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0045 - val_loss: 0.0034\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0038 - val_loss: 0.0034\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0037 - val_loss: 0.0036\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0036 - val_loss: 0.0034\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0036 - val_loss: 0.0035\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0040 - val_loss: 0.0034\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0040 - val_loss: 0.0034\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0036 - val_loss: 0.0033\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0037 - val_loss: 0.0035\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0041 - val_loss: 0.0036\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0035 - val_loss: 0.0033\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0034 - val_loss: 0.0033\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0041 - val_loss: 0.0033\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0038 - val_loss: 0.0033\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0030 - val_loss: 0.0035\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0036 - val_loss: 0.0036\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0031 - val_loss: 0.0031\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0038 - val_loss: 0.0033\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0031 - val_loss: 0.0034\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0039 - val_loss: 0.0032\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038 - val_loss: 0.0033\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0034 - val_loss: 0.0033\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0037 - val_loss: 0.0035\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0031 - val_loss: 0.0032\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0034 - val_loss: 0.0031\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0040 - val_loss: 0.0031\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0034 - val_loss: 0.0033\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - val_loss: 0.0033\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036 - val_loss: 0.0032\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0037 - val_loss: 0.0032\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0039 - val_loss: 0.0031\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0037 - val_loss: 0.0032\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0029 - val_loss: 0.0032\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0032\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0036 - val_loss: 0.0030\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.0029\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0034\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0028 - val_loss: 0.0029\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0028\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0030\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0035 - val_loss: 0.0032\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0043 - val_loss: 0.0031\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0031 - val_loss: 0.0030\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035 - val_loss: 0.0030\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0028 - val_loss: 0.0031\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0029\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0033 - val_loss: 0.0031\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0029 - val_loss: 0.0029\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0030\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0034 - val_loss: 0.0030\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0035 - val_loss: 0.0031\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0036 - val_loss: 0.0030\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0028\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0035 - val_loss: 0.0032\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0032 - val_loss: 0.0034\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0038 - val_loss: 0.0029\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0030 - val_loss: 0.0030\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0031 - val_loss: 0.0031\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0031 - val_loss: 0.0029\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0027 - val_loss: 0.0028\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0035 - val_loss: 0.0028\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0034 - val_loss: 0.0026\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0037 - val_loss: 0.0030\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0034 - val_loss: 0.0031\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0032 - val_loss: 0.0030\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0036 - val_loss: 0.0029\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0025 - val_loss: 0.0029\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0035 - val_loss: 0.0028\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0031 - val_loss: 0.0028\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0031 - val_loss: 0.0029\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0028 - val_loss: 0.0029\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0032 - val_loss: 0.0028\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0028 - val_loss: 0.0029\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0034 - val_loss: 0.0029\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0031 - val_loss: 0.0028\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - val_loss: 0.0028\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0032 - val_loss: 0.0028\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0037 - val_loss: 0.0028\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0029\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0027\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0027\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0028\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0027 - val_loss: 0.0028\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0026 - val_loss: 0.0028\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.0028\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0031 - val_loss: 0.0026\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0031 - val_loss: 0.0028\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0032 - val_loss: 0.0028\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025 - val_loss: 0.0026\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0034 - val_loss: 0.0024\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0027 - val_loss: 0.0028\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0025 - val_loss: 0.0028\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0029 - val_loss: 0.0026\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0029 - val_loss: 0.0027\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0030 - val_loss: 0.0028\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0026\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0028 - val_loss: 0.0026\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0032 - val_loss: 0.0028\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0025\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0028 - val_loss: 0.0025\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0029 - val_loss: 0.0026\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0038 - val_loss: 0.0026\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0024 - val_loss: 0.0027\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0028 - val_loss: 0.0026\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0025\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0028\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0031 - val_loss: 0.0027\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0027 - val_loss: 0.0024\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0028 - val_loss: 0.0026\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0025 - val_loss: 0.0029\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0032 - val_loss: 0.0025\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0029 - val_loss: 0.0025\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0025 - val_loss: 0.0025\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0026 - val_loss: 0.0025\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0024 - val_loss: 0.0025\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1899.3499999999997, Last: 1908.0479628503324\n",
            "\t\t Buying 0.01673309542414942 units of gold on 2021-06-01 00:00:00\n",
            "\t\t Price: 1899.35, unit: 31.7820047938582\n",
            "2020-07-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.6339 - val_loss: 0.1721\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.5540 - val_loss: 0.1429\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4827 - val_loss: 0.1193\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3929 - val_loss: 0.1000\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3432 - val_loss: 0.0836\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3003 - val_loss: 0.0695\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2427 - val_loss: 0.0572\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2078 - val_loss: 0.0459\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1709 - val_loss: 0.0354\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1307 - val_loss: 0.0264\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0943 - val_loss: 0.0187\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0576 - val_loss: 0.0132\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0319 - val_loss: 0.0109\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0126 - val_loss: 0.0108\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0104 - val_loss: 0.0097\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0116 - val_loss: 0.0081\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0092 - val_loss: 0.0079\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0089 - val_loss: 0.0088\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0091 - val_loss: 0.0089\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0085 - val_loss: 0.0090\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0094 - val_loss: 0.0092\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0085 - val_loss: 0.0097\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0079 - val_loss: 0.0095\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0081 - val_loss: 0.0094\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0072 - val_loss: 0.0100\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0069 - val_loss: 0.0100\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0075 - val_loss: 0.0096\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0069 - val_loss: 0.0101\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0069 - val_loss: 0.0103\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0063 - val_loss: 0.0103\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0058 - val_loss: 0.0106\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0064 - val_loss: 0.0104\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0063 - val_loss: 0.0099\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0072 - val_loss: 0.0106\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0059 - val_loss: 0.0101\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0058 - val_loss: 0.0112\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0062 - val_loss: 0.0105\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0068 - val_loss: 0.0103\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0057 - val_loss: 0.0106\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0056 - val_loss: 0.0109\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0064 - val_loss: 0.0104\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0053 - val_loss: 0.0106\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0049 - val_loss: 0.0102\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0063 - val_loss: 0.0111\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0062 - val_loss: 0.0099\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0061 - val_loss: 0.0104\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0054 - val_loss: 0.0105\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0053 - val_loss: 0.0102\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0061 - val_loss: 0.0104\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0060 - val_loss: 0.0105\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0053 - val_loss: 0.0099\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0051 - val_loss: 0.0102\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0057 - val_loss: 0.0111\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0051 - val_loss: 0.0092\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0065 - val_loss: 0.0100\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0064 - val_loss: 0.0101\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0050 - val_loss: 0.0099\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0064 - val_loss: 0.0097\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0055 - val_loss: 0.0095\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0053 - val_loss: 0.0101\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0051 - val_loss: 0.0098\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0055 - val_loss: 0.0097\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0051 - val_loss: 0.0096\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0050 - val_loss: 0.0092\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0047 - val_loss: 0.0095\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0051 - val_loss: 0.0093\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0056 - val_loss: 0.0093\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0050 - val_loss: 0.0098\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0054 - val_loss: 0.0094\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0046 - val_loss: 0.0095\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0053 - val_loss: 0.0090\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0046 - val_loss: 0.0089\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0052 - val_loss: 0.0094\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0053 - val_loss: 0.0093\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0053 - val_loss: 0.0093\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0054 - val_loss: 0.0088\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0056 - val_loss: 0.0085\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0051 - val_loss: 0.0088\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0049 - val_loss: 0.0093\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0053 - val_loss: 0.0084\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0045 - val_loss: 0.0088\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0051 - val_loss: 0.0088\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0048 - val_loss: 0.0083\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0055 - val_loss: 0.0090\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0043 - val_loss: 0.0087\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0051 - val_loss: 0.0082\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0050 - val_loss: 0.0091\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0049 - val_loss: 0.0085\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0050 - val_loss: 0.0087\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0053 - val_loss: 0.0090\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0050 - val_loss: 0.0088\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0044 - val_loss: 0.0087\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0048 - val_loss: 0.0081\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0042 - val_loss: 0.0093\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0041 - val_loss: 0.0087\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0047 - val_loss: 0.0083\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0045 - val_loss: 0.0090\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0051 - val_loss: 0.0090\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0052 - val_loss: 0.0083\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0050 - val_loss: 0.0090\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0044 - val_loss: 0.0085\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0045 - val_loss: 0.0086\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0047 - val_loss: 0.0083\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0043 - val_loss: 0.0087\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0043 - val_loss: 0.0086\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0044 - val_loss: 0.0085\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0050 - val_loss: 0.0083\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0046 - val_loss: 0.0084\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0038 - val_loss: 0.0086\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0051 - val_loss: 0.0087\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0046 - val_loss: 0.0089\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0041 - val_loss: 0.0083\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0051 - val_loss: 0.0085\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0042 - val_loss: 0.0081\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0042 - val_loss: 0.0087\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0042 - val_loss: 0.0081\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0043 - val_loss: 0.0088\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0045 - val_loss: 0.0078\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0038 - val_loss: 0.0089\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0039 - val_loss: 0.0083\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0046 - val_loss: 0.0082\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0048 - val_loss: 0.0087\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0041 - val_loss: 0.0081\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0041 - val_loss: 0.0084\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0042 - val_loss: 0.0087\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0039 - val_loss: 0.0082\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0041 - val_loss: 0.0083\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0040 - val_loss: 0.0083\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0039 - val_loss: 0.0085\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0044 - val_loss: 0.0083\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0042 - val_loss: 0.0084\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0052 - val_loss: 0.0079\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0041 - val_loss: 0.0088\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - val_loss: 0.0084\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0044 - val_loss: 0.0083\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0038 - val_loss: 0.0086\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0044 - val_loss: 0.0084\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0037 - val_loss: 0.0088\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0043 - val_loss: 0.0084\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0039 - val_loss: 0.0085\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0039 - val_loss: 0.0086\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0036 - val_loss: 0.0087\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0040 - val_loss: 0.0086\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0042 - val_loss: 0.0087\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0039 - val_loss: 0.0083\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0091\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035 - val_loss: 0.0088\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0038 - val_loss: 0.0088\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0043 - val_loss: 0.0090\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0046 - val_loss: 0.0091\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0037 - val_loss: 0.0081\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0089\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0044 - val_loss: 0.0085\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0042 - val_loss: 0.0086\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038 - val_loss: 0.0090\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0085\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.0088\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038 - val_loss: 0.0086\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0036 - val_loss: 0.0091\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0035 - val_loss: 0.0089\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038 - val_loss: 0.0088\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0038 - val_loss: 0.0088\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0037 - val_loss: 0.0083\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0039 - val_loss: 0.0092\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0038 - val_loss: 0.0084\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0037 - val_loss: 0.0083\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0039 - val_loss: 0.0092\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0038 - val_loss: 0.0082\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0090\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0088\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0035 - val_loss: 0.0089\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0039 - val_loss: 0.0091\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0037 - val_loss: 0.0088\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0034 - val_loss: 0.0084\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0035 - val_loss: 0.0089\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0033 - val_loss: 0.0085\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0040 - val_loss: 0.0088\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0042 - val_loss: 0.0090\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0040 - val_loss: 0.0087\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0041 - val_loss: 0.0090\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0046 - val_loss: 0.0093\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0032 - val_loss: 0.0085\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0034 - val_loss: 0.0089\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0037 - val_loss: 0.0089\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0037 - val_loss: 0.0088\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0035 - val_loss: 0.0088\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0038 - val_loss: 0.0087\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0036 - val_loss: 0.0086\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0032 - val_loss: 0.0086\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0044 - val_loss: 0.0089\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0035 - val_loss: 0.0088\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0037 - val_loss: 0.0085\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0034 - val_loss: 0.0096\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0041 - val_loss: 0.0080\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0038 - val_loss: 0.0094\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0036 - val_loss: 0.0091\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0030 - val_loss: 0.0093\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0039 - val_loss: 0.0085\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - val_loss: 0.0093\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0037 - val_loss: 0.0087\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1781.5, Last: 1883.3212288379668\n",
            "\t\t Buying 0.016939103874133235 units of gold on 2021-07-01 00:00:00\n",
            "\t\t Price: 1781.5, unit: 30.177013551768358\n",
            "2020-08-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.3081 - val_loss: 0.0976\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1964 - val_loss: 0.0550\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1156 - val_loss: 0.0247\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0485 - val_loss: 0.0092\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0137 - val_loss: 0.0082\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0078 - val_loss: 0.0117\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0127 - val_loss: 0.0092\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0082 - val_loss: 0.0061\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0068 - val_loss: 0.0056\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0074 - val_loss: 0.0057\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0069 - val_loss: 0.0060\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0072 - val_loss: 0.0060\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0069 - val_loss: 0.0055\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0069 - val_loss: 0.0053\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0061 - val_loss: 0.0052\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0069 - val_loss: 0.0050\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0065 - val_loss: 0.0050\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0061 - val_loss: 0.0048\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0063 - val_loss: 0.0048\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0071 - val_loss: 0.0047\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0068 - val_loss: 0.0047\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0062 - val_loss: 0.0047\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0071 - val_loss: 0.0047\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0060 - val_loss: 0.0048\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0064 - val_loss: 0.0048\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0063 - val_loss: 0.0049\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0060 - val_loss: 0.0048\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0060 - val_loss: 0.0049\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0058 - val_loss: 0.0048\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0064 - val_loss: 0.0049\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0062 - val_loss: 0.0052\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0057 - val_loss: 0.0050\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0053 - val_loss: 0.0050\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0054 - val_loss: 0.0051\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0062 - val_loss: 0.0053\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0052 - val_loss: 0.0051\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0055 - val_loss: 0.0051\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0057 - val_loss: 0.0052\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0055 - val_loss: 0.0049\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0058 - val_loss: 0.0049\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0055 - val_loss: 0.0051\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0047 - val_loss: 0.0054\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0052 - val_loss: 0.0055\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0049 - val_loss: 0.0051\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0051 - val_loss: 0.0053\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0048 - val_loss: 0.0055\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0046 - val_loss: 0.0056\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0049 - val_loss: 0.0057\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0049 - val_loss: 0.0058\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0051 - val_loss: 0.0062\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0049 - val_loss: 0.0063\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0047 - val_loss: 0.0063\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0053 - val_loss: 0.0066\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0050 - val_loss: 0.0067\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0051 - val_loss: 0.0065\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0048 - val_loss: 0.0066\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0050 - val_loss: 0.0073\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0053 - val_loss: 0.0073\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0043 - val_loss: 0.0068\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0047 - val_loss: 0.0069\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0046 - val_loss: 0.0076\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0048 - val_loss: 0.0072\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0043 - val_loss: 0.0067\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0051 - val_loss: 0.0073\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0048 - val_loss: 0.0071\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0044 - val_loss: 0.0073\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0047 - val_loss: 0.0072\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0047 - val_loss: 0.0076\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0044 - val_loss: 0.0077\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0041 - val_loss: 0.0076\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0047 - val_loss: 0.0071\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0043 - val_loss: 0.0073\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0048 - val_loss: 0.0077\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0042 - val_loss: 0.0072\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0044 - val_loss: 0.0069\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0042 - val_loss: 0.0068\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0042 - val_loss: 0.0075\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0046 - val_loss: 0.0078\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0045 - val_loss: 0.0075\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0046 - val_loss: 0.0076\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0047 - val_loss: 0.0076\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0048 - val_loss: 0.0072\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0043 - val_loss: 0.0076\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0041 - val_loss: 0.0069\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0046 - val_loss: 0.0081\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0042 - val_loss: 0.0075\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0045 - val_loss: 0.0069\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0041 - val_loss: 0.0069\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0045 - val_loss: 0.0070\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0041 - val_loss: 0.0070\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0042 - val_loss: 0.0070\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0046 - val_loss: 0.0069\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0044 - val_loss: 0.0064\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0039 - val_loss: 0.0067\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0040 - val_loss: 0.0064\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0044 - val_loss: 0.0062\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0042 - val_loss: 0.0062\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0035 - val_loss: 0.0063\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0040 - val_loss: 0.0061\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0041 - val_loss: 0.0062\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0044 - val_loss: 0.0064\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0039 - val_loss: 0.0059\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0043 - val_loss: 0.0060\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0038 - val_loss: 0.0059\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0041 - val_loss: 0.0063\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0037 - val_loss: 0.0066\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0040 - val_loss: 0.0063\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0039 - val_loss: 0.0062\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038 - val_loss: 0.0061\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0038 - val_loss: 0.0062\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0063\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0040 - val_loss: 0.0059\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - val_loss: 0.0056\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0042 - val_loss: 0.0056\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0038 - val_loss: 0.0056\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0038 - val_loss: 0.0055\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0037 - val_loss: 0.0055\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0041 - val_loss: 0.0055\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0037 - val_loss: 0.0054\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0040 - val_loss: 0.0055\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038 - val_loss: 0.0059\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0035 - val_loss: 0.0052\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038 - val_loss: 0.0053\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0035 - val_loss: 0.0054\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0037 - val_loss: 0.0055\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0036 - val_loss: 0.0053\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - val_loss: 0.0053\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0033 - val_loss: 0.0054\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0032 - val_loss: 0.0057\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0037 - val_loss: 0.0052\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0036 - val_loss: 0.0048\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0037 - val_loss: 0.0052\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - val_loss: 0.0050\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0035 - val_loss: 0.0047\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0043\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0032 - val_loss: 0.0046\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035 - val_loss: 0.0049\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0032 - val_loss: 0.0047\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - val_loss: 0.0041\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0039 - val_loss: 0.0046\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0044\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035 - val_loss: 0.0044\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0043\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0033 - val_loss: 0.0043\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0032 - val_loss: 0.0041\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0044\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0034 - val_loss: 0.0041\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0033 - val_loss: 0.0042\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0030 - val_loss: 0.0045\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0033 - val_loss: 0.0038\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - val_loss: 0.0041\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0033 - val_loss: 0.0038\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0046\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0030 - val_loss: 0.0039\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0033 - val_loss: 0.0041\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0033 - val_loss: 0.0039\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0035 - val_loss: 0.0039\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0030 - val_loss: 0.0040\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0030 - val_loss: 0.0035\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0034 - val_loss: 0.0039\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0030 - val_loss: 0.0038\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0030 - val_loss: 0.0039\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0032 - val_loss: 0.0035\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0029 - val_loss: 0.0037\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0027 - val_loss: 0.0036\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0029 - val_loss: 0.0031\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0032 - val_loss: 0.0033\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0029 - val_loss: 0.0031\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0030 - val_loss: 0.0034\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0027 - val_loss: 0.0034\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0028 - val_loss: 0.0032\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0027 - val_loss: 0.0031\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0028 - val_loss: 0.0031\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0026 - val_loss: 0.0031\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0027 - val_loss: 0.0030\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0028 - val_loss: 0.0029\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0027 - val_loss: 0.0031\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0028 - val_loss: 0.0027\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0026 - val_loss: 0.0032\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0027 - val_loss: 0.0029\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0029\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0029 - val_loss: 0.0027\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028 - val_loss: 0.0026\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0029 - val_loss: 0.0025\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0025 - val_loss: 0.0027\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0022 - val_loss: 0.0024\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0023 - val_loss: 0.0026\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0024 - val_loss: 0.0023\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025 - val_loss: 0.0025\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0028 - val_loss: 0.0023\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0023 - val_loss: 0.0025\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0023\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - val_loss: 0.0024\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0025 - val_loss: 0.0022\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0024 - val_loss: 0.0023\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1825.7500000000002, Last: 1847.3502280712128\n",
            "\t\t Buying 0.015693865188226238 units of gold on 2021-08-01 00:00:00\n",
            "\t\t Price: 1825.75, unit: 28.653074367404056\n",
            "2020-09-01 00:00:00\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - loss: 0.3962 - val_loss: 0.2468\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3475 - val_loss: 0.2213\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3302 - val_loss: 0.1981\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.2952 - val_loss: 0.1766\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2717 - val_loss: 0.1560\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2356 - val_loss: 0.1357\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1941 - val_loss: 0.1141\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.1586 - val_loss: 0.0853\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1061 - val_loss: 0.0494\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0535 - val_loss: 0.0207\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0199 - val_loss: 0.0159\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0236 - val_loss: 0.0148\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0171 - val_loss: 0.0164\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0180 - val_loss: 0.0174\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0202 - val_loss: 0.0158\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0161 - val_loss: 0.0148\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0174 - val_loss: 0.0147\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0171 - val_loss: 0.0150\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0160 - val_loss: 0.0152\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0172 - val_loss: 0.0151\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0176 - val_loss: 0.0149\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0147 - val_loss: 0.0150\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0156 - val_loss: 0.0150\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0136 - val_loss: 0.0150\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0176 - val_loss: 0.0149\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0140 - val_loss: 0.0143\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0173 - val_loss: 0.0143\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0150 - val_loss: 0.0145\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0148 - val_loss: 0.0145\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0136 - val_loss: 0.0143\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0152 - val_loss: 0.0145\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0142 - val_loss: 0.0146\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0135 - val_loss: 0.0148\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0152 - val_loss: 0.0149\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0142 - val_loss: 0.0146\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0147 - val_loss: 0.0138\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0148 - val_loss: 0.0137\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0143 - val_loss: 0.0142\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0142 - val_loss: 0.0142\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0138 - val_loss: 0.0140\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0140 - val_loss: 0.0138\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0129 - val_loss: 0.0137\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0136 - val_loss: 0.0140\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0133 - val_loss: 0.0145\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0136 - val_loss: 0.0138\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0130 - val_loss: 0.0135\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0146 - val_loss: 0.0138\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0121 - val_loss: 0.0135\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0112 - val_loss: 0.0134\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0124 - val_loss: 0.0138\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0110 - val_loss: 0.0139\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0120 - val_loss: 0.0139\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0105 - val_loss: 0.0133\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0120 - val_loss: 0.0131\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0114 - val_loss: 0.0134\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0112 - val_loss: 0.0134\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0118 - val_loss: 0.0133\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0115 - val_loss: 0.0131\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0096 - val_loss: 0.0134\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0114 - val_loss: 0.0137\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0117 - val_loss: 0.0138\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0117 - val_loss: 0.0134\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0116 - val_loss: 0.0130\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0101 - val_loss: 0.0131\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0124 - val_loss: 0.0130\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0108 - val_loss: 0.0130\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0113 - val_loss: 0.0127\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0107 - val_loss: 0.0127\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0102 - val_loss: 0.0130\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0108 - val_loss: 0.0131\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0113 - val_loss: 0.0127\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0101 - val_loss: 0.0125\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0109 - val_loss: 0.0125\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0108 - val_loss: 0.0124\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0103 - val_loss: 0.0124\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0087 - val_loss: 0.0123\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0104 - val_loss: 0.0127\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0106 - val_loss: 0.0116\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0098 - val_loss: 0.0115\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0092 - val_loss: 0.0117\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0094 - val_loss: 0.0121\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0096 - val_loss: 0.0118\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0096 - val_loss: 0.0114\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0110 - val_loss: 0.0114\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0105 - val_loss: 0.0115\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0089 - val_loss: 0.0113\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0090 - val_loss: 0.0110\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0106 - val_loss: 0.0110\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0102 - val_loss: 0.0113\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0091 - val_loss: 0.0111\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0096 - val_loss: 0.0109\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0094 - val_loss: 0.0106\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0080 - val_loss: 0.0108\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0092 - val_loss: 0.0111\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0094 - val_loss: 0.0114\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0085 - val_loss: 0.0111\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0085 - val_loss: 0.0109\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0090 - val_loss: 0.0108\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0092 - val_loss: 0.0106\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0089 - val_loss: 0.0105\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0076 - val_loss: 0.0101\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0087 - val_loss: 0.0105\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0082 - val_loss: 0.0100\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0083 - val_loss: 0.0099\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0089 - val_loss: 0.0098\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0086 - val_loss: 0.0094\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0085 - val_loss: 0.0096\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0087 - val_loss: 0.0096\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0080 - val_loss: 0.0093\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0087 - val_loss: 0.0090\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0082 - val_loss: 0.0088\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0074 - val_loss: 0.0088\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0082 - val_loss: 0.0089\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0072 - val_loss: 0.0084\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0080 - val_loss: 0.0086\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0076 - val_loss: 0.0086\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0080 - val_loss: 0.0081\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0075 - val_loss: 0.0078\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0073 - val_loss: 0.0078\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0071 - val_loss: 0.0078\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0073 - val_loss: 0.0080\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0074 - val_loss: 0.0078\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0075 - val_loss: 0.0076\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0074 - val_loss: 0.0077\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0076 - val_loss: 0.0075\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0076 - val_loss: 0.0073\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0068 - val_loss: 0.0076\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0077 - val_loss: 0.0073\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0069 - val_loss: 0.0072\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0075 - val_loss: 0.0072\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0067 - val_loss: 0.0071\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0073 - val_loss: 0.0068\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0071 - val_loss: 0.0070\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0069 - val_loss: 0.0069\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0067 - val_loss: 0.0067\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0067 - val_loss: 0.0067\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0066 - val_loss: 0.0067\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0066 - val_loss: 0.0066\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0065 - val_loss: 0.0066\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0066 - val_loss: 0.0063\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0062 - val_loss: 0.0064\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0063 - val_loss: 0.0065\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0067 - val_loss: 0.0064\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0059 - val_loss: 0.0061\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0068 - val_loss: 0.0061\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0058 - val_loss: 0.0061\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0070 - val_loss: 0.0063\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0052 - val_loss: 0.0058\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0061 - val_loss: 0.0061\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0056 - val_loss: 0.0057\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0059 - val_loss: 0.0054\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0053 - val_loss: 0.0056\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0053 - val_loss: 0.0052\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0059 - val_loss: 0.0052\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0055 - val_loss: 0.0055\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0052 - val_loss: 0.0049\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0052 - val_loss: 0.0055\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0048 - val_loss: 0.0047\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0051 - val_loss: 0.0048\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0053 - val_loss: 0.0050\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0050 - val_loss: 0.0048\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0046 - val_loss: 0.0046\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0044 - val_loss: 0.0045\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0042 - val_loss: 0.0043\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0046 - val_loss: 0.0042\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0039 - val_loss: 0.0040\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045 - val_loss: 0.0041\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0040 - val_loss: 0.0043\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0047 - val_loss: 0.0039\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0043 - val_loss: 0.0040\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0043 - val_loss: 0.0039\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0042 - val_loss: 0.0036\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0043 - val_loss: 0.0037\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0043 - val_loss: 0.0036\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0039 - val_loss: 0.0038\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0040 - val_loss: 0.0034\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0044 - val_loss: 0.0035\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0037 - val_loss: 0.0036\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - val_loss: 0.0032\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0037 - val_loss: 0.0033\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0036 - val_loss: 0.0032\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - val_loss: 0.0033\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0031\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0034 - val_loss: 0.0033\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0037 - val_loss: 0.0030\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0039 - val_loss: 0.0031\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.0031\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0030\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0038 - val_loss: 0.0029\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0033\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.0030\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0035 - val_loss: 0.0031\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0032 - val_loss: 0.0028\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035 - val_loss: 0.0029\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0035 - val_loss: 0.0034\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0043 - val_loss: 0.0028\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0039 - val_loss: 0.0029\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0034 - val_loss: 0.0028\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "Train predicted data:  (366, 1)\n",
            "Test predicted data:  (366, 1)\n",
            "Output of predicted next days:  30\n",
            "\t\t\t First: 1811.8, Last: 1826.6426622629165\n",
            "\t\t Buying 0.015016058125538221 units of gold on 2021-09-01 00:00:00\n",
            "\t\t Price: 1811.8, unit: 27.20609411185015\n"
          ]
        }
      ],
      "source": [
        "from numpy import array\n",
        "alloc = [1000, 0, 0]\n",
        "buy_dates, sell_dates = [], []\n",
        "buy_prices, sell_prices = [], []\n",
        "predicted_prices = []\n",
        "\n",
        "file_path = \"/content/drive/My Drive/modelingComp/LBMA-GOLD.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "prev_date = df.iloc[0]['Date']\n",
        "cut_off_date = pd.to_datetime('9/11/17')\n",
        "\n",
        "df = merged_df\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "\n",
        "  date = df.iloc[i]['Date']\n",
        "\n",
        "  if date < cut_off_date:\n",
        "    prev_date = date\n",
        "    continue\n",
        "  else:\n",
        "    if(date.day < prev_date.day):\n",
        "\n",
        "      price = df.iloc[i]['USD (PM)']\n",
        "      unit = 0.05*alloc[0]\n",
        "      enter_cash = unit\n",
        "\n",
        "      year_ago = df.iloc[i]['Date'].replace(year=df.iloc[i]['Date'].year - 1)\n",
        "      print(year_ago)\n",
        "      temp_df = cut_df(df,year_ago, df.iloc[i]['Date'])\n",
        "      close_stock = temp_df.copy()\n",
        "\n",
        "      del temp_df['Date']\n",
        "      scaler=MinMaxScaler(feature_range=(0,1))\n",
        "      temp_df=scaler.fit_transform(np.array(temp_df).reshape(-1,1))\n",
        "\n",
        "      training_size=int(len(temp_df)*0.60)\n",
        "      test_size=len(temp_df)-training_size\n",
        "      train_data,test_data=temp_df[0:training_size,:],temp_df[training_size:len(temp_df),:1]\n",
        "\n",
        "      time_step = 15\n",
        "      X_train, y_train = create_dataset(train_data, time_step)\n",
        "      X_test, y_test = create_dataset(test_data, time_step)\n",
        "\n",
        "      X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
        "      X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
        "\n",
        "      model=Sequential()\n",
        "\n",
        "      model.add(LSTM(10,input_shape=(None,1),activation=\"relu\"))\n",
        "\n",
        "      model.add(Dense(1))\n",
        "\n",
        "      model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
        "\n",
        "      history = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=200,batch_size=32,verbose=1)\n",
        "\n",
        "      train_predict=model.predict(X_train)\n",
        "      test_predict=model.predict(X_test)\n",
        "\n",
        "      train_predict = scaler.inverse_transform(train_predict)\n",
        "      test_predict = scaler.inverse_transform(test_predict)\n",
        "      original_ytrain = scaler.inverse_transform(y_train.reshape(-1,1))\n",
        "      original_ytest = scaler.inverse_transform(y_test.reshape(-1,1))\n",
        "\n",
        "\n",
        "      look_back=time_step\n",
        "      trainPredictPlot = np.empty_like(temp_df)\n",
        "      trainPredictPlot[:, :] = np.nan\n",
        "      trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n",
        "      print(\"Train predicted data: \", trainPredictPlot.shape)\n",
        "\n",
        "      # shift test predictions for plotting\n",
        "      testPredictPlot = np.empty_like(temp_df)\n",
        "      testPredictPlot[:, :] = np.nan\n",
        "      testPredictPlot[len(train_predict)+(look_back*2)+1:len(temp_df)-1, :] = test_predict\n",
        "      print(\"Test predicted data: \", testPredictPlot.shape)\n",
        "\n",
        "      x_input=test_data[len(test_data)-time_step:].reshape(1,-1)\n",
        "      temp_input=list(x_input)\n",
        "      temp_input=temp_input[0].tolist()\n",
        "\n",
        "      lst_output=[]\n",
        "      n_steps=time_step\n",
        "      i=0\n",
        "      pred_days = 30\n",
        "      while(i<pred_days):\n",
        "\n",
        "          if(len(temp_input)>time_step):\n",
        "\n",
        "              x_input=np.array(temp_input[1:])\n",
        "              #print(\"{} day input {}\".format(i,x_input))\n",
        "              x_input = x_input.reshape(1,-1)\n",
        "              x_input = x_input.reshape((1, n_steps, 1))\n",
        "\n",
        "              yhat = model.predict(x_input, verbose=0)\n",
        "              #print(\"{} day output {}\".format(i,yhat))\n",
        "              temp_input.extend(yhat[0].tolist())\n",
        "              temp_input=temp_input[1:]\n",
        "              #print(temp_input)\n",
        "\n",
        "              lst_output.extend(yhat.tolist())\n",
        "              i=i+1\n",
        "\n",
        "          else:\n",
        "\n",
        "              x_input = x_input.reshape((1, n_steps,1))\n",
        "              yhat = model.predict(x_input, verbose=0)\n",
        "              temp_input.extend(yhat[0].tolist())\n",
        "\n",
        "              lst_output.extend(yhat.tolist())\n",
        "              i=i+1\n",
        "\n",
        "      print(\"Output of predicted next days: \", len(lst_output))\n",
        "\n",
        "      lstmdf=temp_df.tolist()\n",
        "      lstmdf.extend((np.array(lst_output).reshape(-1,1)).tolist())\n",
        "      lstmdf=scaler.inverse_transform(lstmdf).reshape(1,-1).tolist()[0]\n",
        "\n",
        "      predict = lstmdf[(len(lstmdf)-31):]\n",
        "      if predict[-1] > predict[0]:\n",
        "        print(f\"\\t\\t\\t First: {predict[0]}, Last: {predict[-1]}\")\n",
        "        #print(f\"\\t\\t\\t Price: {price}, unit: {enter_cash\n",
        "        if alloc[0] - enter_cash >= 0:\n",
        "          print(f\"\\t\\t Buying {enter_cash / price} units of gold on {date}\")\n",
        "          print(f\"\\t\\t Price: {price}, unit: {enter_cash}\")\n",
        "          alloc[0] -= 1.01 * enter_cash\n",
        "          alloc[2] += enter_cash / price\n",
        "          buy_dates.append(date)\n",
        "          buy_prices.append(price)\n",
        "      else:\n",
        "        if alloc[2] - (enter_cash / price) >= 0:\n",
        "          print(f\"\\t\\t Selling {enter_cash / price} units of gold on {date}\")\n",
        "          print(f\"\\t\\t Price: {price}, unit: {enter_cash}\")\n",
        "          sell = enter_cash / price\n",
        "          #print(\"SELL\")\n",
        "          sell_dates.append(date)\n",
        "          sell_prices.append(price)\n",
        "          alloc[0] += 0.99 * enter_cash\n",
        "          alloc[2] -= sell\n",
        "\n",
        "    prev_date = date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM2UY9mRyd6J",
        "outputId": "693e40bd-852c-46c7-b138-0d44c3911740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cash: 516.6437271840343\n",
            "Gold: 0\n",
            "Bitcoin: 0.27673403487502124\n",
            "Total Money: 516.6437271840343\n",
            "Total Gains -483.35627281596567\n",
            "Returns: -48.335627281596565%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Cash: {alloc[0]}\")\n",
        "#print(f\"Gold: {alloc[1]}\")\n",
        "print(f\"Gold: {alloc[2]}\")\n",
        "total_money = alloc[0] + alloc[1]*df.iloc[-1]['USD (PM)'] #alloc[2]*df.iloc[-1]['Value']\n",
        "print(f\"Total Money: {total_money}\")\n",
        "print(f\"Total Gains {total_money-1000}\")\n",
        "print(f\"Returns: {100*((total_money -1000)/ (1000))}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "uOmE6d_vo8P2",
        "outputId": "1270c6ec-511f-4fd2-88c9-edb4de4c65ae"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1400x700 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot predictions and actual prices\n",
        "prediction_dates = df['Date'][-len(predicted_prices):]\n",
        "\n",
        "# Plot actual and predicted prices\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(df['Date'], df['USD (PM)'], label=\"Actual Prices\", color=\"blue\")\n",
        "#plt.plot(prediction_dates, predicted_prices, label=\"Predicted Prices\", color=\"orange\")\n",
        "\n",
        "# Mark buy and sell points\n",
        "plt.scatter(buy_dates, buy_prices, marker=\"^\", color=\"green\", label=\"Buy\", s=100)\n",
        "plt.scatter(sell_dates, sell_prices, marker=\"v\", color=\"red\", label=\"Sell\", s=100)\n",
        "\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Gold Price\")\n",
        "plt.title(\"Gold Price Prediction and Trading\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}